# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/00_core fast_tfrs_all_code_walkthrough.ipynb (unless otherwise specified).

__all__ = ['setup_ggldrive_github', 'git_push', 'colors', 'cnt_cat_txt_img_split', 'split_cnt_cat_txt_img_dt',
           'df_to_balanced_df', 'split_train_valid_test', 'prep_tfrs_data', 'prep_data', 'default_config', 'check_path',
           'check_url', 'split_cnt_cat_txt_img_datetime', 'cnt_cat_txt_img_split', 'train_test_split_tf',
           'load_resize_image', 'preprocess_cnt_cat_txt_img', 'onehot_encode_label', 'df_to_ds', 'df_to_balanced_ds',
           'ds_to_df', 'txt_module_url_ls', 'txt_module_name_ls', 'txt_module_dim_ls', 'txt_url_dim_ls',
           'txt_module_dic', 'module_name', 'module_url', 'module_dim', 'project_embeddings',
           'create_bert_text_encoder', 'create_text_encoding_model', 'img_module_name_ls', 'img_module_url_dic',
           'img_module_size_dic', 'img_module_dic', 'img_module_name', 'pixels', 'IMAGE_SIZE', 'BATCH_SIZE',
           'get_img_preprocessing_model', 'create_image_encoder_v0', 'create_image_encoder', 'create_bin_cnt_embedding',
           'create_bin_cnt_hash_embedding_model', 'create_cat_encoded_feature', 'create_cat_hash_embedding',
           'create_cat_hash_embedding_model', 'GatedLinearUnit', 'GatedResidualNetwork', 'VariableSelection',
           'LrRangeFinder', 'SGDRScheduler', 'init_imbalanced_class_weight_bias', 'create_model_inputs',
           'encode_inputs', 'create_compiled_model', 'train_valid_eval_multimodal_classifier', 'METRICS', 'focal_loss',
           'get_mispredicted_samples', 'get_performance_metrics', 'plot_loss', 'plot_roc', 'plot_prc',
           'create_deep_cross_layers', 'create_x_model_inputs', 'create_x_compiled_model', 'create_iu_compiled_model',
           'train_valid_eval_iu_classifier', 'save_iu_model', 'load_iu_model', 'ItemUserTwoTowersClassifier',
           'normalize_embeddings', 'samples_to_embeddings', 'find_similar_x', 'get_cosine_similarity_model',
           'keras_knn_recommendations', 'sklearn_knn_recommendations', 'knn_recommendations_1step',
           'knn_1step_prediction', 'Multimodal_Knn_Classification_1step', 'load_knn_model',
           'create_candidates_query_embs_dic', 'knn_recommendations_2steps', 'Multimodal_Knn_Classification_2steps',
           'create_knn_clf_2steps', 'cat_embedding_model', 'cnt_embedding_model', 'cnt_normalize_model',
           'txt_embedding_model', 'img_embedding_model', 'DeepCrossEmbeddingModel', 'DeepCross2taskModel',
           'GatedLinearUnit', 'GatedResidualNetwork', 'VariableSelection', 'create_vsn_feature_selection_layers',
           'VSN_DeepCrossEmbeddingModel', 'VSN_DeepCross2taskModel', 'Classification', 'DeepCross3taskModel',
           'clf_loss_dict', 'create_model', 'plot_history', 'create_fit_validate_evaluate_model',
           'recommend_items_to_user', 'convert_model_config_to_py', 'prep_df_tfds', 'convert_model_config_to_tf',
           'save_model', 'reload_model', 'get_best_model', 'run_models', 'plot_cross_feature_importances']

# Cell
from pathlib import Path
import os, subprocess
from google.colab import drive

def setup_ggldrive_github(path:str='drive/My Drive/fast_tfrs',
                          project_name:str='fast_tfrs',
                          username:str='wjlgatech',
                          password:str='my_password',
                          email:str='wjlgatech@gmail.com',
                          ):
  """Mount your google drive & Link your mounted drive to GitHub.
  ATTENTION: Remove sensitive information before pushing
  """
  drive.mount('/content/drive', force_remount=True)

  path = Path(path) #convert str to Path
  start = os.getcwd()
  os.chdir(path)
  commands = []
  commands.append(f"git config --global user.email {email}")
  commands.append(f"git config --global user.name {username}")
  commands.append("git init")
  commands.append("git remote rm origin")
  commands.append(f"git remote add origin https://{username}:{password}@github.com/{username}/{project_name}.git")
  commands.append("git pull origin master --allow-unrelated-histories")
  for cmd in commands:
    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)
    output, err = process.communicate()
  os.chdir(start)

# Cell
def git_push(path:str='drive/My Drive/fast_tfrs',
             message:str='my 1st push'):
  "Convert the notebooks to scripts and then push to the library"
  path = Path(path) #convert str to Path
  start = os.getcwd()
  os.chdir(path)
  commands = []
  commands.append('nbdev_install_git_hooks')
  commands.append('nbdev_build_lib')
  commands.append('git add *')
  commands.append(f'git commit -m "{message}"')
  commands.append('git push origin master')
  for cmd in commands:
    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)
    output, err = process.communicate()
  os.chdir(start)

# Cell

#@title import packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
import tensorflow_datasets as tfds
import tensorflow_recommenders as tfrs
import tensorflow_hub as hub
import tensorflow_addons as tfa
import tensorflow_ranking as tfr

#metric for imbalanced multi-class classification REF: https://focal-loss.readthedocs.io/en/latest/generated/focal_loss.SparseCategoricalFocalLoss.html#focal_loss.SparseCategoricalFocalLoss
from focal_loss import SparseCategoricalFocalLoss
from tensorflow.keras import regularizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout
from tensorflow.keras.callbacks import EarlyStopping

from datetime import datetime
from scipy.stats import mannwhitneyu
from lshashpy3 import LSHash
from tqdm import notebook
from sklearn.metrics.pairwise import cosine_similarity
from typing import Dict, Text
import re
import pathlib
import shutil
import tempfile

# pretty display
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)
pd.set_option('display.colheader_justify', 'center')
pd.set_option('display.precision', 3)
from IPython.display import display
#display(result_df)
plt.rcParams['figure.figsize'] = (12, 8)
colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
from  IPython import display




# Cell

#@title helpers to identify feature datatype

def cnt_cat_txt_img_split(df:pd.DataFrame, cnt_card=20, txt_card=0.5, excluded_cols:list=['response_status']):
    """To split and group features accroding to their datatypes, including cnt, cat, txt, img
    Args:
      df:pd.DataFrame
      cnt_card (int or float within 0 and 1): cnt cardinarity, e.g. ratio of unique values for cnt column
      txt_card (int or float within 0 and 1): txt cardinarity, e.g. ratio of unique values for txt column
      excluded_cols:list=[label]: e.g. label='response_status', the target/dependant varible column name

    Return:
      txt_cols, cnt_cols, cat_cols, (int_cat_cols, str_cat_cols), img_cols: (list of str)
    Example:
      #for all features:
      txt_cols, cnt_cols, cat_cols, (int_cat_cols, str_cat_cols), img_cols = cnt_cat_txt_split(df, cnt_card=80, txt_card=0.5, excluded_cols=[label, item_id, user_id])
      #for item features:
      i_txt_cols, i_cnt_cols, i_cat_cols, (i_int_cat_cols, i_str_cat_cols), i_img_cols = cnt_cat_txt_img_split(df[i_cols], cnt_card=80, txt_card=0.5, excluded_cols=[label, item_id])
      #for user features:
      u_txt_cols, u_cnt_cols, u_cat_cols, (u_int_cat_cols, u_str_cat_cols), u_img_cols = cnt_cat_txt_img_split(df[u_cols], cnt_card=80, txt_card=0.5, excluded_cols=[label, user_id])

    """
    # init placeholder for cnt, cat (int_cat, str_cat), txt
    cnt_cols, cat_cols, txt_cols, img_cols = [], [], [], []
    int_cat_cols, str_cat_cols = [], []

    # prep cnt cardinality & txt cardinality
    if cnt_card < 1:
        print(f'before adjustment...cnt_card={cnt_card}')
        cnt_card = int(df.shape[0]*cnt_card)
        print(f'before adjustment...cnt_card={cnt_card}')
    if txt_card < 1:
        print(f'before adjustment...txt_card={txt_card}')
        txt_card = int(df.shape[0]*txt_card)
        print(f'before adjustment...txt_card={txt_card}')
    # exclude target
    cols = set(df.columns) - set(excluded_cols)

    # separate cnt, cat, txt columns
    for col in cols:
        if ((pd.api.types.is_integer_dtype(df[col].dtype) and
            df[col].unique().shape[0] > cnt_card) or
            pd.api.types.is_float_dtype(df[col].dtype)): #add to cnt_cols
            cnt_cols.append(col)
        elif (pd.api.types.is_string_dtype(df[col].dtype) and
            df[col].unique().shape[0] > txt_card):
            # make sure to convert Nan(which is np numeric) to str dtype
            if all(['.png' in str(x) or '.jpg' in str(x) for x in df[col].sample(10)]): # and (all([check_url(path) for path in df[col].sample(10)]) or all([check_path(path) for path in df[col].sample(10)])): #check 10 samples to see if they are either valid url or valid path
              img_cols.append(col)
            else: #add to txt_cols
              txt_cols.append(col)
        else: #add to cat_cols
            cat_cols.append(col)
            if pd.api.types.is_integer_dtype(df[col].dtype): #separate cat_cols into int_cat_cols and str_cat_cols
              int_cat_cols.append(col)
            else:
              str_cat_cols.append(col)
    return sorted(cnt_cols), sorted(cat_cols), (sorted(int_cat_cols), sorted(str_cat_cols)), sorted(txt_cols), sorted(img_cols)

def split_cnt_cat_txt_img_dt(df:pd.DataFrame, cnt_card=20, excluded_cols:list=['response_status'], txt_card=0.5):
    """Helper function that returns column names of cnt, cat (furtherly split into int_cat, str_cat), txt variables from given df.
    Args:
      df
      cnt_card (int or float within 0 and 1): cnt cardinarity, e.g. ratio of unique values for cnt column
      excluded_cols:list=[label]: e.g. label='response_status', the target/dependant varible column name
      txt_card (int or float within 0 and 1): txt cardinarity, e.g. ratio of unique values for txt column
    Return:
      txt_cols, cnt_cols, cat_cols, (int_cat_cols, str_cat_cols), img_cols: (list of str)
    Example:
      #for all features:
      txt_cols, cnt_cols, cat_cols, (int_cat_cols, str_cat_cols), img_cols = cnt_cat_txt_split(df, cnt_card=80, excluded_cols=[label, item_id, user_id], txt_card=0.5)
      #for item features:
      i_txt_cols, i_cnt_cols, i_cat_cols, (i_int_cat_cols, i_str_cat_cols), i_img_cols = cnt_cat_txt_img_split(df[i_cols], cnt_card=80, excluded_cols=[label, item_id], txt_card=0.5)
      #for user features:
      u_txt_cols, u_cnt_cols, u_cat_cols, (u_int_cat_cols, u_str_cat_cols), u_img_cols = cnt_cat_txt_img_split(df[u_cols], cnt_card=80, excluded_cols=[label, user_id], txt_card=0.5)

    """
    # init placeholder for cnt, cat (int_cat, str_cat), txt
    cnt_cols, cat_cols, txt_cols, img_cols, dt_cols = [], [], [], [], []
    int_cat_cols, str_cat_cols = [], []

    # prep cnt cardinality & txt cardinality
    if cnt_card < 1:
        print(f'before adjustment...cnt_card={cnt_card}')
        cnt_card = int(df.shape[0]*cnt_card)
        print(f'before adjustment...cnt_card={cnt_card}')
    if txt_card < 1:
        print(f'before adjustment...txt_card={txt_card}')
        txt_card = int(df.shape[0]*txt_card)
        print(f'before adjustment...txt_card={txt_card}')
    # exclude target
    cols = set(df.columns) - set(excluded_cols)

    # separate cnt, cat, txt columns
    for col in cols:
        if pd.api.types.is_datetime64_any_dtype(df[col].dtype):
            dt_cols.append(col)
        elif ((pd.api.types.is_integer_dtype(df[col].dtype) and ### if dtype is float or int with card > threshold, add to cnt_cols
            df[col].unique().shape[0] > cnt_card) or
            pd.api.types.is_float_dtype(df[col].dtype)):
            cnt_cols.append(col)
        elif (pd.api.types.is_string_dtype(df[col].dtype) and ### if dtype is string, add to img_cols if with '.png' or '.jpd' otherwise add to txt_cols
            df[col].unique().shape[0] > txt_card):
            # make sure to convert Nan(which is np numeric) to str dtype
            if all(['.png' in str(x) or '.jpg' in str(x) for x in df[col].sample(10)]): # and (all([check_url(path) for path in df[col].sample(10)]) or all([check_path(path) for path in df[col].sample(10)])): #check 10 samples to see if they are either valid url or valid path
              img_cols.append(col)
            else: ###add to txt_cols
              txt_cols.append(col)
        else: ###all the rest, add to cat_cols
            cat_cols.append(col)
            if pd.api.types.is_integer_dtype(df[col].dtype): #separate cat_cols into int_cat_cols and str_cat_cols
              int_cat_cols.append(col)
            else:
              str_cat_cols.append(col)
    return  sorted(cnt_cols), sorted(cat_cols), (sorted(int_cat_cols), sorted(str_cat_cols)), sorted(txt_cols), sorted(img_cols), sorted(dt_cols)




# Cell
#title df_to_balanced_df
def df_to_balanced_df(df:pd.DataFrame, label:str):
    """"turn imbalanced df to balanced df
    Example:
      train_df_bal = df_to_balanced_df(train_df.copy(), label)
    """
    from imblearn.over_sampling import RandomOverSampler
    from collections import Counter
    over_sampler = RandomOverSampler(random_state=42)
    y= df[label]
    X= df.copy().drop([label], axis=1)
    X_bal, y_bal = over_sampler.fit_resample(X, y)
    X_bal[label]=y_bal #add label column to the feature dataframe X_bal
    print(f"Original data target statistics: {Counter(y)}")
    print(f"Balanced data target statistics: {Counter(y_bal)}")

    return X_bal
#title prep data for recommender
def split_train_valid_test(df, train_valid_test=[0.7,0.15, 0.15], target='response_status', random_state=123, sort_split_by_col='start_datetime'):
    '''Splits a Pandas Dataframe into training, evaluation and serving sets, stratifying on target column.

    Args:
            df : pandas dataframe to split
            train_valid_test: a list of 3 positive numbers, each being either float or integer
            target (string): the name of target column
            random_state (int or None): the random seed to shuffle df; if None, do not shuffle df
            sort_split_by_col (str or list of str) e.g.'index', 'start_datetime' or ['start_datetime','event_end_datetime']
    Returns:
            train_df: Training dataframe(i.e. 70% of the entire dataset)
            valid_df: Evaluation dataframe (i.e. 15% of the entire dataset)
            test_df: Serving dataframe (i.e. 15% of the entire dataset, label column dropped)
            keep_datetime_order (default True): after splitting data into train < validation < serving
    Ref:
        C2W1_assignment.ipynb using TFDV to visulize, validate and moritor data at scale
    '''
    if len(train_valid_test)==3 and not any(x < 0 for x in train_valid_test):
        tot = sum(train_valid_test)
        train_valid_test = [x/tot for x in train_valid_test]
    else:
        raise ValueError('train_valid_test need to be a list of 3 positive numbers!')

    if sort_split_by_col is not None:
        if sort_split_by_col=='index':
            df.sort_index(inplace=True) #for ui, datetime info is stored in df.index
            df.reset_index(drop=False, inplace=True)

        ls = list(range(df.shape[0])) #range_of(df)
        df.sort_values(by=sort_split_by_col, inplace=True)
        ls_train = ls[:int(df.shape[0]*train_valid_test[0])]
        train_cut_date = df.iloc[ls_train[-1],:][sort_split_by_col]
        train_df = df[df[sort_split_by_col]<=train_cut_date]

        ls_test = ls[:int(df.shape[0]*sum(train_valid_test[0:2]))]
        test_cut_date = df.iloc[ls_test[-1],:][sort_split_by_col]
        test_cut_date = max(test_cut_date, train_cut_date)
        test_df = df[df[sort_split_by_col]>test_cut_date]

        try: valid_df=df[(df[sort_split_by_col]>train_cut_date) & (df[sort_split_by_col]<=test_cut_date)]
        except: valid_df = pd.DataFrame()

        ls_train = ls[:int(train_df.shape[0])]
        ls_test = ls[-int(test_df.shape[0]):]
        try: ls_valid = ls[int(train_df.shape[0]):-int(test_df.shape[0])]
        except: ls_valid = []
        n_train, n_valid, n_serv = train_df.shape[0], valid_df.shape[0], test_df.shape[0]
        print(f'================Double check: the indices of train, valid and test are sorted by {sort_split_by_col}=================== ')
        print(f'train_df=\n{df.iloc[:n_train,:][sort_split_by_col].tail()}\n')
        print(f'valid_df.head()=\n{df.iloc[n_train:(n_train+n_valid),:][sort_split_by_col].head()}\n')
        print(f'valid_df.tail()=\n{df.iloc[n_train:(n_train+n_valid),:][sort_split_by_col].tail()}\n')
        print(f'test_df=\n{df.iloc[(n_train+n_valid):,:][sort_split_by_col].head()}')
        df.reset_index(drop=True, inplace=True)
        train_df, valid_df, test_df  = df.iloc[:n_train,:], df.iloc[n_train:(n_train+n_valid),:], df.iloc[(n_train+n_valid):,:]#.drop([target], axis=1)
        return train_df, valid_df, test_df# X_test, y_test

    # downstream dl clf can not accept datetime index, therefore df.reset_index()
    df.reset_index(drop=True, inplace=True)
    from sklearn.model_selection import train_test_split
    train_df, eval_serv = train_test_split(df, stratify=df[target], test_size = 1 - train_valid_test[0], random_state=random_state)
    if train_valid_test[1]>0:
        valid_df, test_df = train_test_split(eval_serv, stratify=eval_serv[target], test_size = train_valid_test[1]/(1 - train_valid_test[0]), random_state=random_state)
    else:
        valid_df, test_df = None, eval_serv
    # Serving data emulates the data that would be submitted for predictions, so it should not have the label column.
    #y_test = test_df[target]
    #X_test = test_df.drop([target], axis=1)

    return train_df, valid_df, test_df

def prep_tfrs_data(df:pd.DataFrame, label:str, user_id:str, item_id:str, cat_cols:list, cnt_cols:list, txt_cols:list,img_cols:list, i_cat_cols:list, i_cnt_cols:list, i_txt_cols:list, u_cat_cols:list, u_cnt_cols:list, u_txt_cols:list):
    """a wraper function that turn df into formated datasets needed for tfrs recommender
    Args:
      df, label, user_id, item_id, cat_cols, cnt_cols, txt_cols
    Returns:
      {'interactions':interactions, 'items':items, 'users':users, 'unique_item_ids':unique_item_ids, 'unique_user_ids':unique_user_ids}


    """
    # remove sample rows with nan on label
    if np.isnan(df[label]).any():
      print('===========There is nan in df[label] which will be removed===========')
      df = df[df[label].notna()]

    interactions_df = df.groupby(list(set([user_id, item_id])|set(cat_cols)|set(cnt_cols)|set(txt_cols)))[label].sum().reset_index()
    interactions_dict = {name: np.array(value) for name, value in interactions_df.items()}
    interactions_tfds = tf.data.Dataset.from_tensor_slices(interactions_dict)

    interactions = interactions_tfds.map(lambda x: {**{
                                                user_id : x[user_id],
                                                item_id : x[item_id],
                                                label: float(x[label])},
                                                **{cnt_col: x[cnt_col] for cnt_col in cnt_cols},
                                                **{cat_col: x[cat_col] for cat_col in cat_cols},
                                                **{txt_col: x[txt_col] for txt_col in txt_cols},
                                                **{img_col: x[img_col] for img_col in img_cols},

                                                })

    items_df = df[[item_id]+i_cnt_cols+i_cat_cols+i_txt_cols].drop_duplicates()
    items_dict = {name: np.array(value) for name, value in items_df.items()}
    items_tfds = tf.data.Dataset.from_tensor_slices(items_dict)

    items = items_tfds.map(lambda x: {**{
                                        #user_id : x[user_id],
                                        item_id : x[item_id],
                                        #label: float(x[label])
                                        },
                                        **{cnt_col: x[cnt_col] for cnt_col in i_cnt_cols},
                                        **{cat_col: x[cat_col] for cat_col in i_cat_cols},
                                        **{txt_col: x[txt_col] for txt_col in i_txt_cols},
                                        **{img_col: x[img_col] for img_col in img_cols},
                                      })

    users = interactions_tfds.map(lambda x: {**{
                                        user_id : x[user_id],
                                        #item_id : x[item_id],
                                        #label: float(x[label])
                                        },
                                        **{cnt_col: x[cnt_col] for cnt_col in u_cnt_cols},
                                        **{cat_col: x[cat_col] for cat_col in u_cat_cols},
                                        **{txt_col: x[txt_col] for txt_col in u_txt_cols},
                                        **{img_col: x[img_col] for img_col in img_cols},
                                        })

    unique_item_ids = np.unique(items_df[item_id])
    unique_user_ids = np.unique(interactions_df[user_id])
    return {'interactions':interactions, 'items':items, 'users':users, 'unique_item_ids':unique_item_ids, 'unique_user_ids':unique_user_ids}

def prep_data(iu:pd.DataFrame,
              label:str,
              user_id:str,
              item_id:str,
              u_cols:list,
              i_cols:list,
              cnt_cols:list,
              cat_cols:list,
              txt_cols:list,
              img_cols:list,
              dt_cols:list,
              sel_cols:list,
              flag_dt_to_cnt:bool=True,
              cnt_cols_to_bin:list=[],
              flag_train_valid_test_split:bool=True,
              sort_split_by_col:str='timestamp',
              flag_balancing_train_df:bool=True,
              ):
    """prep data (global variables and global datasets) needed to build a recommender
    Args:
      iu:pd.DataFrame,
      label:str,
      user_id:str,
      item_id:str,
      u_cols:list,
      i_cols:list,
      cnt_cols:list,
      cat_cols:list,
      txt_cols:list,
      img_cols:list,
      dt_cols:list,
      flag_dt_to_cnt:bool=True, converting dt_cols into cnt_cols
      sel_cols:list,
      cnt_cols_to_bin:list=[], (a subset of) cnt_cols chosen by user to be bucketized
      flag_train_test_split:bool=True, whether or not to split data into train-valid-test parts
      sort_split_by_col:str='timestamp', args for function cnt_cat_txt_img_split()
      flag_rm_dt_cols:bool=True, remove dt_cols after using them for feature engineering
    Returns:
      global_vars = (label, user_id, item_id, x_cols, u_cols, i_cols, cnt_cols, u_cnt_cols, i_cnt_cols,cnt_cols_to_bin, u_cnt_cols_to_bin, i_cnt_cols_to_bin,bin_cnt_cols, u_bin_cnt_cols, i_bin_cnt_cols, cat_cols, u_cat_cols, i_cat_cols, txt_cols, u_txt_cols, i_txt_cols, img_cols, u_img_cols, i_img_cols, label_str2num,label_num2str, num_classes, label_keys)
      global_data = (df, interactions, users, items, unique_user_ids, unique_item_ids, train_df, valid_df, test_df, cached_train, cached_valid, cached_test, test_items,)
    """
    df = iu[list(set([user_id, item_id, label]+sel_cols))].copy()

    # remove sample rows with nan on label
    if np.isnan(df[label]).any():
      print('===========There is nan in df[label] which will be removed===========')
      df = df[df[label].notna()]

    # update features of various datatype
    cnt_cols = list(set(cnt_cols) & set(sel_cols))
    cat_cols = list(set(cat_cols) & set(sel_cols))
    txt_cols = list(set(txt_cols) & set(sel_cols))
    img_cols = list(set(img_cols) & set(sel_cols))
    dt_cols = list(set(dt_cols) & set(sel_cols))
    i_cols = list(set(i_cols) & set(sel_cols))
    u_cols = list(set(u_cols) & set(sel_cols))
    # convert datetime into cnt
    from datetime import datetime
    from time import mktime

    if flag_dt_to_cnt: #converting dt to cnt
        for col in dt_cols:
          df[col] = [mktime(t.timetuple())for t in df[col]]
        cnt_cols += dt_cols
        dt_cols = []
    else:
      for col in dt_cols:
        iu[col] = pd.to_datetime(df[col])


    label_keys = list(iu[label].value_counts().keys())
    label_keys.sort()

    # define i_cnt_cols, i_cat_cols, i_txt_cols, i_img_cols
    i_cnt_cols = list(set(cnt_cols) & set(i_cols))
    i_cat_cols = list(set(cat_cols) & set(i_cols))
    i_txt_cols = list(set(txt_cols) & set(i_cols))
    i_img_cols = list(set(img_cols) & set(i_cols))
    i_dt_cols = list(set(dt_cols) & set(i_cols))

    # define u_cnt_cols, u_cat_cols, u_txt_cols, u_img_cols
    u_cnt_cols = list(set(cnt_cols) & set(u_cols))
    u_cat_cols = list(set(cat_cols) & set(u_cols))
    u_txt_cols = list(set(txt_cols) & set(u_cols))
    u_img_cols = list(set(img_cols) & set(u_cols))
    u_dt_cols = list(set(dt_cols) & set(u_cols))

    ##### standardlize data types: user_id, item_id as str; label as float; cat_cols as str

    #if df[label].dtype == 'object':
    label_str2num = {x:i for (i,x) in enumerate(label_keys)} # e.g. {'accepted':2, 'declined':0, 'tentative':1, 'needsAction':1}
    label_num2str = {v:k for (k,v) in label_str2num.items()}
    # convert label to numeric
    df[label] = df[label].map(label_str2num)


    # replace NaN by 'TBD' for 2 text cols
    for col in txt_cols:
        df[col] = df[col].fillna('TBD')
        df[col] = [str(x).split('<br>')[0] for x in df[col]] # get rid of anything after <br>, which is the google meeting formating info, irrelevant to our application

    ## change dtype
    df[cat_cols] = df[cat_cols].astype(str)
    df[cnt_cols] = df[cnt_cols].astype(float)
    df[txt_cols] = df[txt_cols].astype(str)


    """if flag_rm_dt_cols:
      for col in dt_cols:
        df.drop(col, axis=1, inplace=True)
      i_dt_cols = []
      u_dt_cols = []
      dt_cols = []
      i_cols = list(set(i_cols)-set(dt_cols))
      u_cols = list(set(u_cols)-set(dt_cols))"""



    # number of classes
    num_classes = len(label_keys)

    #dbck
    print(f'\n=======\n label_vals = {label_keys},\n label_vals distribution: \n{df[label].value_counts()/df.shape[0]} \n num_classes = {num_classes},\n label_str2num = {label_str2num}')

    # all features for user-item|user|item
    x_cols = list(set([item_id, user_id]) | set(cnt_cols) | set(cat_cols) | set(txt_cols) | set(img_cols)|set(dt_cols))
    u_cols = list(set([user_id]) | set(u_cols))
    i_cols = list(set([item_id]) | set(i_cols))

    # continuous features to be bucketized (bin) for user-item|user|item
    u_cnt_cols_to_bin = list(set(cnt_cols_to_bin)&set(u_cols))
    i_cnt_cols_to_bin = list(set(cnt_cols_to_bin)&set(i_cols))

    # create bin(bucketized) continuous features for user-item|user|item
    if len(cnt_cols_to_bin)>0:
        bin_cnt_cols = ['bin_'+col for col in cnt_cols_to_bin]
        print(f'=======Given cnt_cols_to_bin = {cnt_cols_to_bin}=======')
        print(f'=======Now create bin_cnt_cols = {bin_cnt_cols}=======')
    else:
        bin_cnt_cols = []

    if len(u_cnt_cols_to_bin)>0:
        u_bin_cnt_cols = ['bin_'+col for col in u_cnt_cols_to_bin]
        print(f'=======Given cnt_cols_to_bin = {u_cnt_cols_to_bin}=======')
        print(f'=======Now create bin_cnt_cols = {u_bin_cnt_cols}=======')
    else:
        u_bin_cnt_cols = []

    if len(i_cnt_cols_to_bin)>0:
        i_bin_cnt_cols = ['bin_'+col for col in i_cnt_cols_to_bin]
        print(f'=======Given cnt_cols_to_bin = {i_cnt_cols_to_bin}=======')
        print(f'=======Now create bin_cnt_cols = {i_bin_cnt_cols}=======')
    else:
        i_bin_cnt_cols = []
    #update u_cat_cols, i_cat_cols, cat_cols with u_bin_cnt_cols, i_bin_cnt_cols
    u_cat_cols = u_cat_cols + u_bin_cnt_cols
    i_cat_cols = i_cat_cols + i_bin_cnt_cols
    cat_cols = i_cat_cols + u_cat_cols

    print(f'\n=======\n x_cols={x_cols}, \n u_cols={u_cols}, \n i_cols={i_cols},\n=======\n cnt_cols={cnt_cols}, \n u_cnt_cols={u_cnt_cols}, \n i_cnt_cols={i_cnt_cols},\n=======\n cnt_cols_to_bin={cnt_cols_to_bin}, \n u_cnt_cols_to_bin={u_cnt_cols_to_bin}, \n i_cnt_cols_to_bin={i_cnt_cols_to_bin},\n=======\n bin_cnt_cols={bin_cnt_cols}, \n u_bin_cnt_cols={u_bin_cnt_cols},\n i_bin_cnt_cols={i_bin_cnt_cols},\n=======\n cat_cols={cat_cols}, \n u_cat_cols={u_cat_cols}, \n i_cat_cols={i_cat_cols}, \n=======\n txt_cols={txt_cols}, \n u_txt_cols={u_txt_cols}, \n i_txt_cols={i_txt_cols},\n=======\n img_cols={img_cols}, \n u_img_cols={u_img_cols}, \n i_img_cols={i_img_cols} ')
    print(f'\n=======\n set(x_cols) - set(u_cols) == set(i_cols): {set(x_cols) - set(u_cols) == set(i_cols)},\n=======\n  set(i_cols) == set([item_id]+i_cnt_cols+i_cat_cols+i_txt_cols+i_img_cols+i_dt_cols): {set(i_cols) == set([item_id]+i_cnt_cols+i_cat_cols+i_txt_cols+i_img_cols+i_dt_cols)},\n=======\n  set(u_cols) == set([user_id]+u_cnt_cols+u_cat_cols+u_txt_cols+u_img_cols+u_dt_cols):{set(u_cols) == set([user_id]+u_cnt_cols+u_cat_cols+u_txt_cols+u_img_cols+u_dt_cols)}')
    print(f'\n=======\n set(cnt_cols) - set(u_cnt_cols) == set(i_cnt_cols): {set(cnt_cols) - set(u_cnt_cols) == set(i_cnt_cols)}')
    print(f'\n=======\n set(cat_cols) - set(u_cat_cols) == set(i_cat_cols): {set(cat_cols) - set(u_cat_cols) == set(i_cat_cols)}')
    print(f'\n=======\n set(txt_cols) - set(u_txt_cols) == set(i_txt_cols): {set(txt_cols) - set(u_txt_cols) == set(i_txt_cols)}')
    print(f'\n=======\n set(dt_cols) - set(u_dt_cols) == set(i_dt_cols): {set(dt_cols) - set(u_dt_cols) == set(i_dt_cols)}')
    print(f'\n=======\n df[label].value_counts()/df.shape[0]= {df[label].value_counts()/df.shape[0]}')

    # drop nan rows after all fillna efforts
    df = df.dropna()

    # split df into train valid & test
    if flag_train_valid_test_split:
        train_df, valid_df, test_df = split_train_valid_test(df, train_valid_test=[0.7,0.15, 0.15], target=label, random_state=123, sort_split_by_col=sort_split_by_col)
        train_n,valid_n, test_n = train_df.shape[0], valid_df.shape[0], test_df.shape[0]
        print(f'**BEFORE balancing** =====train_df.shape[0]={train_df.shape[0]},valid_df.shape[0]={valid_df.shape[0]}, test_df.shape[0]={test_df.shape[0]}=====')
        print(f'\n**BEFORE balancing** =======\n train_df[label].value_counts()/train_df.shape[0]= {train_df[label].value_counts()/train_df.shape[0]}')
        print(f'\n=======\n valid_df[label].value_counts()/valid_df.shape[0]= {valid_df[label].value_counts()/valid_df.shape[0]}')
        print(f'\n=======\n test_df[label].value_counts()/test_df.shape[0]= {test_df[label].value_counts()/test_df.shape[0]}')
        #print(f'\n=======\n how many personaliz candidate_df need? ...len(test_df[user_id].unique())= {len(test_df[user_id].unique())}')
        #print(f'\n=======\n how many upcoming meeting need to predict ...len(test_df[item_id].unique())= {len(test_df[item_id].unique())}')

        # split df into train_df, valid_df, test_df. NOTE: to random split, set `sort_split_by_col=None`; otherwise, 'start_datetime'
        #train_df, valid_df, test_df = split_train_valid_test(df, train_valid_test=[0.7,0.15,0.15], target=label, random_state=123, sort_split_by_col='start_datetime')

        if flag_balancing_train_df:
            train_df = df_to_balanced_df(train_df.copy(), label)
            df = pd.concat([train_df, valid_df, test_df], axis=0)
            train_n,valid_n, test_n = train_df.shape[0], valid_df.shape[0], test_df.shape[0]
            print(f'\**AFTER balancing** n=======\n train_df.shape={train_df.shape},valid_df.shape={valid_df.shape}, test_df.shape={test_df.shape}')
            print(f'\n**AFTER balancing** ======= train label distribution\n{train_df[label].value_counts()/train_df.shape[0]}')
            print(f'\n======= valid label distribution\n{valid_df[label].value_counts()/valid_df.shape[0]}')
            print(f'\n======= test label distribution\n {test_df[label].value_counts()/test_df.shape[0]} ')



    #######################################
    ### define data for tfrs recommender
    #######################################
    """ wrap the following code into prep_tfrs_data()
    interactions_df = df.groupby(list(set([user_id, item_id])|set(cat_cols)|set(cnt_cols)|set(txt_cols)))[label].sum().reset_index()
    interactions_dict = {name: np.array(value) for name, value in interactions_df.items()}
    interactions_tfds = tf.data.Dataset.from_tensor_slices(interactions_dict)

    interactions = interactions_tfds.map(lambda x: {**{
                                                user_id : x[user_id],
                                                item_id : x[item_id],
                                                label: float(x[label])},
                                                **{cnt_col: x[cnt_col] for cnt_col in cnt_cols},
                                                **{cat_col: x[cat_col] for cat_col in cat_cols},
                                                **{txt_col: x[txt_col] for txt_col in txt_cols}})

    items_df = df[[item_id]+i_cnt_cols+i_cat_cols+i_txt_cols].drop_duplicates()
    items_dict = {name: np.array(value) for name, value in items_df.items()}
    items_tfds = tf.data.Dataset.from_tensor_slices(items_dict)

    items = items_tfds.map(lambda x: {**{
                                        #user_id : x[user_id],
                                        item_id : x[item_id],
                                        #label: float(x[label])
                                        },
                                        **{cnt_col: x[cnt_col] for cnt_col in i_cnt_cols},
                                        **{cat_col: x[cat_col] for cat_col in i_cat_cols},
                                        **{txt_col: x[txt_col] for txt_col in i_txt_cols}})

    users = interactions_tfds.map(lambda x: {**{
                                        user_id : x[user_id],
                                        #item_id : x[item_id],
                                        #label: float(x[label])
                                        },
                                        **{cnt_col: x[cnt_col] for cnt_col in u_cnt_cols},
                                        **{cat_col: x[cat_col] for cat_col in u_cat_cols},
                                        **{txt_col: x[txt_col] for txt_col in u_txt_cols}})

    unique_item_ids = np.unique(items_df[item_id])
    unique_user_ids = np.unique(interactions_df[user_id])
    """



    if flag_train_valid_test_split:
        train_dict = prep_tfrs_data(train_df, label, user_id, item_id, cat_cols, cnt_cols, txt_cols,img_cols, i_cat_cols, i_cnt_cols, i_txt_cols, u_cat_cols, u_cnt_cols, u_txt_cols)

        valid_dict = prep_tfrs_data(valid_df,label, user_id, item_id, cat_cols, cnt_cols, txt_cols, img_cols, i_cat_cols, i_cnt_cols, i_txt_cols, u_cat_cols, u_cnt_cols, u_txt_cols)

        test_dict = prep_tfrs_data(test_df, label, user_id, item_id, cat_cols, cnt_cols, txt_cols, img_cols, i_cat_cols, i_cnt_cols, i_txt_cols, u_cat_cols, u_cnt_cols, u_txt_cols)

        df_dict = prep_tfrs_data(df, label, user_id, item_id, cat_cols, cnt_cols, txt_cols, img_cols, i_cat_cols, i_cnt_cols, i_txt_cols, u_cat_cols, u_cnt_cols, u_txt_cols)

        interactions = df_dict['interactions']
        items = df_dict['items']
        users = df_dict['users']
        unique_item_ids = df_dict['unique_item_ids']
        unique_user_ids = df_dict['unique_user_ids']


        # split interactions data into train & test according to user-defined proportion along accending datetime
        print(f'=====train_n={train_n}, valid_n={valid_n}, test_n={test_n}=====')
        train = train_dict['interactions'].take(train_n)
        valid = valid_dict['interactions'].take(valid_n)
        test = test_dict['interactions'].take(test_n)

        cached_train = train.shuffle(train_n).batch(8192).cache()
        cached_valid = valid.batch(4096).cache()
        cached_test = test.batch(4096).cache()

        # define items in test data. Items in the test data are the upcoming meetings we want to recommend for users:
        test_items_df = test_df[[item_id]+i_cnt_cols+i_cat_cols+i_txt_cols].drop_duplicates()
        test_items_dict = {name: np.array(value) for name, value in test_items_df.items()}
        test_items_tfds = tf.data.Dataset.from_tensor_slices(test_items_dict)

        test_items = test_items_tfds.map(lambda x: {**{
                                                    #user_id : x[user_id],
                                                    item_id : x[item_id],
                                                    #label: float(x[label])
                                                    },
                                                    **{cnt_col: x[cnt_col] for cnt_col in i_cnt_cols},
                                                    **{cat_col: x[cat_col] for cat_col in i_cat_cols},
                                                    **{txt_col: x[txt_col] for txt_col in i_txt_cols}})
    else:
        train_df, valid_df, test_df, train_ds, valid_ds, test_ds, cached_train, cached_valid, cached_test, test_items=None, None, None, None, None,None, None, None, None, None
    global_vars = (label, user_id, item_id, x_cols, u_cols, i_cols, cnt_cols, u_cnt_cols, i_cnt_cols,cnt_cols_to_bin, u_cnt_cols_to_bin, i_cnt_cols_to_bin,bin_cnt_cols, u_bin_cnt_cols, i_bin_cnt_cols, cat_cols, u_cat_cols, i_cat_cols, txt_cols, u_txt_cols, i_txt_cols, img_cols, u_img_cols, i_img_cols, dt_cols, u_dt_cols, i_dt_cols, label_str2num,label_num2str, num_classes, label_keys)
    global_data = (df, interactions, users, items, unique_user_ids, unique_item_ids, train_df, valid_df, test_df, cached_train, cached_valid, cached_test, test_items,)
    return global_vars, global_data




# Cell

#@title define default_config

# default_config need to be updated with the commented out items
default_config = {
 'alpha': 0.0001,
 'batch_size': 256,
 'bias_initializer': None,
 'bin_cnt_cols': None,#bin_cnt_cols
 'candidate_embs_dic': None,
 'cached_train':None,#cashed_train,
 'cached_valid':None,#cahsed_valid,
 'cached_test':None,#cashed_test,
 'cat_cols': None,#cat_cols
 'classification_weight': 0.0,
 'clf_loss_name': ['SparseCategoricalCrossentropy',
                   'SparseCategoricalFocalLoss',
                   'CategoricalCrossentropy',
                   'SigmoidFocalCrossEntropy'][-1],

 'clf_metrics': [
      tf.keras.metrics.TruePositives(name='tp'),
      tf.keras.metrics.FalsePositives(name='fp'),
      tf.keras.metrics.TrueNegatives(name='tn'),
      tf.keras.metrics.FalseNegatives(name='fn'),
      tf.keras.metrics.BinaryAccuracy(name='accuracy'),
      tf.keras.metrics.Precision(name='precision'), # the percentage of predicted positives that were correctly classified, precision = TP/(TP+FP), the denominator is all the predicted positive
      tf.keras.metrics.Recall(name='recall'), #  the percentage of actual positives that were correctly classified, recall = TP/(TP+FN), where the demoninator is all the factual positive
      tf.keras.metrics.AUC(name='roc'), #auc_roc curve: the probability that a classifier will rank a random positive sample higher than a random negative sample
      tf.keras.metrics.AUC(name='prc', curve='PR'), # auc precision-recall curve: precision-recall pairs for different probability thresholds.
      ],
 'clf_optimizer':[tf.keras.optimizers.Adam(learning_rate=3e-4),
                  tf.keras.optimizers.Adadelta()][0],
 'cnt_cols': None,#cnt_cols
 'config_filepath': '/content/drive/My Drive/fast_tfrs/model/encoding_model_config.pkl',
 'cos_sim_method': 'sklearn',
 'data_path': '/content/drive/My Drive/fast_tfrs/data/',
 'df':None,#df,
 'dropout_rate': 0.2,
 'embedding_dim': 16,
 'encoding_model': None,
 'epochs': 100,
 'flag_bin_cnt': False,
 'flag_balance_train_df':True, #do not use resampled/balanced train_ds for model training
 'flag_SGDRScheduler':True,
 'flag_init_bias_class_weight':False,
 'flag_trainable': False,
 'flag_txt_bert': False,
 'flag_use_norm': True,
 'flag_vsn_deep_cross': False,
 'flag_dcn_cross':True,
 'global_vars': None,
 'i_cat_cols': [],#i_cat_cols,
 'i_cnt_cols': [],#i_cnt_cols,
 'i_cnt_cols_bins': None,#{col:5 for col in i_cnt_cols}
 'i_img_cols': [],#i_img_cols
 'i_txt_cols': [],#i_txt_cols
 'interactions':None, #interactions
 'items':None, #items,
 'img_cols': [],#img_cols
 'img_module_url': 'https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4',
 'item_id': '',#item_id
 'item_layer_sizes': [32],
 'l1_l2': 1,
 'label': None,#label
 'label_num2str': None, #label_num2str
 'label_str2num': None, #label_str2num
 'layer_sizes': [128, 64, 32],
 'lr': 0.1,
 'max_lr': 0.005,
 'min_candidates_count': 5,
 'min_lr': 0.0001,
 'model_name': 'mdl',
 'model_path': '/content/drive/My Drive/fast_tfrs/model/',
 'model_type': 'non-vsn',
 'monitor': 'val_prc',
 'num_bins': 5,
 'num_classes': None,
 'num_projection_layers': 2,
 'patience':5,
 'prob': None,
 'prob_numkey': None,
 'projection_dim': 16,
 'query_embs_dic': None,
 'rating_weight': 0.85,
 'rating_optimizer':None,
 'rating_metrics':['factorized_top_k/top_50_categorical_accuracy',
                  'factorized_top_k/top_100_categorical_accuracy',
                  'ndcg_metric',
                  ],
 'rating_loss_name':['listwise','mse','pairwise'][1],
 'result_df_ls': None,
 'retrieval_weight': 0.15,
 'sort_split_by_col': 'order_purchase_timestamp',
 'test_df': None,
 'top_k': 5,
 'top_u': 3,
 'train_df': None,
 'train_valid_test': [0.7, 0.15, 0.15],
 'txt_cols': None, #txt_cols
 'u_cat_cols': [],#u_cat_cols
 'u_cnt_cols': [],#u_cnt_cols
 'u_cnt_cols_bins': None,#{col:5 for col in u_cnt_cols}
 'u_img_cols': [],#u_img_cols
 'u_txt_cols': [],#u_txt_cols
 'user_id': 'customer_id',
 'user_layer_sizes': [32],
 'weight_filepath': '/content/drive/My Drive/fast_tfrs/model/encoding_model_weight',
 'X_train': None,
 'y_train': None,
 }

# Cell

#@title grouping features into cnt, cat, txt, img, dt
import requests
import pandas as pd
def check_path(path):
    """check if path is a valid directory or not"""
    try:
      return os.path.exists(os.path.dirname(path))
    except:
      return False
def check_url(path):
    """check if path is a valid url or not"""
    try: return requests.get(path)
    except:
      if 'http' in path:
        return True
      else:
        return False

def split_cnt_cat_txt_img_datetime(df:pd.DataFrame, cnt_card=20, excluded_cols:list=['response_status'], txt_card=0.5):
    """Helper function that returns column names of cnt, cat (furtherly split into int_cat, str_cat), txt variables from given df.
    Args:
      df
      cnt_card (int or float within 0 and 1): cnt cardinarity, e.g. ratio of unique values for cnt column
      excluded_cols:list=[label]: e.g. label='response_status', the target/dependant varible column name
      txt_card (int or float within 0 and 1): txt cardinarity, e.g. ratio of unique values for txt column
    Return:
      txt_cols, cnt_cols, cat_cols, (int_cat_cols, str_cat_cols), img_cols: (list of str)
    Example:
      #for all features:
      txt_cols, cnt_cols, cat_cols, (int_cat_cols, str_cat_cols), img_cols = cnt_cat_txt_split(df, cnt_card=80, excluded_cols=[label, item_id, user_id], txt_card=0.5)
      #for item features:
      i_txt_cols, i_cnt_cols, i_cat_cols, (i_int_cat_cols, i_str_cat_cols), i_img_cols = cnt_cat_txt_img_split(df[i_cols], cnt_card=80, excluded_cols=[label, item_id], txt_card=0.5)
      #for user features:
      u_txt_cols, u_cnt_cols, u_cat_cols, (u_int_cat_cols, u_str_cat_cols), u_img_cols = cnt_cat_txt_img_split(df[u_cols], cnt_card=80, excluded_cols=[label, user_id], txt_card=0.5)

    """
    # init placeholder for cnt, cat (int_cat, str_cat), txt
    cnt_cols, cat_cols, txt_cols, img_cols, datetime_cols = [], [], [], [], []
    int_cat_cols, str_cat_cols = [], []

    # prep cnt cardinality & txt cardinality
    if cnt_card < 1:
        print(f'before adjustment...cnt_card={cnt_card}')
        cnt_card = int(df.shape[0]*cnt_card)
        print(f'before adjustment...cnt_card={cnt_card}')
    if txt_card < 1:
        print(f'before adjustment...txt_card={txt_card}')
        txt_card = int(df.shape[0]*txt_card)
        print(f'before adjustment...txt_card={txt_card}')
    # exclude target
    cols = set(df.columns) - set(excluded_cols)

    # separate cnt, cat, txt columns
    for col in cols:
        if ((pd.api.types.is_integer_dtype(df[col].dtype) and
            df[col].unique().shape[0] > cnt_card) or
            pd.api.types.is_float_dtype(df[col].dtype)): #add to cnt_cols
            cnt_cols.append(col)
        elif (pd.api.types.is_string_dtype(df[col].dtype) and
            df[col].unique().shape[0] > txt_card):
            # make sure to convert Nan(which is np numeric) to str dtype
            if all(['.png' in str(x) or '.jpg' in str(x) for x in df[col].sample(10)]): # and (all([check_url(path) for path in df[col].sample(10)]) or all([check_path(path) for path in df[col].sample(10)])): #check 10 samples to see if they are either valid url or valid path
              img_cols.append(col)
            else: #add to txt_cols
              txt_cols.append(col)
        elif pd.api.types.is_datetime64_any_dtype(df[col].dtype):
            datetime_cols.append(col)
        else: #add to cat_cols
            cat_cols.append(col)
            if pd.api.types.is_integer_dtype(df[col].dtype): #separate cat_cols into int_cat_cols and str_cat_cols
              int_cat_cols.append(col)
            else:
              str_cat_cols.append(col)
    return sorted(txt_cols), sorted(cnt_cols), sorted(cat_cols), (sorted(int_cat_cols), sorted(str_cat_cols)), sorted(img_cols), sorted(datetime_cols)


#old
def cnt_cat_txt_img_split(df:pd.DataFrame, cnt_card=20, excluded_cols:list=['response_status'], txt_card=0.5):
    """Helper function that returns column names of cnt, cat (furtherly split into int_cat, str_cat), txt variables from given df.
    Args:
      df
      cnt_card (int or float within 0 and 1): cnt cardinarity, e.g. ratio of unique values for cnt column
      excluded_cols:list=[label]: e.g. label='response_status', the target/dependant varible column name
      txt_card (int or float within 0 and 1): txt cardinarity, e.g. ratio of unique values for txt column
    Return:
      txt_cols, cnt_cols, cat_cols, (int_cat_cols, str_cat_cols), img_cols: (list of str)
    Example:
      #for all features:
      txt_cols, cnt_cols, cat_cols, (int_cat_cols, str_cat_cols), img_cols = cnt_cat_txt_split(df, cnt_card=80, excluded_cols=[label, item_id, user_id], txt_card=0.5)
      #for item features:
      i_txt_cols, i_cnt_cols, i_cat_cols, (i_int_cat_cols, i_str_cat_cols), i_img_cols = cnt_cat_txt_img_split(df[i_cols], cnt_card=80, excluded_cols=[label, item_id], txt_card=0.5)
      #for user features:
      u_txt_cols, u_cnt_cols, u_cat_cols, (u_int_cat_cols, u_str_cat_cols), u_img_cols = cnt_cat_txt_img_split(df[u_cols], cnt_card=80, excluded_cols=[label, user_id], txt_card=0.5)

    """
    # init placeholder for cnt, cat (int_cat, str_cat), txt
    cnt_cols, cat_cols, txt_cols, img_cols = [], [], [], []
    int_cat_cols, str_cat_cols = [], []

    # prep cnt cardinality & txt cardinality
    if cnt_card < 1:
        print(f'before adjustment...cnt_card={cnt_card}')
        cnt_card = int(df.shape[0]*cnt_card)
        print(f'before adjustment...cnt_card={cnt_card}')
    if txt_card < 1:
        print(f'before adjustment...txt_card={txt_card}')
        txt_card = int(df.shape[0]*txt_card)
        print(f'before adjustment...txt_card={txt_card}')
    # exclude target
    cols = set(df.columns) - set(excluded_cols)

    # separate cnt, cat, txt columns
    for col in cols:
        if ((pd.api.types.is_integer_dtype(df[col].dtype) and
            df[col].unique().shape[0] > cnt_card) or
            pd.api.types.is_float_dtype(df[col].dtype)): #add to cnt_cols
            cnt_cols.append(col)
        elif (pd.api.types.is_string_dtype(df[col].dtype) and
            df[col].unique().shape[0] > txt_card):
            # make sure to convert Nan(which is np numeric) to str dtype
            if all(['.png' in str(x) or '.jpg' in str(x) for x in df[col].sample(10)]): # and (all([check_url(path) for path in df[col].sample(10)]) or all([check_path(path) for path in df[col].sample(10)])): #check 10 samples to see if they are either valid url or valid path
              img_cols.append(col)
            else: #add to txt_cols
              txt_cols.append(col)
        else: #add to cat_cols
            cat_cols.append(col)
            if pd.api.types.is_integer_dtype(df[col].dtype): #separate cat_cols into int_cat_cols and str_cat_cols
              int_cat_cols.append(col)
            else:
              str_cat_cols.append(col)
    return sorted(txt_cols), sorted(cnt_cols), sorted(cat_cols), (sorted(int_cat_cols), sorted(str_cat_cols)), sorted(img_cols)



# Cell

#@title helpers converting df to ds
def train_test_split_tf(ds, total_size:int=100_000, test_size:float=0.2, seed:int=42):
  """randomly split ds into train_ds, test_ds after shuffling ds
  Args:
    total_size: the number of samples to sample from ds
    test_size: the proportion of data taken as test set
    seed: random seed for repeatability
  Returns:
    train_ds, test_ds
  Example:
    train_ds, test_ds = train_test_split_tf(ds=ratings,
                                  total_size=100, #randomly sample 100 samples from ds
                                  test_size=0.2, # split these 100 sample into train & test with proportion 0.8:0.2
                                  )
  """
  tf.random.set_seed(seed)
  shuffled = ds.shuffle(total_size, seed=seed, reshuffle_each_iteration=False)
  train_size = int(total_size * (1-test_size))
  train_ds = shuffled.take(train_size)
  test_ds = shuffled.skip(train_size).take(total_size-train_size)
  return train_ds, test_ds


def load_resize_image(image_path, resize=(128, 128)):
    """load image from image_path and resize it
    Inherited def preprocess_image(image_path) in https://keras.io/examples/nlp/multimodal_entailment/
    """
    extension = tf.strings.split(image_path)[-1]

    image = tf.io.read_file(image_path)
    if extension == b"jpg":
        image = tf.image.decode_jpeg(image, 3)
    else:
        image = tf.image.decode_png(image, 3)
    image = tf.image.resize(image, resize)
    return image


def preprocess_cnt_cat_txt_img(sample, resize=(128, 128), global_vars:tuple=None):
    """preprocess cnt, cat, txt, img symbolically on tensorflow dataset (tf ds).
    Args:
      sample: tf ds
      resize:2-tuple=(128, 128); images will be resized to this specification
      global_vars:tuple=None e.g. (label, user_id, item_id, x_cols, u_cols, i_cols, cnt_cols, u_cnt_cols, i_cnt_cols,cnt_cols_to_bin, u_cnt_cols_to_bin, i_cnt_cols_to_bin,bin_cnt_cols, u_bin_cnt_cols, i_bin_cnt_cols, cat_cols, u_cat_cols, i_cat_cols, txt_cols, u_txt_cols, i_txt_cols, img_cols, u_img_cols, i_img_cols, label_str2num,label_num2str, num_classes, label_keys) = global_vars

    Return:
      {**cnt_dict,**bin_cnt_dict, **cat_dict, **image_dict, **text_dict}: a dictionary of column_name:preprocessed_value pairs for all feature types

    Example:
      ds = dataframe_to_dataset(df)
      if flag_training:
          ds = ds.shuffle(len(df))
      ds = ds.map(lambda x, y: (preprocess_cnt_cat_txt_img(x), y)).cache()
    """
    # unpack global_vars
    (label, user_id, item_id, x_cols, u_cols, i_cols, cnt_cols, u_cnt_cols, i_cnt_cols,cnt_cols_to_bin, u_cnt_cols_to_bin, i_cnt_cols_to_bin,bin_cnt_cols, u_bin_cnt_cols, i_bin_cnt_cols, cat_cols, u_cat_cols, i_cat_cols, txt_cols, u_txt_cols, i_txt_cols, img_cols, u_img_cols, i_img_cols, dt_cols, u_dt_cols, i_dt_cols, label_str2num,label_num2str, num_classes, label_keys) = global_vars

    # prep cnt_cols
    cnt_dict = {col:sample[col] for col in cnt_cols}
    if len(cnt_cols_to_bin)>0:
        bin_cnt_dict = {'bin_'+col:sample[col] for col in cnt_cols_to_bin}
    else:
        bin_cnt_dict = dict()
    # prep cat_cols
    cat_dict = {col: tf.cast(sample[col], tf.dtypes.string) for col in cat_cols}
    #cat_dict = {col: [str(x) for x in sample[col]] for col in cat_cols}

    # prep img_cols
    image_dict = {img_col:load_resize_image(sample[img_col], resize) for img_col in img_cols}

    # prep txt_cols
    text_dict = {txt_col: sample[txt_col] for txt_col in txt_cols}
    return {**cnt_dict,**bin_cnt_dict, **cat_dict, **image_dict, **text_dict} #{"image_1": image_1, "image_2": image_2, "text": text}

def onehot_encode_label(X, y, num_classes:int, flag_return_y0_y=False):
  """tf one-hot encode label y (here y is integer; in case y is string, y will be mapped to integer before using this function)
  Ref: https://stackoverflow.com/questions/68347501/tensorflow-keras-using-specific-class-recall-as-metric-for-sparse-categorical-cr
  ATTENTION: one-hot-encode label <=> tf.keras.losses.CategoricalCrossentropy()
  Args:
    X: features
    y: label
    num_classes:int e.g. len(df[label].value_counts())
    flag_return_y0_y: whether or not return both original label (y0) and the onehot encoded label (y)
  Example:
    num_classes = len(df[label].value_counts())
    tf_onehot_encode_label(y=df[label], num_classes=num_classes)
  """
  y0 = tf.identity(tf.cast(y, tf.dtypes.int64)) #.copy() tensor does not
  y = tf.one_hot(y0, num_classes) # e.g. 3 is the number of classes
  if flag_return_y0_y:
    return X, y0, y
  else:
    return X, y


def df_to_ds(df:pd.DataFrame,
             flag_training:bool=False,
             batch_size:int=128,
             shuffle_size:int=32,
             flag_bin_cnt:bool=False,
             resize:tuple=(128, 128),
             #label_str2num:dict=label_str2num, #label_str2num
             flag_one_hot_encode_label=True,
             global_vars:tuple=None,
             ):
    """convert pandas df to tensorflow ds, with label column one-hot encoded.
    The outputted tf ds is the input for tf model
    Args:
      df:pd.DataFrame,
      flag_training:bool=True,
      batch_size:int=32,
      shuffle_size:int=32,
      flag_bin_cnt:bool=False,
      resize:tuple=(128, 128),
      flag_one_hot_encode_label:bool=True, whether or not do the one hot encode label. When set True, use 'CategoricalCrossentropy'
          e.g. in model.compile() use loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False)
          When set as False, use 'SparseCategoricalCrossentropy' as loss function i.e. loss = tf.keras.losses.SparseCategoricalCrossentropy
      global_vars:tuple=(label, user_id, item_id, x_cols, u_cols, i_cols, cnt_cols, u_cnt_cols, i_cnt_cols,cnt_cols_to_bin, u_cnt_cols_to_bin, i_cnt_cols_to_bin,bin_cnt_cols, u_bin_cnt_cols, i_bin_cnt_cols, cat_cols, u_cat_cols, i_cat_cols, txt_cols, u_txt_cols, i_txt_cols, img_cols, u_img_cols, i_img_cols, label_str2num,label_num2str, num_classes, label_keys)
    Returns:
      tf dataset
     """
    # unpack global vars
    (label, user_id, item_id, x_cols, u_cols, i_cols, cnt_cols, u_cnt_cols, i_cnt_cols,cnt_cols_to_bin, u_cnt_cols_to_bin, i_cnt_cols_to_bin,bin_cnt_cols, u_bin_cnt_cols, i_bin_cnt_cols, cat_cols, u_cat_cols, i_cat_cols, txt_cols, u_txt_cols, i_txt_cols, img_cols, u_img_cols, i_img_cols, dt_cols, u_dt_cols, i_dt_cols, label_str2num,label_num2str, num_classes, label_keys) = global_vars

    # work on the copy instead of the original df
    df = df.copy()

    # convert dtype from object to str & add bin cnt columns to df; also fill missing value
    for col in df.columns:
      if df[[col]].dtypes[col]==np.dtype('O'):
          df[col] = df[col].astype('str')
          df[[col]].fillna('NA', inplace=True)
      if col in cat_cols: # force cat_col to be str type, whether the original one is str or int
          df[col] = df[col].astype('str')
          df[[col]].fillna('NA', inplace=True)
      if flag_bin_cnt and col in bin_cnt_cols:
          df['bin_'+col] = df[col].astype('float') #add extra col to df copy(), so that there is real data under bin_cnt_col, not symbolic
          df[[col]].fillna(0.0, inplace=True)
      if col in cnt_cols:
          df[col] = df[col].astype('float')
          df[[col]].fillna(0.0, inplace=True)
      try: # to deal with the situation df has only one row, df.shape[0]=1
          df[col].fillna(df[col].mode()[0], inplace=True)
      except Exception:
          pass

    # convert label fr string to integer
    if df[label].dtype != int:
        df[label] = df[label].map(label_str2num)

    # divide df into X and y
    y = df.pop(label)
    X = df

    # debug: tf run out of memory
    # TBD: use TFRecord instead  https://stackoverflow.com/questions/69031604/tensorflow-running-out-of-gpu-memory-allocator-gpu-0-bfc-ran-out-of-memory-tr
    ds = tf.data.Dataset.from_tensor_slices((dict(X), y))


    #ds = DataGenerator(X, y, 32)
    if flag_one_hot_encode_label:
      ds = ds.map(lambda X, y: onehot_encode_label(X, y, num_classes=num_classes, flag_return_y0_y=False))

    """if flag_training:
        ds = ds.shuffle(buffer_size=len(df))
    # apply preprocessing asynchonously with tf.data
    ds = ds.map(lambda x, y: (preprocess_cnt_cat_txt_img(x, resize), y),#,
                num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)
    """
    if flag_training:
        # apply preprocessing asynchonously with tf.data
        ds = ds.map(lambda X, y: (preprocess_cnt_cat_txt_img(X, resize, global_vars), y),
                    num_parallel_calls=tf.data.AUTOTUNE).shuffle(shuffle_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)
    else:
        # apply preprocessing asynchonously with tf.data
        ds = ds.map(lambda x, y: (preprocess_cnt_cat_txt_img(x, resize, global_vars), y),
                    num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)


    return ds

# Cell

#title helpers converting df to balanced ds
def df_to_balanced_ds(df:pd.DataFrame,
                      flag_training:bool=False,
                      batch_size:int=128,
                      shuffle_size:int=32,
                      flag_bin_cnt:bool=False,
                      resize:tuple=(128, 128),
                      #label_str2num:dict=label_str2num, #label_str2num
                      flag_one_hot_encode_label=True,
                      global_vars:tuple=None,
                      ):
    """convert pandas df to tensorflow ds, balancing imbalanced classes with tf.data.Dataset.sample_from_datasets() resample method; with the option that the label column can be one-hot encoded.
    The outputted tf ds is the input for tf model
    Args:
      df:pd.DataFrame,
      flag_training:bool=True,
      batch_size:int=32,
      shuffle_size:int=32,
      flag_bin_cnt:bool=False,
      resize:tuple=(128, 128),
      flag_one_hot_encode_label:bool=True, whether or not do the one hot encode label. When set True, use 'CategoricalCrossentropy'
          e.g. in model.compile() use loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False)
          When set as False, use 'SparseCategoricalCrossentropy' as loss function i.e. loss = tf.keras.losses.SparseCategoricalCrossentropy
      global_vars:tuple=(label, user_id, item_id, x_cols, u_cols, i_cols, cnt_cols, u_cnt_cols, i_cnt_cols,cnt_cols_to_bin, u_cnt_cols_to_bin, i_cnt_cols_to_bin,bin_cnt_cols, u_bin_cnt_cols, i_bin_cnt_cols, cat_cols, u_cat_cols, i_cat_cols, txt_cols, u_txt_cols, i_txt_cols, img_cols, u_img_cols, i_img_cols, label_str2num,label_num2str, num_classes, label_keys)
    Returns:
      tf dataset: a tf ds dataset with balanced classes

    ATTENTION:
      Usually we only apply df_to_balanced_ds() to train_df, NOT to valid_df, test_df when we intent to test the method's effectiveness.
    Example:
      train_ds_balanced=df_to_balanced_ds(df=train_df, flag_training=True, batch_size=128, flag_bin_cnt=flag_bin_cnt, resize=(128, 128), global_vars=global_vars, flag_one_hot_encode_label=False)
    Reference:
      https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#oversampling

    Testing:
      # df to balanced ds
      train_ds_balanced=df_to_balanced_ds(df=train_df, flag_training=True, batch_size=128, flag_bin_cnt=[], resize=(128, 128), global_vars=global_vars, flag_one_hot_encode_label=False)
      #dbck expect: in the balanced dataset, the mean of target values is about 2 = (0+1+2+3+5)/5
      mean_ls = [list(train_ds_balanced.batch(batch_size).take(1))[0][-1].numpy().mean() for i in range(50)]
      print(mean_ls)
      np.mean(mean_ls)
    """
    # unpack global vars
    (label, user_id, item_id, x_cols, u_cols, i_cols, cnt_cols, u_cnt_cols, i_cnt_cols,cnt_cols_to_bin, u_cnt_cols_to_bin, i_cnt_cols_to_bin,bin_cnt_cols, u_bin_cnt_cols, i_bin_cnt_cols, cat_cols, u_cat_cols, i_cat_cols, txt_cols, u_txt_cols, i_txt_cols, img_cols, u_img_cols, i_img_cols, dt_cols, u_dt_cols, i_dt_cols, label_str2num,label_num2str, num_classes, label_keys) = global_vars

    # split df according to label's different value
    df_ls = [df[df[label]==n] for (n, s) in label_num2str.items()]
    ds_ls = [df_to_ds(df=df,
             flag_training=flag_training,
             batch_size=batch_size,
             shuffle_size=shuffle_size,
             flag_bin_cnt=flag_bin_cnt,
             resize=resize,
             #label_str2num:dict=label_str2num, #label_str2num
             flag_one_hot_encode_label=flag_one_hot_encode_label,
             global_vars=global_vars,
             ) for df in df_ls]


    weights=[1/len(label_num2str)] *len(label_num2str)
    balanced_ds = tf.data.Dataset.sample_from_datasets(ds_ls, weights=[1/len(label_num2str)] *len(label_num2str))


    return balanced_ds

# Cell

#@title helper converting ds to df

def ds_to_df(ds:tf.data.Dataset, nrows:int=10**5)->pd.DataFrame:
  """turn tf dataset into pd dataframe
  Args:
    ds:tf.data.Dataset e.g. ds1 = tfds.load('movielens/100k-ratings', split='train'), ds2 = tfds.load('movielens/100k-movies', split="train")
    nrown:int=10**5
  Returns:
    pd.DataFrame
  Example:
    ds = tfds.load('movielens/100k-ratings', split='train')
    df = ds_to_df(ds, nrows=10)
  """
  ds = ds.take(nrows)
  import pandas as pd
  return pd.DataFrame([dic for dic in tfds.as_numpy(ds)])


# Cell

#@title various text encoder modules
txt_module_url_ls = ["https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1", "https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1", "https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1", "https://tfhub.dev/google/universal-sentence-encoder/4", "https://tfhub.dev/google/universal-sentence-encoder-large/5"]
txt_module_name_ls = ['-'.join(x.split('/')[-2:]) for x in txt_module_url_ls]
txt_module_dim_ls = [20, 50, 128, 512, 512]

txt_url_dim_ls =[(url, dim) for (url, dim) in zip(txt_module_url_ls, txt_module_dim_ls)]
txt_module_dic = dict([(name, tupl) for (name, tupl) in zip(txt_module_name_ls, txt_url_dim_ls)])
#print(f'txt_module_dic={txt_module_dic}')

module_name = txt_module_name_ls[0]
module_url = txt_module_dic[module_name][0]
module_dim = txt_module_dic[module_name][1]

# Cell

#@title text encoder
def project_embeddings(embeddings, num_projection_layers, projection_dims, dropout_rate):
    projected_embeddings = tf.keras.layers.Dense(units=projection_dims)(embeddings)
    for _ in range(num_projection_layers):
        x = tf.nn.gelu(projected_embeddings)
        x = tf.keras.layers.Dense(projection_dims)(x)
        x = tf.keras.layers.Dropout(dropout_rate)(x)
        x = tf.keras.layers.Add()([projected_embeddings, x])
        projected_embeddings = tf.keras.layers.LayerNormalization()(x)
    return projected_embeddings

# OLD version for reference
def create_bert_text_encoder(num_projection_layers, projection_dims, dropout_rate, flag_trainable, txt_cols):
    # Load the pre-trained BERT model to be used as the base encoder.
    bert_model_path = (
    "https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1"
)
    bert = hub.KerasLayer(bert_model_path, name="bert", trainable=flag_trainable)

    # Receive the text as inputs.
    bert_input_features = ["input_type_ids", "input_mask", "input_word_ids"]
    inputs = {
        feature: tf.keras.Input(shape=(128,), dtype=tf.int32, name=feature)
        for feature in bert_input_features
    }

    # Generate embeddings for the preprocessed text using the BERT model.
    embeddings = bert(inputs)["pooled_output"]

    # Project the embeddings produced by the model.
    outputs = project_embeddings(
        embeddings, num_projection_layers, projection_dims, dropout_rate
    )
    # Create the text encoder model.
    return tf.keras.Model(inputs, outputs, name='_'.join(txt_cols))

def create_text_encoding_model(num_projection_layers:int=2, projection_dims:int=16, dropout_rate:float=0.2, flag_trainable:bool=False, txt_cols:list=[], module_url:str='https://tfhub.dev/google/universal-sentence-encoder/4', embed_size:int=512):
    """create a text encoding model (text encoder) by a trained module stored in tfhub; this text encoder take (multiple text columns in a df) raw text string as input and output user-defined length (e.g. 16) embedding vector
    Example:
      inputs = [
          tf.keras.layers.Input(shape=(), dtype=tf.string, name=col)
          for col in txt_cols]
      txt_encoder = create_text_encoding_model(num_projection_layers=2,
                                  projection_dims=16,
                                  dropout_rate=0.2,
                                  flag_trainable=flag_trainable,
                                  txt_cols=txt_cols,
                                  module_url=module_url,
                                  embed_size=embed_size)
      embeddings = txt_encoder(inputs)
    """
    inputs = [
        tf.keras.layers.Input(shape=(), dtype=tf.string, name=col)
        for col in txt_cols
    ]

    hub_layer = hub.KerasLayer(module_url,
                              input_shape=[], # allow arbitary text length
                              output_shape=embed_size, #it depends on what module we choose
                              dtype=tf.string, # raw text input
                              trainable=flag_trainable)
    embedding_layer_ls = [hub_layer(input) for input in inputs]
    embeddings = tf.keras.layers.Concatenate()(embedding_layer_ls)

    # Project the embeddings produced by the model.
    outputs = project_embeddings(
        embeddings, num_projection_layers, projection_dims, dropout_rate
    )
    # Create the text encoder model.
    return tf.keras.Model(inputs, outputs, name="all_"+'_'.join(txt_cols))

# Cell

#@title various image encoder modules

img_module_name_ls = ['efficientnetv2-s', 'efficientnetv2-m', 'efficientnetv2-l', 'efficientnetv2-s-21k', 'efficientnetv2-m-21k', 'efficientnetv2-l-21k', 'efficientnetv2-xl-21k', 'efficientnetv2-b0-21k', 'efficientnetv2-b1-21k', 'efficientnetv2-b2-21k', 'efficientnetv2-b3-21k', 'efficientnetv2-s-21k-ft1k', 'efficientnetv2-m-21k-ft1k', 'efficientnetv2-l-21k-ft1k', 'efficientnetv2-xl-21k-ft1k', 'efficientnetv2-b0-21k-ft1k', 'efficientnetv2-b1-21k-ft1k', 'efficientnetv2-b2-21k-ft1k', 'efficientnetv2-b3-21k-ft1k', 'efficientnetv2-b0', 'efficientnetv2-b1', 'efficientnetv2-b2', 'efficientnetv2-b3', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7',  'inception_v3', 'inception_resnet_v2', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152', 'nasnet_large', 'nasnet_mobile', 'pnasnet_large', 'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']#'bit_s-r50x1',

img_module_url_dic = {
  "efficientnetv2-s": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2",
  "efficientnetv2-m": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/feature_vector/2",
  "efficientnetv2-l": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/feature_vector/2",
  "efficientnetv2-s-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2",
  "efficientnetv2-m-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/feature_vector/2",
  "efficientnetv2-l-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/feature_vector/2",
  "efficientnetv2-xl-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2",
  "efficientnetv2-b0-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2",
  "efficientnetv2-b1-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/feature_vector/2",
  "efficientnetv2-b2-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/feature_vector/2",
  "efficientnetv2-b3-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/feature_vector/2",
  "efficientnetv2-s-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_vector/2",
  "efficientnetv2-m-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/2",
  "efficientnetv2-l-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/feature_vector/2",
  "efficientnetv2-xl-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/feature_vector/2",
  "efficientnetv2-b0-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2",
  "efficientnetv2-b1-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/feature_vector/2",
  "efficientnetv2-b2-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/feature_vector/2",
  "efficientnetv2-b3-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/feature_vector/2",
  "efficientnetv2-b0": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2",
  "efficientnetv2-b1": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/feature_vector/2",
  "efficientnetv2-b2": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/feature_vector/2",
  "efficientnetv2-b3": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/feature_vector/2",
  "efficientnet_b0": "https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1",
  "efficientnet_b1": "https://tfhub.dev/tensorflow/efficientnet/b1/feature-vector/1",
  "efficientnet_b2": "https://tfhub.dev/tensorflow/efficientnet/b2/feature-vector/1",
  "efficientnet_b3": "https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1",
  "efficientnet_b4": "https://tfhub.dev/tensorflow/efficientnet/b4/feature-vector/1",
  "efficientnet_b5": "https://tfhub.dev/tensorflow/efficientnet/b5/feature-vector/1",
  "efficientnet_b6": "https://tfhub.dev/tensorflow/efficientnet/b6/feature-vector/1",
  "efficientnet_b7": "https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1",
  "bit_s-r50x1": "https://tfhub.dev/google/bit/s-r50x1/1",
  "inception_v3": "https://tfhub.dev/google/imagenet/inception_v3/feature-vector/4",
  "inception_resnet_v2": "https://tfhub.dev/google/imagenet/inception_resnet_v2/feature-vector/4",
  "resnet_v1_50": "https://tfhub.dev/google/imagenet/resnet_v1_50/feature-vector/4",
  "resnet_v1_101": "https://tfhub.dev/google/imagenet/resnet_v1_101/feature-vector/4",
  "resnet_v1_152": "https://tfhub.dev/google/imagenet/resnet_v1_152/feature-vector/4",
  "resnet_v2_50": "https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4",
  "resnet_v2_101": "https://tfhub.dev/google/imagenet/resnet_v2_101/feature-vector/4",
  "resnet_v2_152": 'https://tfhub.dev/google/imagenet/resnet_v2_152/feature_vector/5',
  "nasnet_large": "https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/4",
  "nasnet_mobile": "https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4",
  "pnasnet_large": "https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4",
  "mobilenet_v2_100_224": "https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4",
  "mobilenet_v2_130_224": "https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/4",
  "mobilenet_v2_140_224": "https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4",
  "mobilenet_v3_small_100_224": "https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5",
  "mobilenet_v3_small_075_224": "https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5",
  "mobilenet_v3_large_100_224": "https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5",
  "mobilenet_v3_large_075_224": "https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5",
}

img_module_size_dic = {
  "efficientnetv2-s": 384,
  "efficientnetv2-m": 480,
  "efficientnetv2-l": 480,
  "efficientnetv2-b0": 224,
  "efficientnetv2-b1": 240,
  "efficientnetv2-b2": 260,
  "efficientnetv2-b3": 300,
  "efficientnetv2-s-21k": 384,
  "efficientnetv2-m-21k": 480,
  "efficientnetv2-l-21k": 480,
  "efficientnetv2-xl-21k": 512,
  "efficientnetv2-b0-21k": 224,
  "efficientnetv2-b1-21k": 240,
  "efficientnetv2-b2-21k": 260,
  "efficientnetv2-b3-21k": 300,
  "efficientnetv2-s-21k-ft1k": 384,
  "efficientnetv2-m-21k-ft1k": 480,
  "efficientnetv2-l-21k-ft1k": 480,
  "efficientnetv2-xl-21k-ft1k": 512,
  "efficientnetv2-b0-21k-ft1k": 224,
  "efficientnetv2-b1-21k-ft1k": 240,
  "efficientnetv2-b2-21k-ft1k": 260,
  "efficientnetv2-b3-21k-ft1k": 300,
  "efficientnet_b0": 224,
  "efficientnet_b1": 240,
  "efficientnet_b2": 260,
  "efficientnet_b3": 300,
  "efficientnet_b4": 380,
  "efficientnet_b5": 456,
  "efficientnet_b6": 528,
  "efficientnet_b7": 600,
  "inception_v3": 299,
  "inception_resnet_v2": 299,
  "mobilenet_v2_100_224": 224,
  "mobilenet_v2_130_224": 224,
  "mobilenet_v2_140_224": 224,
  "mobilenet_v3_small_100_224": 224,
  "mobilenet_v3_small_075_224": 224,
  "mobilenet_v3_large_100_224": 224,
  "mobilenet_v3_large_075_224":224,
  "nasnet_large": 331,
  "pnasnet_large": 331,
  "resnet_v1_50": 50,
  "resnet_v1_101": 101,
  "resnet_v1_152": 152,
  "resnet_v2_50": 50,
  "resnet_v2_101": 101,
  "resnet_v2_152": 224,
}

# create module dictionary with pattern {module_name: (module_url, module_size)}

img_module_dic = {module: (img_module_url_dic[module], img_module_size_dic[module]) for module in (set(img_module_url_dic.keys()) & set(img_module_size_dic.keys()))}
#print(img_module_dic)

img_module_name =["efficientnetv2-xl-21k",'resnet_v2_152'][1]
#print(img_module_dic[module_name])

pixels = img_module_size_dic.get(img_module_name, 224)
IMAGE_SIZE = (pixels, pixels)
#print(f"Input size {IMAGE_SIZE + (3,)}")

BATCH_SIZE = 16


# Cell

#@title image encoder

def get_img_preprocessing_model(flag_data_augmentation:bool=False):
    """return image preprocessing_model that
    1. normalize image
    2. (optional) augment image
    """
    normalization_layer = tf.keras.layers.Rescaling(1. / 255)
    img_preprocessing_model = tf.keras.Sequential([normalization_layer])
    flag_data_augmentation = False #'@param' {type:"boolean"}
    if flag_data_augmentation:
      img_preprocessing_model.add(
          tf.keras.layers.RandomRotation(40))
      img_preprocessing_model.add(
          tf.keras.layers.RandomTranslation(0, 0.2))
      img_preprocessing_model.add(
          tf.keras.layers.RandomTranslation(0.2, 0))
      # Like the old tf.keras.preprocessing.image.ImageDataGenerator(),
      # image sizes are fixed when reading, and then a random zoom is applied.
      # If all training inputs are larger than image_size, one could also use
      # RandomCrop with a batch size of 1 and rebatch later.
      img_preprocessing_model.add(
          tf.keras.layers.RandomZoom(0.2, 0.2))
      img_preprocessing_model.add(
          tf.keras.layers.RandomFlip(mode="horizontal"))
    return img_preprocessing_model



# OLD as reference: use tfhub module to create something like this
def create_image_encoder_v0(num_projection_layers, projection_dims, dropout_rate, flag_trainable, img_cols):
    # Load the pre-trained ResNet50V2 model to be used as the base encoder.
    resnet_v2 = tf.keras.applications.ResNet50V2(
        include_top=False, weights="imagenet", pooling="avg"
    )
    # Set the trainability of the base encoder.
    for layer in resnet_v2.layers:
        layer.trainable = flag_trainable

    # Receive the images as inputs.
    img_inputs = [tf.keras.Input(shape=(128, 128, 3), name=img_col) for img_col in img_cols]
    #image_2 = keras.Input(shape=(128, 128, 3), name="image_2")

    # Preprocess the input image.
    preprocessed_ls = [tf.keras.applications.resnet_v2.preprocess_input(img_input) for img_input in img_inputs]
    #preprocessed_2 = keras.applications.resnet_v2.preprocess_input(image_2)

    # Generate the embeddings for the images using the resnet_v2 model
    # concatenate them.
    embeddings_ls = [resnet_v2(preprocessed) for preprocessed in preprocessed_ls]
    #embeddings_2 = resnet_v2(preprocessed_2)
    embeddings = tf.keras.layers.Concatenate()(embeddings_ls)

    # Project the embeddings produced by the model.
    outputs = project_embeddings(
        embeddings, num_projection_layers, projection_dims, dropout_rate
    )
    # Create the vision encoder model.
    return tf.keras.Model(img_inputs, outputs, name='_'.join(img_cols))


def create_image_encoder(num_projection_layers:int=2, projection_dims:int=16, dropout_rate:float=0.2, flag_trainable:bool=False, img_cols:list=[], module_url:str="https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4"):
    # create image inputs.
    img_inputs = [tf.keras.Input(shape=IMAGE_SIZE+(3,), name=img_col) for img_col in img_cols]
    #image_2 = keras.Input(shape=(128, 128, 3), name="image_2")

    # Load the pre-trained ResNet50V2 model to be used as the base encoder.
    hub_layer = hub.KerasLayer(module_url, trainable=flag_trainable)
    embedding_layer_ls = [hub_layer(input) for input in img_inputs]
    embeddings = tf.keras.layers.Concatenate()(embedding_layer_ls)

    # Project the embeddings produced by the model.
    outputs = project_embeddings(
        embeddings, num_projection_layers, projection_dims, dropout_rate
    )
    # Create the vision encoder model.
    return tf.keras.Model(img_inputs, outputs, name='_'.join(img_cols))



# Cell

#@title encoders for cnt, cat
#-------------------------------
# to encode bucketized cnt_cols (e.g. bin_cnt_cols)
#-------------------------------
def create_bin_cnt_embedding(inputs, col, num_bins, embedding_dim):
    """get embedding for cnt col by: cnt_col => inputs[cnt_col] => discretize(inputs(cat_col)) => hash(discretize(inputs(cat_col))) => embed(hash(discretize(inputs(cat_col))))
    Example: encoded_feature = create_bin_cnt_embedding(inputs, col, num_bins, embedding_dim)
    """
    feature = tf.keras.layers.Discretization(num_bins=num_bins)(inputs[col])
    feature = tf.keras.layers.Hashing(num_bins=embedding_dim)(feature)
    encoded_feature = tf.keras.layers.Embedding(input_dim=embedding_dim, output_dim=embedding_dim)(feature)
    return encoded_feature
# define a corresponding function that does not return encoded feature but model
def create_bin_cnt_hash_embedding_model(num_bins, embedding_dim):
    """create tf model to discretize cnt features
    Example: encoded_feature = create_bin_cnt_embedding(inputs, col, num_bins, embedding_dim)
    """
    inputs = [
              tf.keras.layers.Input(shape=(), dtype=tf.float64, name=col)
              for col in bin_cnt_cols
              ]
    x = tf.keras.layers.Discretization(num_bins=num_bins)(inputs)
    x = tf.keras.layers.Hashing(num_bins=embedding_dim)(x)
    outputs = tf.keras.layers.Embedding(input_dim=embedding_dim, output_dim=embedding_dim)(x)
    return tf.keras.Model(inputs, outputs, name='_'.join(bin_cnt_cols))

#-------------------------------
# to encode cat_cols
#-------------------------------
def create_cat_encoded_feature(inputs, col, embedding_dim):
    """get embedding for cat col by: cat_col => inputs[cat_col] => embed(inputs(cat_col))
    Example: encoded_feature = create_cat_encoded_feature(inputs, col, embedding_dim)
    """

    cat_cols_vocab = {col: sorted([str(x) for x in list(df[col].unique())])}

    vocabulary = cat_cols_vocab[col]
    # Create a lookup to convert a string values to an integer indices.
    # Since we are not using a mask token nor expecting any out of vocabulary
    # (oov) token, we set mask_token to None and  num_oov_indices to 0.
    index = tf.keras.layers.StringLookup(vocabulary=vocabulary, mask_token=None, num_oov_indices=0)
    # Convert the string input values into integer indices.
    value_index = index(inputs[col])
    # Create an embedding layer with the specified dimensions
    embedding_ecoder = tf.layers.Embedding(
        input_dim=len(vocabulary), output_dim=embedding_dim
    )
    # Convert the index values to embedding representations.
    encoded_feature = embedding_ecoder(value_index)

    return encoded_feature

#Q: why it return keras.layers.embeddings.Embedding instead of KerasTensor? A: Embedding layer need to act on the hashed feature to get KerasTensor
def create_cat_hash_embedding(inputs, col, embedding_dim):
    """get embedding for cat col by: cat_col => inputs[cat_col] => hash(inputs[cat_col]) => embed(hash(cat_col))
    Example: encoded_feature = create_cat_hash_embedding(inputs, col, embedding_dim)
    """
    feature = tf.keras.layers.Hashing(num_bins=embedding_dim)(inputs[col])
    encoded_feature = tf.keras.layers.Embedding(input_dim=embedding_dim, output_dim=embedding_dim)(feature)
    return encoded_feature

# define a corresponding function that does not return encoded feature but model
def create_cat_hash_embedding_model(embedding_dim:int, cat_cols:list):
    """get embedding for cat col by: cat_col => inputs[cat_col] => hash(inputs[cat_col]) => embed(hash(cat_col))
    Example: encoded_feature = create_cat_hash_embedding(inputs, col, embedding_dim)
    """
    inputs = [tf.keras.layers.Input(
                name=col, shape=(), dtype=tf.string) for col in cat_cols]
    feature = tf.keras.layers.Hashing(num_bins=embedding_dim)(inputs)
    outputs = tf.keras.layers.Embedding(input_dim=embedding_dim, output_dim=embedding_dim)(feature)
    return tf.keras.Model(inputs, outputs, name='+'.join(cat_cols))


# Cell

#@title vsn feature selection
class GatedLinearUnit(tf.keras.layers.Layer):
    def __init__(self, units):
        super(GatedLinearUnit, self).__init__()
        self.linear = tf.keras.layers.Dense(units)
        self.softmax = tf.keras.layers.Dense(units, activation="softmax")
        self.units = units

    def call(self, inputs):
        return self.linear(inputs) * self.softmax(inputs)
    def get_config(self):
        config = super().get_config()
        config.update({
            "units": self.units,
        })
        return config

class GatedResidualNetwork(tf.keras.layers.Layer):
    def __init__(self, units, dropout_rate):
        super(GatedResidualNetwork, self).__init__()
        self.units = units
        self.dropout_rate = dropout_rate
        self.elu_dense = tf.keras.layers.Dense(units, activation="elu")
        self.linear_dense = tf.keras.layers.Dense(units)
        self.dropout = tf.keras.layers.Dropout(dropout_rate)
        self.gated_linear_unit = GatedLinearUnit(units)
        self.layer_norm = tf.keras.layers.LayerNormalization()
        self.project = tf.keras.layers.Dense(units)

    def call(self, inputs):
        x = self.elu_dense(inputs)
        x = self.linear_dense(x)
        x = self.dropout(x)
        if inputs.shape[-1] != self.units:
            inputs = self.project(inputs)
        x = inputs + self.gated_linear_unit(x)
        x = self.layer_norm(x)
        return x
    def get_config(self):
        config = super().get_config()
        config.update({
            "units": self.units,
            "dropout_rate": self.dropout_rate,
        })
        return config

class VariableSelection(tf.keras.layers.Layer):
    def __init__(self, num_features, units, dropout_rate):
        super(VariableSelection, self).__init__()
        self.grns = list()
        # Create a GRN for each feature independently
        for idx in range(num_features):
            grn = GatedResidualNetwork(units, dropout_rate)
            self.grns.append(grn)
        # Create a GRN for the concatenation of all the features
        self.grn_concat = GatedResidualNetwork(units, dropout_rate)
        self.softmax = tf.keras.layers.Dense(units=num_features, activation="softmax")
        self.units = units
        self.dropout_rate = dropout_rate
        self.num_features = num_features

    def call(self, inputs):
        v = tf.keras.layers.concatenate(inputs)
        v = self.grn_concat(v)
        v = tf.expand_dims(self.softmax(v), axis=-1)

        x = []
        for idx, input in enumerate(inputs):
            x.append(self.grns[idx](input))
        x = tf.stack(x, axis=1)

        outputs = tf.squeeze(tf.matmul(v, x, transpose_a=True), axis=1)
        return outputs

    def get_config(self):
        config = super().get_config()
        config.update({
            "units": self.units,
            "dropout_rate": self.dropout_rate,
            "num_features": self.num_features,
        })
        return config

# Cell

#@title custom learning rate scheduler
class LrRangeFinder(tf.keras.callbacks.Callback):
  """find the best learning rate range
  Example:
      lrRangeFinder = LrRangeFinder(start_lr=0.1, end_lr=2)
      model.fit(X_train, y_train,
              epochs=epochs,
              batch_size=batch_size,
              callbacks=[lrRangeFinder],
              class_weight=class_weight,
              )
  Reference: https://mancap314.github.io/cyclical-learning-rates-with-tensorflow-implementation.html
  """
  def __init__(self, start_lr, end_lr):
    super().__init__()
    self.start_lr = start_lr
    self.end_lr = end_lr

  def on_train_begin(self, logs={}):
    self.lrs = []
    self.losses = []
    tf.keras.backend.set_value(
        self.model.optimizer.lr,
        self.start_lr
    )

    n_steps = self.params['steps'] \
        if self.params['steps'] is not None \
        else round(self.params['samples'] \
                        / self.params['batch_size'])
    n_steps *= self.params['epochs']
    self.by = (self.end_lr - self.start_lr) / n_steps


  def on_batch_end(self, batch, logs={}):
    lr = float(tf.keras.backend.get_value(
                    self.model.optimizer.lr))
    self.lrs.append(lr)
    self.losses.append(logs.get('loss'))
    lr += self.by
    tf.keras.backend.set_value(self.model.optimizer.lr, lr)

class SGDRScheduler(tf.keras.callbacks.Callback):
    """Cosine annealing learning rate scheduler with periodic restarts.

    # Usage
        ```python
            schedule = SGDRScheduler(min_lr=1e-5,
                                     max_lr=1e-2,
                                     lr_decay=0.9,
                                     cycle_length=5,
                                     mult_factor=1.5)
            model.fit(X_train, Y_train,
                        epochs=100,
                        callbacks=[schedule])
        ```

    # Arguments
        min_lr: The lower bound of the learning rate range.
        max_lr: The upper bound of the learning rate range.
        lr_decay: Reduce the max_lr after
                        completion of each cycle.
                  Ex. To reduce the max_lr by 20%
                        after each cycle, set
                        this value to 0.8.
        cycle_length: Initial number of epochs in a cycle.
        mult_factor: Scale epochs_to_restart after each
                        full cycle completion.

    # References
        Original paper: http://arxiv.org/abs/1608.03983
    """
    def __init__(self,
                 min_lr,
                 max_lr,
                 lr_decay=1,
                 cycle_length=10,
                 mult_factor=2):

        self.min_lr = min_lr
        self.max_lr = max_lr
        self.lr_decay = lr_decay

        self.batch_since_restart = 0
        self.next_restart = cycle_length

        self.cycle_length = cycle_length
        self.mult_factor = mult_factor

        self.history = {}

    def clr(self):
        '''Calculate the learning rate.'''
        fraction_to_restart = self.batch_since_restart \
                / (self.steps_per_epoch * self.cycle_length)
        lr = self.min_lr + 0.5 \
            * (self.max_lr - self.min_lr) \
            * (1 + np.cos(fraction_to_restart * np.pi))
        return lr

    def on_train_begin(self, logs={}):
        '''Initialize the learning rate to the
        minimum value at the start of training.'''
        self.steps_per_epoch = self.params['steps'] \
            if self.params['steps'] is not None \
            else round(self.params['samples'] \
                        / self.params['batch_size'])
        logs = logs or {}
        tf.keras.backend.set_value(self.model.optimizer.lr,
                                    self.max_lr)

    def on_batch_end(self, batch, logs={}):
        '''Record previous batch statistics
        and update the learning rate.'''
        logs = logs or {}
        self.history.setdefault('lr', []).append(
            tf.keras.backend.get_value(
                self.model.optimizer.lr
            )
        )
        for k, v in logs.items():
            self.history.setdefault(k, []).append(v)

        self.batch_since_restart += 1
        tf.keras.backend.set_value(
            self.model.optimizer.lr,
            self.clr()
        )

    def on_epoch_end(self, epoch, logs={}):
        '''Check for end of current cycle,
        apply restarts when necessary.'''
        if epoch + 1 == self.next_restart:
            self.batch_since_restart = 0
            self.cycle_length = np.ceil(
                self.cycle_length * self.mult_factor)
            self.next_restart += self.cycle_length
            self.max_lr *= self.lr_decay
            self.best_weights = self.model.get_weights()

    def on_train_end(self, logs={}):
        '''Set weights to the values from the end of
        the most recent cycle for best performance.'''
        self.model.set_weights(self.best_weights)


# Cell

#title core functions for 1-tower model

def init_imbalanced_class_weight_bias(df:pd.DataFrame, label:str, flag_return_dict_array:bool=False):
  """To handle imbalanced classification, provide initial bias list and class weight dictionary to 2 places in a tf classifier

  1) In the last layer of classifier: tf.keras.layers.Dense(..., bias_initializer = bias_init)
  2) model.fit(train_ds, #x=dict(X_train), y=y_train,
                batch_size=batch_size,
                validation_data= valid_ds, #(dict(X_test), y_test),
                epochs=epochs,
                callbacks=callbacks,
                class_weight=class_weight,
                )
  Args:
    df:pd.DataFrame=train_df
    label:str
    flag_return_dict_array:bool=False
  Returns:
    class_weight:dict, e.g. {0: 1.6282051282051282, 1: 0.7604790419161677, 2: 0.9338235294117647}
    bias_init:K.variable(list) e.g. K.variable([0.3222079660508266, 0.1168690393701237, -0.43907701967633633])
  Examples:
    class_weight, bias_init = init_imbalanced_class_weight_bias(df=train_df, label=label)
  References:
  1. https://www.tensorflow.org/tutorials/structured_data/imbalanced_data
  2. https://stackoverflow.com/questions/60307239/setting-bias-for-multiclass-classification-python-tensorflow-keras#new-answer

  """
  from scipy.optimize import fsolve
  from math import exp
  import tensorflow.keras.backend as K

  # to deal with imbalance classification, calculate class_weight
  d = dict(df[label].value_counts())
  m = np.mean(list(d.values()))
  class_weight = {k:m/v for (k,v) in d.items()} #e.g. {0: 1.6282051282051282, 1: 0.7604790419161677, 2: 0.9338235294117647}

  # define classes frequency list
  frequency = list(list(d.values())/sum(d.values()))

  # define equations to solve initial bias
  def eqn(x, frequency=frequency):
      sum_exp = sum([exp(x_i) for x_i in x])
      return [exp(x[i])/sum_exp - frequency[i] for i in range(len(frequency))]

  # calculate init bias
  bias_init = fsolve(func=eqn,
                    x0=[0]*len(frequency),
                    )
  if flag_return_dict_array:
    return class_weight, bias_init
  else:
    return class_weight, tf.constant_initializer(bias_init) #K.variable(bias_init) #tf.keras.initializers.Constant(bias_init)

def create_model_inputs(flag_txt_bert:bool=False, global_vars:tuple=None):
    """create model inputs
    Args:
      flag_txt_bert:bool=False
    Returns:
      inputs: a dictionary of col_name:tf.keras.layers.Input() pairs
    """
    inputs = {}

    #unpack global vars
    (label, user_id, item_id, x_cols, u_cols, i_cols, cnt_cols, u_cnt_cols, i_cnt_cols,cnt_cols_to_bin, u_cnt_cols_to_bin, i_cnt_cols_to_bin,bin_cnt_cols, u_bin_cnt_cols, i_bin_cnt_cols, cat_cols, u_cat_cols, i_cat_cols, txt_cols, u_txt_cols, i_txt_cols, img_cols, u_img_cols, i_img_cols, dt_cols, u_dt_cols, i_dt_cols, label_str2num,label_num2str, num_classes, label_keys) = global_vars


    for col in x_cols:
        if col in cnt_cols+bin_cnt_cols:
            inputs[col] = tf.keras.layers.Input(
                name=col, shape=(), dtype=tf.float64
            )
        elif not flag_txt_bert and col in txt_cols:
            inputs[col] = tf.keras.Input(shape=(1,), name=col, dtype='string')
        elif col in img_cols:
            inputs[col] = tf.keras.Input(shape=(128, 128, 3), name=col)
        elif col in cat_cols:
            inputs[col] = tf.keras.layers.Input(
                name=col, shape=(), dtype=tf.string
            )
    if flag_txt_bert and len(txt_cols)>0:
        inputs['_'.join(txt_cols)] = {x: tf.keras.Input(shape=(128,), dtype=tf.int32, name=x) for x in ["input_type_ids", "input_mask", "input_word_ids"]}

    return inputs



def encode_inputs(inputs:dict,
                  embedding_dim:int=16,
                  num_projection_layers:int=3,
                  dropout_rate:float=0.2,
                  flag_trainable:bool=True,
                  flag_bin_cnt:bool=True,
                  n_bucket:int=5,
                  flag_txt_bert:bool=False,
                  flag_img_resnet:bool=False,
                  txt_module_url:str= txt_module_dic['universal-sentence-encoder-4'][0],
                  embed_size:int=txt_module_dic['universal-sentence-encoder-4'][1],
                  global_vars:tuple=None,
                  ):
    """encode inputs (dict) into encoded inputs (list)"""
    encoded_features = []
    cols = list(inputs.keys())

    #unpack global vars
    (label, user_id, item_id, x_cols, u_cols, i_cols, cnt_cols, u_cnt_cols, i_cnt_cols,cnt_cols_to_bin, u_cnt_cols_to_bin, i_cnt_cols_to_bin,bin_cnt_cols, u_bin_cnt_cols, i_bin_cnt_cols, cat_cols, u_cat_cols, i_cat_cols, txt_cols, u_txt_cols, i_txt_cols, img_cols, u_img_cols, i_img_cols, dt_cols, u_dt_cols, i_dt_cols, label_str2num,label_num2str, num_classes, label_keys)=global_vars

    for col in cols:
        #print(f'=======================col = {col}')
        if col in cnt_cols:
            #print(f'cnt_col={col}')
            # Project the numeric feature to embedding_dim using linear transformation.
            encoded_feature = tf.expand_dims(inputs[col], -1)
            encoded_feature = tf.keras.layers.Dense(units=embedding_dim)(encoded_feature)
        elif flag_bin_cnt and col in bin_cnt_cols:
            #print(f'bin_cnt_col={col}')
            #bin_encoder = create_cnt_embedding_bucketizer(df=train_df, cnt_col=col.split('bin_')[-1], n_bucket=n_bucket, embedding_dim=embedding_dim)
            #encoded_feature = tf.reshape(bin_encoder(inputs[col]), (-1, embedding_dim))
            encoded_feature = create_bin_cnt_embedding(inputs, col, num_bins=5, embedding_dim=embedding_dim)
        elif col in cat_cols:
            #print(f'cat_col={col}')
            #encoded_feature = create_cat_encoded_feature(inputs, col, embedding_dim)
            encoded_feature = create_cat_hash_embedding(inputs, col, embedding_dim)
        elif col in txt_cols:
            #print(f'txt_col={col}')
            if flag_txt_bert:
                txt_encoder = create_bert_text_encoder(num_projection_layers, projection_dims=embedding_dim, dropout_rate=dropout_rate, flag_trainable=flag_trainable, name=col)
            else:
                #txt_encoder = get_text_encoding_layer(df=df,txt_col=col,max_tokens=10000,embedding_dimding_dim)
                txt_encoder = create_text_encoding_model(num_projection_layers=2,
                                            projection_dims=16,
                                            dropout_rate=0.2,
                                            flag_trainable=flag_trainable,
                                            txt_cols=[col],
                                            module_url=txt_module_url,
                                            embed_size=embed_size)
            encoded_feature = txt_encoder(inputs[col])
        elif col in img_cols:
            #print(f'img_col={col}')
            if flag_img_resnet:
                img_encoder = create_image_encoder_v0(num_projection_layers, projection_dims=embedding_dim, dropout_rate=dropout_rate, flag_trainable=flag_trainable, name=col)
            else:
                img_encoder = create_image_encoder(num_projection_layers=2,
                                                  projection_dims=16,
                                                  dropout_rate=0.2,
                                                  flag_trainable=False,
                                                  img_cols=[col],
                                                  module_url=module_dic[module_name][0]
                                                  )
            encoded_feature = img_encoder(inputs[col])

        encoded_features.append(encoded_feature)

    return encoded_features


METRICS = [
      tf.keras.metrics.TruePositives(name='tp'),
      tf.keras.metrics.FalsePositives(name='fp'),
      tf.keras.metrics.TrueNegatives(name='tn'),
      tf.keras.metrics.FalseNegatives(name='fn'),
      tf.keras.metrics.BinaryAccuracy(name='accuracy'),
      #tf.keras.metrics.SparseCategoricalAccuracy(),
      tf.keras.metrics.Precision(name='precision'), # the percentage of predicted positives that were correctly classified, precision = TP/(TP+FP), the denominator is all the predicted positive
      tf.keras.metrics.Recall(name='recall'), #  the percentage of actual positives that were correctly classified, recall = TP/(TP+FN), where the demoninator is all the factual positive
      tf.keras.metrics.AUC(name='roc'), #auc_roc curve: the probability that a classifier will rank a random positive sample higher than a random negative sample
      tf.keras.metrics.AUC(name='prc', curve='PR'), # auc precision-recall curve: precision-recall pairs for different probability thresholds.
]

from focal_loss import SparseCategoricalFocalLoss
focal_loss = SparseCategoricalFocalLoss(gamma=2)
def create_compiled_model(num_classes:int,
                          model_type:str='vsn',
                          flag_txt_bert:bool=False,
                          flag_trainable:bool=False,
                          num_bins:int=5,
                          embedding_dim:int=16,
                          num_projection_layers:int=2,
                          dropout_rate:float=0.2,
                          flag_bin_cnt:bool=True,
                          global_vars:tuple =None,
                          bias_initializer=None,
                          metrics = METRICS,
                          loss=[tf.keras.losses.CategoricalCrossentropy(from_logits=False), SparseCategoricalFocalLoss(gamma=2),][0],
                          optimizer=[tf.keras.optimizers.Adam(learning_rate=3e-4),tf.keras.optimizers.Adadelta()][0],
                         ):
    """create a compiled keras model
    Args:
      num_classes:int=len(df[label].value_counts())
      model_type:str='vsn', TBD: to be extended to include variouse estimators
      flag_txt_bert:bool=False
      embedding_dim:int=16,
      num_projection_layers:int=2,
      dropout_rate:float=0.2,
      flag_bin_cnt:bool=True,
      metrics = METRICS,
      bias_initializer = None, if the data is imbalanced, we set the output layer's bias to reflect that, which help with initial convergence. REF: [A Recipe for Training Neural Networks: "init well"](http://karpathy.github.io/2019/04/25/recipe/#2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines)
    Returns:
      model: a compiled keras model
    """

    # create model inputs
    inputs = create_model_inputs(flag_txt_bert, global_vars=global_vars)
    print(f'=============inputs={inputs}')

    # encode inputs
    encoded_inputs = encode_inputs(inputs, embedding_dim, num_projection_layers, dropout_rate, flag_trainable, flag_bin_cnt, global_vars=global_vars)
    print(f'=============encoded_inputs={encoded_inputs}')

    #---------------------
    if model_type == 'vsn':
        num_features = len(encoded_inputs)
        # select encoded inputs by learning weight for each feature
        weighted_features = VariableSelection(num_features, embedding_dim, dropout_rate)(
            encoded_inputs
        )

        # map weighted_features to outputs by 1 dense layer
        outputs = tf.keras.layers.Dense(units=num_classes,
                                        activation="softmax",
                                        bias_initializer=bias_initializer,
                                        )(weighted_features)
        model = tf.keras.Model(inputs=inputs, outputs=outputs)
    else: #TBD: to provide options of using pre-made tf estimators
        concated_inputs = tf.keras.layers.Concatenate()(encoded_inputs)
        d1_layer = tf.keras.layers.Dense(256,activation = 'relu',
            kernel_regularizer=tf.keras.regularizers.L2(0.01))(concated_inputs)
        dropout = tf.keras.layers.Dropout(0.2)(d1_layer)
        outputs = tf.keras.layers.Dense(num_classes,
                                        activation = 'softmax',
                                        bias_initializer=bias_initializer,
                                        )(dropout)
        model = tf.keras.Model(inputs, outputs)



    model.compile(optimizer=optimizer,
                  loss=loss,
                  metrics=metrics,
                  )
    return model

def train_valid_eval_multimodal_classifier(df:pd.DataFrame,
                                            cnt_cols:list,
                                            cat_cols:list,
                                            bin_cnt_cols:list,
                                            txt_cols:list,
                                            img_cols:list,
                                            label:str,
                                            sort_split_by_col:str=None,
                                            epochs:int=100,
                                            model_path:str='/content/drive/My Drive/fast_tfrs/model/',
                                            model_name:str=None,
                                            flag_trainable:bool=False,
                                            num_bins:int=5,
                                            embedding_dim:int=16,
                                            flag_txt_bert:bool=False,
                                            num_projection_layers:int=2,
                                            dropout_rate:float=0.2,
                                            flag_bin_cnt:bool=True,
                                            model_type:str='non-vsn',
                                            train_valid_test:list=[0.7,0.15, 0.15],
                                            batch_size:int=64,
                                            max_lr=5e-3,
                                            min_lr=5e-3/(2.6**4),
                                            patience:int=30,
                                            global_vars=None,
                                            #metrics = METRICS,
                                            loss_name= ['SparseCategoricalCrossentropy',
                                                        'SparseCategoricalFocalLoss',
                                                        'CategoricalCrossentropy',#need onehotencode lable
                                                        'SigmoidFocalCrossEntropy', #need onehotencode lable
                                                        ][-4],
                                            optimizer=[tf.keras.optimizers.Adam(learning_rate=3e-4), tf.keras.optimizers.Adadelta()][0],
                                            flag_init_bias_class_weight:bool=True,
                                            flag_balance_train_df:bool=True,
                                            monitor:str=['val_prc', 'val_loss'][0],
                                            #flag_one_hot_encode_label=True,
                                            ):
    """train (using train_df), validate (using valid_df) and evaluate (using test_df) a multimodal tabular classifier
    Args:
      df:pd.DataFrame,
      cnt_cols:list=cnt_cols,
      cat_cols:list=cat_cols,
      bin_cnt_cols:list=bin_cnt_cols,
      txt_cols:list=txt_cols,
      img_cols:list=img_cols,
      label:str=label,
      sort_split_by_col:str=None, parameter used in `split_train_valid_test` to sort df and then split it into train_df, valid_df, test_df
      epochs:int=100, the maximum number of epochs of running through the whole dataset; the actual number of epochs will be determined by `early stopping` callback
      model_path:str='/content/drive/My Drive/tf_multimodal/model/',
      model_name:str=None, In case of None, the model is NOT saved
      flag_trainable:bool=False,
      num_bins:int=5, determine how many bins to bucketize the selected continuous features `cnt_cols_to_bin`
      embedding_dim:int=16,
      flag_txt_bert:bool=False,
      num_projection_layers:int=2,
      dropout_rate:float=0.2,
      flag_bin_cnt:bool=True,
      model_type:str='vsn',
      train_valid_test:list=[0.7,0.15, 0.15],
      batch_size:int=64,
      max_lr=5e-3,
      min_lr=5e-3/(2.6**4)
      patience:int=30,
      global_vars=None,
      #metrics = METRICS,  let loss function decide which metrics to use:
        - label: integers <=> loss: Sparse_xxx_loss <=> tf.keras.metrics.SparseCategoricalAccuracy()
        - label: one hot encoded <=> loss: CategoricalCrossentropy <=> METRICS

      loss_name=['SparseCategoricalFocalLoss', #need one_hot_encode label
                'SparseCategoricalCrossentropy', #need one_hot_encode label
                'CategoricalCrossentropy',
                'tfa_focal_loss',
                ][0],
      optimizer=[tf.keras.optimizers.Adam(learning_rate=3e-4), tf.keras.optimizers.Adadelta()][0],
      flag_init_bias_class_weight:bool=True, whether or not initialize bias and use class_weight
      flag_balance_train_df:bool=True, whether or not to use resampled/balanced train_ds for model training
      monitor:str=['val_prc', 'val_loss'][0],
      flag_one_hot_encode_label=True, argument in df_to_ds(), choose False when your loss is SparseCategoricalFocalLoss() or SparseCategoricalEntropy()
    Returns:
      model, history
    """
    # make sure to convert cat_cols dtype from object to str
    for col in cat_cols:
        df[col] = df[col].astype(str)

    # calculate number of classes
    num_classes = len(df[label].value_counts())

    # split df into train_df, valid_df, test_df. NOTE: to random split, set `sort_split_by_col=None`
    train_df, valid_df, test_df = split_train_valid_test(df, train_valid_test=train_valid_test, target=label, random_state=123, sort_split_by_col=sort_split_by_col)
    print(f'=======train_df.shape={train_df.shape},valid_df.shape={valid_df.shape}, test_df.shape={test_df.shape}========')

    # define loss_dict which maps loss_name to real loss function
    loss_dict = {'SparseCategoricalFocalLoss':SparseCategoricalFocalLoss(gamma=2),
                 'SparseCategoricalCrossentropy':tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                 'CategoricalCrossentropy': tf.keras.losses.CategoricalCrossentropy(from_logits=False),
                 'SigmoidFocalCrossEntropy': tfa.losses.SigmoidFocalCrossEntropy(from_logits = False,
                                                            alpha= 0.25,
                                                            gamma= 2.0,
                                                            reduction=tf.keras.losses.Reduction.AUTO,
                                                            name = 'sigmoid_focal_crossentropy'
                                                            ),
                 }
    if loss_name in ['CategoricalCrossentropy','SigmoidFocalCrossEntropy']:
      flag_one_hot_encode_label=True
      metrics=METRICS
    else: # loss_name in ['SparseCategoricalCrossentropy','SparseCategoricalFocalLoss',, ]
      flag_one_hot_encode_label=False
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]

    # df to ds
    if flag_balance_train_df: # use balanced train_ds by resampling method
      train_ds=df_to_balanced_ds(df=train_df, flag_training=True, batch_size=batch_size, flag_bin_cnt=flag_bin_cnt, resize=(128, 128), flag_one_hot_encode_label=flag_one_hot_encode_label, global_vars=global_vars)
    else: # use imbalanced train_ds without using resampling method
      train_ds=df_to_ds(df=train_df, flag_training=True, batch_size=batch_size, flag_bin_cnt=flag_bin_cnt, resize=(128, 128), flag_one_hot_encode_label=flag_one_hot_encode_label, global_vars=global_vars)
    valid_ds=df_to_ds(df=valid_df, flag_training=False, batch_size=batch_size, flag_bin_cnt=flag_bin_cnt, resize=(128, 128), flag_one_hot_encode_label=flag_one_hot_encode_label, global_vars=global_vars)
    test_ds=df_to_ds(df=test_df, flag_training=False, batch_size=batch_size, flag_bin_cnt=flag_bin_cnt, resize=(128, 128), flag_one_hot_encode_label=flag_one_hot_encode_label, global_vars=global_vars)

    print(f'+++++++++++++++dbck: lable is one-hot-encoded : {flag_one_hot_encode_label}++++++++++++')
    print(ds_to_df(ds=train_ds, nrows=7))

    # to handle imbalanced classification issue, calculate class_weight, bias_init
    class_weight, bias_init = init_imbalanced_class_weight_bias(df=train_df, label=label)
    if not flag_init_bias_class_weight:
        class_weight = {k:1 for (k,v) in class_weight.items()} # assign the same weight 1 to different classes
        bias_init = tf.constant_initializer(np.array([0 for (k,v) in class_weight.items()])) # assign 0s to bias
        #print(f'======class_weight = {class_weight}\n======bias_init={bias_init}')
    #tmp for debugging
    #bias_init = 'zeros' #tf.keras.initializers.HeNormal()

    # create complied model
    model = create_compiled_model(num_classes,
                                  model_type=model_type,
                                  flag_txt_bert=flag_txt_bert,
                                  flag_trainable=flag_trainable,
                                  num_bins=num_bins,
                                  embedding_dim=embedding_dim,
                                  num_projection_layers=num_projection_layers,
                                  dropout_rate=dropout_rate,
                                  flag_bin_cnt=flag_bin_cnt,
                                  global_vars=global_vars,
                                  bias_initializer=bias_init,
                                  metrics = metrics, #METRICS
                                  loss=loss_dict[loss_name], #[tf.keras.losses.CategoricalCrossentropy(from_logits=False), SparseCategoricalFocalLoss(gamma=2),][0],
                                  optimizer=optimizer, #[tf.keras.optimizers.Adam(learning_rate=3e-4), tf.keras.optimizers.Adadelta()][0],
                                  )



    if flag_balance_train_df:
      scheduler_ls = []
    else:
      scheduler_ls = [SGDRScheduler(min_lr=min_lr,
                                      max_lr=max_lr,
                                      lr_decay=.9,
                                      cycle_length=3,
                                      mult_factor=1.5)]

    mode_dict={'val_prc':'max', 'val_loss':'min'}
    callbacks = [
                tf.keras.callbacks.EarlyStopping(     #EarlyStopping is used to stop at the epoch where val_accuracy does not improve significantly
                    monitor=monitor,
                    min_delta=1e-4,
                    patience=patience,
                    mode=mode_dict[monitor],
                    verbose=1,
                    restore_best_weights=True,
                ),
                tf.keras.callbacks.ModelCheckpoint(
                    filepath=model_path, #'weights.h5',
                    monitor=monitor,
                    mode=mode_dict[monitor],
                    save_best_only=True,
                    save_weights_only=True,
                    verbose=1,
                    ),
                #sgd_scheduler, # NOT working for flag_balance_train_df=True,
                ] + scheduler_ls
    print(f'===========class_weight ={class_weight}, \n==========bias_init ={bias_init}')
    history = model.fit(x=train_ds, #x=dict(X_train), y=y_train,
                        batch_size=batch_size,
                        validation_data= valid_ds, #(dict(X_test), y_test),
                        epochs=epochs,
                        callbacks=callbacks,
                        class_weight=class_weight,
                        )
    test_metrics = model.evaluate(test_ds, verbose=False, return_dict=True)
    print(f'==========test_metrics = {test_metrics}==========')

    #model.load_weights('weights.h5')
    if model_name is not None:
      model.save(model_path+model_name+'.hdf5')
    return model, history


# Cell

#@title helpers for performance & error analysis

def get_mispredicted_samples(y_pred, test_df, label, label_num2str, label_val):
    """get the mispredicted samples (samples where the model prediction != sample's true lable), such as
    false positive samples: df_fp = get_falsly_predicted_samples(y_true=y_test, y_pred=preds0, true_val=0) #here 0 is the label value regarded by user as negative
    false negative samples: df_fn = get_falsly_predicted_samples(y_true=y_test, y_pred=preds0, true_val=1) #here 1 is the label value regarded by user as positive
    Args:
      y_pred:np.array, the predictions by a model; y_pred has numeric values, not string
      test_df:pd.DataFrame a df with label
      label:str,  label column name e.g. 'response_status'
      label_num2str:dict a dictionary mapping num to label string
      label_val: a possible value of label e.g. -1 in {-1,0,1} here; or 'accepted' in {'declined', 'no action', 'accepted'}
    Returns:
      df
    Example:
      dls = multimodal_classifier.tab_clf.dls
      label_str2num = {k: dls.vocab.o2i[k] for k in list(dls.vocab.o2i)}
      label_num2str = {val:key for (key,val) in label_str2num.items()}
      get_mispredicted_samples(y_pred=preds0, test_df=test_df, label=label, label_num2str=label_num2str, label_val=-1)

    """
    y_true= test_df[label]
    y_pred_label = pd.Series(y_pred) #.map(label_num2str)
    df1 = pd.DataFrame(zip(y_true, y_pred_label), columns=['y_true', 'y_pred'], index=test_df.index)
    df2=pd.concat([df1, test_df],axis=1)
    return df2[(df2['y_true']==label_val) * (df2['y_pred']!=df2['y_true'])]

def get_performance_metrics(y_true, y_prob, y_pred=None, labels:list=[0,1,2], normalize:str=None, **kwargs):
    """a wrapper to show common performance scores of a ML model
    Args:
      y_true,
      y_prob,
      y_pred=None,
      label_str2num:dict=None, a dictionary mapping label str to its numberic representation in ML model
      **kwargs
      normalize : {'true', 'pred', 'all'}, default=None Normalizes confusion matrix over the true (rows), predicted (columns) conditions or all the population. If None, confusion matrix will not be normalized.
    Returns:
      clf_report, cm_df, macro_roc_auc_ovo, weighted_roc_auc_ovo, macro_roc_auc_ovr, weighted_roc_auc_ovr

    """
    import seaborn as sns
    from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix

    if y_pred is None:
      y_pred = y_prob.argmax(axis=1)

    # Creating  a confusion matrix,which compares the y_test and y_pred
    #ATTN: in confusion_matrix() API, If labels = None is given, those that appear at least once in y_true or y_pred are used in sorted order.
    #e.g. confusion matrix is organized in such a way that cols:Left(low-value) to Right(high-value), rows:up(low-value) to down(high-value)

    cm = confusion_matrix(y_true, y_pred, normalize=normalize, labels=labels)

    # Creating a dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.

    """if labels is None: #in case labels is not provided by user
      labels = list(label_str2num.values())
      labels.sort()

    labels_str = [label_num2str[v] for v in labels]"""

    cm_df = pd.DataFrame(cm,
                     index = labels,# e.g.  ['SETOSA','VERSICOLR','VIRGINICA']
                     columns =labels)

    #Plotting the confusion matrix
    plt.figure(figsize=(5,4))
    sns.heatmap(cm_df,  cmap="YlGnBu", annot=True)
    plt.title('Confusion Matrix')
    plt.ylabel('Actal Values')
    plt.xlabel('Predicted Values')
    plt.show()

    # create classification report
    clf_report = classification_report(y_true, y_pred, **kwargs)
    print(f"=============classification report===============\n")
    print(clf_report)

    # make various kinds of roc_auc
    macro_roc_auc_ovo = roc_auc_score(y_true, y_prob, multi_class="ovo", average="macro")

    weighted_roc_auc_ovo = roc_auc_score(
        y_true, y_prob, multi_class="ovo", average="weighted"
    )

    macro_roc_auc_ovr = roc_auc_score(y_true, y_prob, multi_class="ovr", average="macro")

    weighted_roc_auc_ovr = roc_auc_score(
        y_true, y_prob, multi_class="ovr", average="weighted"
    )
    print(
        "One-vs-One ROC AUC scores:\n{:.6f} (macro),\n{:.6f} "
        "(weighted by prevalence)".format(macro_roc_auc_ovo, weighted_roc_auc_ovo)
    )
    print(
        "One-vs-Rest ROC AUC scores:\n{:.6f} (macro),\n{:.6f} "
        "(weighted by prevalence)".format(macro_roc_auc_ovr, weighted_roc_auc_ovr)
    )
    return (clf_report, cm_df , macro_roc_auc_ovo, weighted_roc_auc_ovo, macro_roc_auc_ovr, weighted_roc_auc_ovr)

def plot_loss(history, txt_label:str, n:int):
  # Use a log scale on y-axis to show the wide range of values.
  plt.semilogy(history.epoch, history.history['loss'],
               color=colors[n], label='Train ' + txt_label)
  plt.semilogy(history.epoch, history.history['val_loss'],
               color=colors[n], label='Val ' + txt_label,
               linestyle="--")
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.legend()

def plot_roc(name:str, y_true:list, y_prob:np.array, **kwargs):
  """plot ROC curve for each category for a multiclass classifier

  REF: https://stackoverflow.com/questions/56090541/how-to-plot-precision-and-recall-of-multiclass-classifier
  """
  from sklearn.metrics import roc_curve
  fpr = dict()
  tpr = dict()

  for i in range(y_prob.shape[1]):
      fpr[i], tpr[i], _ = roc_curve(y_true[:, i],
                                    y_prob[:, i])
      plt.plot(fpr[i], tpr[i], lw=2, label='class {}'.format(i))

  plt.xlabel("false positive rate")
  plt.ylabel("true positive rate")
  plt.legend(loc="best")
  plt.title("ROC curve")
  plt.show()

def plot_prc(name:str, y_true:list, y_prob:np.array, **kwargs):
    """plot precision recall curve for each category for a multiclass classifier

    REF: https://stackoverflow.com/questions/56090541/how-to-plot-precision-and-recall-of-multiclass-classifier
    """
    from sklearn.metrics import precision_recall_curve
    precision = dict()
    recall = dict()
    for i in range(y_prob):
        precision[i], recall[i], _ = precision_recall_curve(y_true[:, i],
                                                            y_prob[:, i])
        plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(i))

    plt.xlabel("recall")
    plt.ylabel("precision")
    plt.legend(loc="best")
    plt.title("precision vs. recall curve")
    plt.show()


# Cell

#title 2-towers functions

def create_deep_cross_layers(layer_sizes:list=[128, 64, 32],
                             projection_dim:int=32,
                             l1_l2:int=0,
                             alpha:float=1e-4,
                             dropout_rate:float=0.2,
                             name:str='last_deepcross_layer',
                             flag_dcn_cross:bool=True,
                             )->tf.keras.Sequential:
    """create deep_cross_layers, which is a tf.keras.Sequential() model
    Args:
        layer_sizes (list of int): e.g. [128, 64, 32]
        projection_dim (int or None): e.g. 32
        l1_l2 (int): choose from {0, 1, 2}, 0: not using l1/l2 regularization, 1: use l1, 2: use l2
        alpha (float): l1 or l2 regularization parameter, usually is a small number e.g. 1e-2
        dropout_rate (float): drop out proportion, usually a small number between 0 and .5.
    Returns:
        deep_cross_layers (tf.keras.Sequential() model)
        - input for deep_cross_layers:tf concat embedding_list/encoded_inputs
        - output for deep_cross_layers: embedding for user/item

    """
    # construct the cross layers.
    if flag_dcn_cross:
        deep_cross_layers = tf.keras.Sequential(tfrs.layers.dcn.Cross(projection_dim=projection_dim,
                                                                  kernel_initializer="glorot_uniform"))
    else:
        deep_cross_layers = tf.keras.Sequential()

    # Use the ReLU activation for all but the last layer.
    regu_dict = {0:regularizers.l1(0), 1: regularizers.l1(alpha), 2:regularizers.l2(alpha)}
    for layer_size in layer_sizes[:-1]:
        deep_cross_layers.add(tf.keras.layers.Dense(layer_size,
                                                activation="relu",
                                                activity_regularizer=regu_dict[l1_l2])) # to deal with overfitting, use l1 or l2 regularization
        deep_cross_layers.add(Dropout(dropout_rate)) # to deal with overfitting, use dropout

    # No activation for the last layer.
    deep_cross_layers.add(tf.keras.layers.Dense(layer_sizes[-1], name=name))

    return deep_cross_layers

def create_x_model_inputs(flag_txt_bert:bool,
                          x_cols:list,
                          x_cnt_cols:list,
                          x_bin_cnt_cols:list,
                          x_cat_cols:list,
                          x_txt_cols:list,
                          x_img_cols:list):
    """create x(user|item) model inputs
    Args:
      flag_txt_bert:bool=False
    Returns:
      inputs: a dictionary of col_name:tf.keras.layers.Input() pairs
    """
    inputs = {}

    for col in x_cols:
        if col in x_cnt_cols+x_bin_cnt_cols:
            inputs[col] = tf.keras.layers.Input(
                name=col, shape=(), dtype=tf.float64
            )
        elif not flag_txt_bert and col in x_txt_cols:
            inputs[col] = tf.keras.Input(shape=(1,), name=col, dtype='string')
        elif col in x_img_cols:
            inputs[col] = tf.keras.Input(shape=(128, 128, 3), name=col)
        elif col in x_cat_cols:
            inputs[col] = tf.keras.layers.Input(
                name=col, shape=(), dtype=tf.string
            )
    if flag_txt_bert and len(x_txt_cols)>0:
        inputs['_'.join(x_txt_cols)] = {x: tf.keras.Input(shape=(128,), dtype=tf.int32, name=x) for x in ["input_type_ids", "input_mask", "input_word_ids"]}

    return inputs

# get user(item) information layers, including, inputs, encoded_inputs, weighted_features, embeddings, compiled model
def create_x_compiled_model(num_classes:int,
                            model_type:str='vsn',
                            flag_txt_bert:bool=False,
                            flag_trainable:bool=False,
                            num_bins:int=5,
                            embedding_dim:int=16,
                            num_projection_layers:int=2,
                            dropout_rate:float=0.2,
                            flag_bin_cnt:bool=True,
                            ####user|item features info####
                            x_name:str=None,#['user', 'item'][0]
                            x_cols:list=None,#u_cols,
                            x_cnt_cols:list=None,#u_cnt_cols,
                            x_bin_cnt_cols:list=None,#u_bin_cnt_cols,
                            x_cat_cols:list=None, #u_cat_cols,
                            x_txt_cols:list=None, #u_txt_cols,
                            x_img_cols:list=None, #u_img_cols,

                            ### newly added
                            global_vars:tuple=None,#global_vars,
                            bias_initializer=None,
                            metrics=None,
                            loss=None,
                            optimizer=[tf.keras.optimizers.Adam(learning_rate=3e-4),tf.keras.optimizers.Adadelta()][0],

                          ):
    """get user(item)'s information layers, including, inputs, encoded_inputs, weighted_features, embeddings, compiled model

    Args:
      num_classes:int=len(df[label].value_counts())
      model_type:str='vsn'
      flag_txt_bert:bool=False
      embedding_dim:int=16,
      num_projection_layers:int=2,
      dropout_rate:float=0.2,
      flag_bin_cnt:bool=True,
      ####user|item features info####
      x_cols=u_cols, x_cnt_cols=u_cnt_cols, x_bin_cnt_cols=u_bin_cnt_cols, x_cat_cols=u_cat_cols, x_txt_cols=u_txt_cols, x_img_cols=u_img_cols

      ### newly added
      global_vars:tuple=None,#global_vars,
      bias_initializer=None,
      metrics = METRICS,
      loss=[tf.keras.losses.CategoricalCrossentropy(from_logits=False), SparseCategoricalFocalLoss(gamma=2),][0],
      optimizer=[tf.keras.optimizers.Adam(learning_rate=3e-4),tf.keras.optimizers.Adadelta()][0],

    Returns:
      inputs: a list of input
      embs:
    """

    # create user|item model inputs
    inputs = create_x_model_inputs(flag_txt_bert, x_cols, x_cnt_cols, x_bin_cnt_cols, x_cat_cols, x_txt_cols, x_img_cols)
    #print(f'=============inputs={inputs}')

    # encode user|item inputs
    encoded_inputs = encode_inputs(inputs, embedding_dim, num_projection_layers, dropout_rate, flag_trainable, flag_bin_cnt, global_vars=global_vars)
    #print(f'=============encoded_inputs={encoded_inputs}')

    #---------------------
    if model_type == 'vsn':
        num_features = len(encoded_inputs)
        # select encoded inputs by learning weight for each feature
        vsn = VariableSelection(num_features, embedding_dim, dropout_rate)
        vsn._name = x_name+'_vsn' #assign a unique name so that I can extract its embeddings
        features_embs = vsn(encoded_inputs)

        """# map weighted_features by a dense layer to get user features embedding
        features_embs = tf.keras.layers.Dense(units=embedding_dim,
                                              activation="linear",
                                              name=x_name+"_embedding_layer",
                                              use_bias=False)(weighted_features)"""

        # map features embedding by a dense layer to get classification output
        outputs = tf.keras.layers.Dense(units=num_classes,
                                        activation='softmax',
                                        bias_initializer=bias_initializer,
                                        )(features_embs)

        model = tf.keras.Model(inputs, outputs)
    else: #TBD: to provide options of using pre-made tf estimators
        concated_inputs = tf.keras.layers.Concatenate()(encoded_inputs)
        features_embs = tf.keras.layers.Dense(256,
                                              activation = 'relu',
                                              kernel_regularizer=tf.keras.regularizers.L2(0.01),
                                              name=x_name[0]+'_emb',
                                              )(concated_inputs)
        dropout = tf.keras.layers.Dropout(0.2)(features_embs)
        outputs = tf.keras.layers.Dense(units=num_classes,
                                        activation = 'softmax',
                                        bias_initializer=bias_initializer,
                                        )(dropout)
        model = tf.keras.Model(inputs, outputs)

    model.summary()

    model.compile(optimizer=optimizer,
                  loss=loss, #tf.keras.losses.CategoricalCrossentropy(from_logits=False), # change fr. from_logits=True to deal with warning message
                  metrics=metrics,
                  )
    return model, features_embs

def create_iu_compiled_model( global_vars:tuple,
                              model_type:str='non-vsn',
                              flag_trainable:bool=True,
                              num_bins:int=5,
                              embedding_dim:int=16,
                              flag_txt_bert:bool=False,
                              num_projection_layers:int=2,
                              dropout_rate:float=0.2,
                              flag_bin_cnt:bool=True,

                              layer_sizes:list=[128, 64, 32],
                              projection_dim:int=32,
                              l1_l2:int=1,
                              alpha:float=1e-4,
                              ### newly added @06/17/2022
                              bias_initializer=None,
                              #metrics = None,
                              #loss=None,
                              loss_name=['CategoricalCrossentropy','SigmoidFocalCrossEntropy', 'SparseCategoricalCrossentropy','SparseCategoricalFocalLoss'][0],
                              optimizer=[tf.keras.optimizers.Adam(learning_rate=3e-4), tf.keras.optimizers.Adadelta()][0],
                              flag_dcn_cross=False,
                              ):
    """create a user-item 2 towers multimodal tabular classifier
    Args:
      global_vars:tuple,
      #num_classes:int,
      #cnt_cols:list,
      #cat_cols:list,
      #bin_cnt_cols:list,
      #txt_cols:list,
      #img_cols:list,

      flag_trainable:bool=True,
      num_bins:int=5,
      embedding_dim:int=16,
      flag_txt_bert:bool=False,
      num_projection_layers:int=2,
      dropout_rate:float=0.2,
      flag_bin_cnt:bool=True,

      layer_sizes:list=[128, 64, 32],
      projection_dim:int=32,
      l1_l2:int=2,
      alpha:float=1e-4,
      flag_dcn_cross:bool=False,
    Returns:
      model: a 2towers model compiled (with optimizer)
    """
    # unpack global vars
    (label, user_id, item_id, x_cols, u_cols, i_cols, cnt_cols, u_cnt_cols, i_cnt_cols,cnt_cols_to_bin, u_cnt_cols_to_bin, i_cnt_cols_to_bin,bin_cnt_cols, u_bin_cnt_cols, i_bin_cnt_cols, cat_cols, u_cat_cols, i_cat_cols, txt_cols, u_txt_cols, i_txt_cols, img_cols, u_img_cols, i_img_cols, dt_cols, u_dt_cols, i_dt_cols, label_str2num,label_num2str, num_classes, label_keys) = global_vars

    ################ item #######################
    # create item model inputs
    i_inputs = create_x_model_inputs(flag_txt_bert, i_cols, i_cnt_cols, i_bin_cnt_cols, i_cat_cols, i_txt_cols, i_img_cols)
    #print(f'=============i_inputs={i_inputs}')

    # encode item inputs
    i_encoded_inputs = encode_inputs(i_inputs, embedding_dim, num_projection_layers, dropout_rate, flag_trainable, flag_bin_cnt, global_vars=global_vars)
    #print(f'=============i_encoded_inputs={i_encoded_inputs}')

    #---------------------
    if model_type == 'vsn':
        i_num_features = len(i_encoded_inputs)
        # select encoded inputs by learning weight for each feature
        #i_embs = VariableSelection(i_num_features, embedding_dim, dropout_rate)(i_encoded_inputs)
        i_vsn = VariableSelection(i_num_features, embedding_dim, dropout_rate)
        i_vsn._name = 'i_emb' #assign a unique name so that I can extract its embeddings
        i_embs = i_vsn(i_encoded_inputs)
    else:
        i_concated_inputs = tf.keras.layers.Concatenate()(i_encoded_inputs)
        i_features_embs = tf.keras.layers.Dense(256,
                                                activation = 'relu',
                                                kernel_regularizer=tf.keras.regularizers.L2(0.01),
                                                name='i_emb')(i_concated_inputs)
        i_embs = tf.keras.layers.Dropout(0.2)(i_features_embs)


    ################ user #######################
    # create user model inputs
    u_inputs = create_x_model_inputs(flag_txt_bert, u_cols, u_cnt_cols, u_bin_cnt_cols, u_cat_cols, u_txt_cols, u_img_cols)
    #print(f'=============u_inputs={u_inputs}')

    # encode user inputs
    u_encoded_inputs = encode_inputs(u_inputs, embedding_dim, num_projection_layers, dropout_rate, flag_trainable, flag_bin_cnt, global_vars=global_vars)
    #print(f'=============u_encoded_inputs={u_encoded_inputs}')

    if model_type == 'vsn':
        u_num_features = len(u_encoded_inputs)
        # select encoded inputs by learning weight for each feature
        #u_embs = VariableSelection(u_num_features, embedding_dim, dropout_rate)(u_encoded_inputs)
        u_vsn = VariableSelection(u_num_features, embedding_dim, dropout_rate)
        u_vsn._name = 'u_emb' #assign a unique name so that I can extract its embeddings
        u_embs = u_vsn(u_encoded_inputs)
    else:
        u_concated_inputs = tf.keras.layers.Concatenate()(u_encoded_inputs)
        u_features_embs = tf.keras.layers.Dense(256,
                                                activation = 'relu',
                                                kernel_regularizer=tf.keras.regularizers.L2(0.01),
                                                name='u_emb',
                                                )(u_concated_inputs)
        u_embs = tf.keras.layers.Dropout(0.2)(u_features_embs)
    iu_embs = tf.keras.layers.concatenate([i_embs, u_embs], name='iu_emb')

    # to enable effective feature crossing, add deep cross layers
    deep_cross_layers = create_deep_cross_layers(layer_sizes,
                                                 projection_dim,
                                                 l1_l2,
                                                 alpha,
                                                 dropout_rate,
                                                 name='iu_emb',
                                                 flag_dcn_cross=flag_dcn_cross)
    iu_embs = deep_cross_layers(iu_embs)

    # map features embedding by a dense layer to get classification output
    outputs = tf.keras.layers.Dense(units=num_classes,
                                    activation='softmax',
                                    bias_initializer=bias_initializer,
                                    )(iu_embs)
    # i_inputs is a list, u_inputs is another list, to combine the two, just list1+list2
    model = tf.keras.Model(inputs={**i_inputs, **u_inputs}, outputs=outputs)

    print('========================item-user 2 towers model.summary()=======================')
    model.summary()
    tf.keras.utils.plot_model(model, show_shapes=True)


    # define loss_dict which maps loss_name to real loss function
    loss_dict = {'SparseCategoricalFocalLoss':SparseCategoricalFocalLoss(gamma=2),
                 'SparseCategoricalCrossentropy':tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                 'CategoricalCrossentropy': tf.keras.losses.CategoricalCrossentropy(from_logits=False),
                 'SigmoidFocalCrossEntropy': tfa.losses.SigmoidFocalCrossEntropy(from_logits = False,
                                                            alpha= 0.25,
                                                            gamma= 2.0,
                                                            reduction=tf.keras.losses.Reduction.AUTO,
                                                            name = 'sigmoid_focal_crossentropy'
                                                            ),
                 }
    if loss_name in ['CategoricalCrossentropy','SigmoidFocalCrossEntropy']:
      flag_one_hot_encode_label=True
      metrics=METRICS
    else: # loss_name in ['SparseCategoricalCrossentropy','SparseCategoricalFocalLoss', ]
      flag_one_hot_encode_label=False
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]



    model.compile(optimizer=optimizer,
                  loss=loss_dict[loss_name], #tf.keras.losses.CategoricalCrossentropy(from_logits=True),#loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=metrics,
                  )
    # add config attribute to model
    model.config = {'num_classes':num_classes,
                    'cnt_cols':cnt_cols,
                    'cat_cols':cat_cols,
                    'bin_cnt_cols':bin_cnt_cols,
                    'txt_cols':txt_cols,
                    'img_cols':img_cols,

                    'flag_trainable':flag_trainable,
                    'num_bins':num_bins,
                    'embedding_dim':embedding_dim,
                    'flag_txt_bert':flag_txt_bert,
                    'num_projection_layers':num_projection_layers,
                    'dropout_rate':dropout_rate,
                    'flag_bin_cnt':flag_bin_cnt,

                    'layer_sizes':layer_sizes,
                    'projection_dim':projection_dim,
                    'l1_l2':l1_l2,
                    'alpha':alpha,
                    'bias_initializer':bias_initializer,
                    'clf_metrics': metrics,
                    'clf_loss_name':loss_name,
                    'clf_optimizer':optimizer,
                    'global_vars':global_vars,
                    'model_type':model_type,
                    'flag_dcn_cross':flag_dcn_cross,
                    }
    return model

def train_valid_eval_iu_classifier(df:pd.DataFrame,
                                    cnt_cols:list,
                                    cat_cols:list,
                                    bin_cnt_cols:list,
                                    txt_cols:list,
                                    img_cols:list,
                                    label:str,
                                    sort_split_by_col:str=None,#'start_datetime',
                                    epochs:int=10,
                                    model_path:str=None,#'/content/drive/My Drive/fast_tfrs/model/',
                                    model_name:str=None,#'iu_2towers_model',
                                    flag_trainable:bool=True,
                                    num_bins:int=5,
                                    embedding_dim:int=16,
                                    flag_txt_bert:bool=False,
                                    num_projection_layers:int=2,
                                    dropout_rate:float=0.2,
                                    flag_bin_cnt:bool=True,
                                    model_type:str='non-vsn',
                                    train_valid_test:list=[0.7,0.15, 0.15],
                                    batch_size:int=64,
                                    max_lr:float=5e-3,
                                    min_lr:float=5e-3/(2.6**4),
                                    layer_sizes:list=[128, 64, 32],
                                    projection_dim:int=32,
                                    l1_l2:int=2,
                                    alpha:float=1e-4,
                                    patience=5,
                                    #newly added @ 06/17/2022
                                    #bias_initializer=bias_init,
                                    global_vars:tuple=None,
                                    #metrics = METRICS,
                                    loss_name= ['SparseCategoricalCrossentropy',
                                                'SparseCategoricalFocalLoss',
                                                'CategoricalCrossentropy',#need onehotencode label
                                                'SigmoidFocalCrossEntropy', #need onehotencode label
                                                ][-4],
                                    optimizer=[tf.keras.optimizers.Adam(learning_rate=3e-4), tf.keras.optimizers.Adadelta()][0],
                                    flag_init_bias_class_weight:bool=True, #using neither bias initialization nor class_weight
                                    flag_balance_train_df:bool=True, #do not use resampled/balanced train_ds for model training
                                    flag_SGDRScheduler:bool=False,
                                    monitor:str=['val_prc', 'val_loss'][0],
                                    flag_dcn_cross=True, #<========new
                                    ):
    """train (using train_df), validate (using valid_df) and evaluate (using test_df) a user-item 2 towers multimodal tabular classifier
    Args:
      df:pd.DataFrame,
      cnt_cols:list=cnt_cols,
      cat_cols:list=cat_cols,
      bin_cnt_cols:list=bin_cnt_cols,
      txt_cols:list=txt_cols,
      img_cols:list=img_cols,
      label:str=label,
      sort_split_by_col:str=None, parameter used in `split_train_valid_test` to sort df and then split it into train_df, valid_df, test_df
      epochs:int=100, the maximum number of epochs of running through the whole dataset; the actual number of epochs will be determined by `early stopping` callback
      model_path:str='/content/drive/My Drive/tf_multimodal/model/',
      model_name:str=None, In case of None, the model is NOT saved
      flag_trainable:bool=False,
      num_bins:int=5, determine how many bins to bucketize the selected continuous features `cnt_cols_to_bin`
      embedding_dim:int=16,
      flag_txt_bert:bool=False,
      num_projection_layers:int=2,
      dropout_rate:float=0.2,
      flag_bin_cnt:bool=True,
      model_type:str='vsn',
      train_valid_test:list=[0.7,0.15, 0.15],
      batch_size:int=64,
      max_lr=5e-3,
      min_lr=5e-3/(2.6**4)
    Returns:
      model
    """
    # make sure to convert cat_cols dtype from object to str
    for col in cat_cols:
        df[col] = df[col].astype(str)

    # calculate number of classes
    num_classes = len(df[label].value_counts())

    # split df into train_df, valid_df, test_df. NOTE: to random split, set `sort_split_by_col=None`
    train_df, valid_df, test_df = split_train_valid_test(df, train_valid_test=train_valid_test, target=label, random_state=123, sort_split_by_col=sort_split_by_col)
    print(f'=======train_df.shape={train_df.shape},valid_df.shape={valid_df.shape}, test_df.shape={test_df.shape}========')


    # define loss_dict which maps loss_name to real loss function
    loss_dict = {'SparseCategoricalFocalLoss':SparseCategoricalFocalLoss(gamma=2),
                 'SparseCategoricalCrossentropy':tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                 'CategoricalCrossentropy': tf.keras.losses.CategoricalCrossentropy(from_logits=False),
                 'SigmoidFocalCrossEntropy': tfa.losses.SigmoidFocalCrossEntropy(from_logits = False,
                                                            alpha= 0.25,
                                                            gamma= 2.0,
                                                            reduction=tf.keras.losses.Reduction.AUTO,
                                                            name = 'sigmoid_focal_crossentropy'
                                                            ),
                 }
    if loss_name in ['CategoricalCrossentropy','SigmoidFocalCrossEntropy']:
      flag_one_hot_encode_label=True
      metrics=METRICS
    else: # loss_name in ['SparseCategoricalCrossentropy','SparseCategoricalFocalLoss',, ]
      flag_one_hot_encode_label=False
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]




    # df to ds
    if flag_balance_train_df:
        train_ds=df_to_balanced_ds(df=train_df, flag_training=True, batch_size=batch_size, flag_bin_cnt=flag_bin_cnt, resize=(128, 128), flag_one_hot_encode_label=flag_one_hot_encode_label, global_vars=global_vars)
    else:
        train_ds=df_to_ds(df=train_df, flag_training=True, batch_size=batch_size, flag_bin_cnt=flag_bin_cnt, resize=(128, 128), flag_one_hot_encode_label=flag_one_hot_encode_label, global_vars=global_vars)
    valid_ds=df_to_ds(df=valid_df, flag_training=False, batch_size=batch_size, flag_bin_cnt=flag_bin_cnt, resize=(128, 128), flag_one_hot_encode_label=flag_one_hot_encode_label, global_vars=global_vars)
    test_ds=df_to_ds(df=test_df, flag_training=False, batch_size=batch_size, flag_bin_cnt=flag_bin_cnt, resize=(128, 128), flag_one_hot_encode_label=flag_one_hot_encode_label, global_vars=global_vars)

    # to handle imbalanced classification issue, calculate class_weight, bias_init
    class_weight, bias_init = init_imbalanced_class_weight_bias(df=train_df, label=label)
    if not flag_init_bias_class_weight:
        class_weight = {k:1 for (k,v) in class_weight.items()} # assign the same weight 1 to different classes
        bias_init = tf.constant_initializer(np.array([0 for (k,v) in class_weight.items()])) # assign 0s to bias

    print(f'===========class_weight = {class_weight}\n==========bias_init={bias_init}')

    #create complied iu 2 towers model
    model = create_iu_compiled_model(global_vars = global_vars,
                                     model_type=model_type,
                                    flag_trainable=flag_trainable,
                                    num_bins=num_bins,
                                    embedding_dim=embedding_dim,
                                    flag_txt_bert=flag_txt_bert,
                                    num_projection_layers=num_projection_layers,
                                    dropout_rate=dropout_rate,
                                    flag_bin_cnt=flag_bin_cnt,

                                    layer_sizes=layer_sizes,
                                    projection_dim=projection_dim,
                                    l1_l2=l1_l2,
                                    alpha=alpha,
                                    ###newly added @06/17/2022
                                    bias_initializer=bias_init,
                                    #metrics = metrics,
                                    loss_name=loss_name,
                                    optimizer=optimizer,
                                    flag_dcn_cross=flag_dcn_cross,
                                    )
    if flag_balance_train_df:
      scheduler_ls = []
    else:
      if flag_SGDRScheduler:
        scheduler_ls = [SGDRScheduler(min_lr=min_lr,
                                      max_lr=max_lr,
                                      lr_decay=.9,
                                      cycle_length=3,
                                      mult_factor=1.5)]
      else:
        scheduler_ls = []

    mode_dict={'val_prc':'max', 'val_loss':'mix'}
    callbacks = [
                  tf.keras.callbacks.EarlyStopping(     #EarlyStopping is used to stop at the epoch where val_accuracy does not improve significantly
                      monitor=monitor,
                      min_delta=1e-4,
                      patience=patience,
                      mode=mode_dict[monitor],
                      verbose=1,
                  ),
                  tf.keras.callbacks.ModelCheckpoint(
                      filepath=model_path+'weights',# replace 'weights.h5'
                      monitor=monitor,
                      mode=mode_dict[monitor],
                      save_best_only=True,
                      save_weights_only=True,
                      verbose=1
                      ),
                  #sgd_scheduler  # TBD: NOT working for flag_balance_train_df=True,
                  ] + scheduler_ls


    # 2 inputs
    if valid_ds is not None:
      history = model.fit(train_ds, #[train_ds, train_ds], #x=dict(X_train), y=y_train,
                          batch_size=batch_size,
                          validation_data= valid_ds, #[valid_ds, valid_ds],#(dict(X_test), y_test),
                          epochs=epochs,
                          callbacks=callbacks,
                          class_weight=class_weight,
                          )
    else:
      history = model.fit(train_ds, #[train_ds, train_ds], #x=dict(X_train), y=y_train,
                          batch_size=batch_size,
                          validation_split= 0.3, #[valid_ds, valid_ds],#(dict(X_test), y_test),
                          epochs=epochs,
                          callbacks=callbacks,
                          class_weight=class_weight,
                          )
    if test_ds is not None:
      test_metrics = model.evaluate(test_ds, verbose=False, return_dict=True)
      print(f'==========model test_metrics = {test_metrics}==========')

    #model.load_weights('weights.h5')
    if model_name is not None:
      #tf.saved_model.save(model, model_path+model_name)
      model.save(model_path+model_name)
      print(f"========To load this model: model = tf.keras.models.load_model({model_path}{model_name}) =======")
    return model

#model_path='/content/drive/My Drive/fast_tfrs/model/'
def save_iu_model(iu_model, #knn_clf.encoding_model
                  weight_filepath:str,#=model_path+'encoding_model_weight',
                  config_filepath:str,#=model_path+'encoding_model_config.pkl'
                  ):
    """save the model weight & config
    Args:
        iu_model: after trained
        weight_filepath:str=model_path+'encoding_model_weight',
        config_filepath:str=model_path+'encoding_model_config.pkl'
    Example:

    """

    #!mkdir -p saved_model
    # save model weights
    iu_model.save_weights(filepath=weight_filepath, save_format='tf')

    # pickle model config

    #to avoid pickle error, temperatively remove iu_model.config['metrics']
    tmp_metrics = iu_model.config['clf_metrics']
    tmp_optimizer = iu_model.config['clf_optimizer']

    iu_model.config['clf_metrics']=None
    iu_model.config['clf_optimizer']=None

    import dill as pickle
    with open(config_filepath, 'wb') as handle:
        pickle.dump(iu_model.config, handle, protocol=pickle.HIGHEST_PROTOCOL)
    #restore back
    iu_model.config['clf_metrics'] = tmp_metrics
    iu_model.config['clf_optimizer']=tmp_optimizer
    #iu_model.config['clf_optimizer']=tmp_optimizer


def load_iu_model(weight_filepath:str, #=model_path+'encoding_model_weight',
                  config_filepath:str, #=model_path+'encoding_model_config.pkl',
                  flag_return_ItemUserClassifier_instance:bool=False,
                  ):
    """reload the model weight & config
    Args:
      weight_filepath:str=model_path+'encoding_model_weight',
      config_filepath:str=model_path+'encoding_model_config.pkl'
      flag_return_ItemUserClassifier_instance:bool=False,
        - in case of False, return a tf.keras.models.Model, the same kind of model return by train_valid_eval_iu_classifier()
        - in case of True, return a ItemUserTwoTowersClassifier() instance
    Example:
        new_model = reload_model(weight_filepath='saved_model/best_model_weight', config_filepath='saved_model/best_model_config.pkl')
    """
    import dill as pickle
    with open(config_filepath, 'rb') as handle:
        config = pickle.load(handle)

    clf_loss_name = config['clf_loss_name']
    # define loss_dict which maps loss_name to real loss function
    clf_loss_dict = {'SparseCategoricalFocalLoss':SparseCategoricalFocalLoss(gamma=2),
                 'SparseCategoricalCrossentropy':tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                 'CategoricalCrossentropy': tf.keras.losses.CategoricalCrossentropy(from_logits=False),
                 'SigmoidFocalCrossEntropy': tfa.losses.SigmoidFocalCrossEntropy(from_logits = False,
                                                            alpha= 0.25,
                                                            gamma= 2.0,
                                                            reduction=tf.keras.losses.Reduction.AUTO,
                                                            name = 'sigmoid_focal_crossentropy'
                                                            ),
                 }
    if clf_loss_name in ['CategoricalCrossentropy','SigmoidFocalCrossEntropy']:
      flag_one_hot_encode_label=True
      clf_metrics=METRICS
    else: # loss_name in ['SparseCategoricalCrossentropy','SparseCategoricalFocalLoss',, ]
      flag_one_hot_encode_label=False
      clf_metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]

    # add config['metrics'] separately to avoid pickle error
    config['clf_metrics'] = clf_metrics
    config['clf_optimizer'] = [tf.keras.optimizers.Adam(learning_rate=3e-4), tf.keras.optimizers.Adadelta()][0]

    # ini & compile a new model using the reloaded model config
    #repl: new_model = create_model(config=model_config)
    new_iu_model = create_iu_compiled_model(global_vars=config['global_vars'],
                                                  model_type=config['model_type'],
                                                  flag_trainable=config['flag_trainable'],
                                                  num_bins=config['num_bins'],
                                                  embedding_dim=config['embedding_dim'],
                                                  flag_txt_bert=config['flag_txt_bert'],
                                                  num_projection_layers=config['num_projection_layers'],
                                                  dropout_rate=config['dropout_rate'],
                                                  flag_bin_cnt=config['flag_bin_cnt'],

                                                  layer_sizes=config['layer_sizes'],
                                                  projection_dim=config['projection_dim'],
                                                  l1_l2=config['l1_l2'],
                                                  alpha=config['alpha'],
                                                  bias_initializer=config['bias_initializer'],
                                                  #metrics = config['clf_metrics'],
                                                  loss_name=config['clf_loss_name'],
                                                  optimizer=config['clf_optimizer'],
                                                  flag_dcn_cross=config['flag_dcn_cross'],
                                                  )

    # step1: reload the weights back to the new model
    new_iu_model.load_weights(weight_filepath)

    if flag_return_ItemUserClassifier_instance:
      # step2: instantiate an instance of ItemUserTwoTowersClassifier() again, with config = loaded_model.config:
      new_iu_class_instance = ItemUserTwoTowersClassifier(config=config)
      new_iu_class_instance.model = new_iu_model
      return new_iu_class_instance
    else:
      #instantiate an instance
      return new_iu_model



# Cell

#title 2-tower class

class ItemUserTwoTowersClassifier():
  def __init__(self, config):

    self.config = config

    # define loss_dict which maps loss_name to real loss function
    loss_dict = {'SparseCategoricalFocalLoss':SparseCategoricalFocalLoss(gamma=2),
                 'SparseCategoricalCrossentropy':tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                 'CategoricalCrossentropy': tf.keras.losses.CategoricalCrossentropy(from_logits=False),
                 'SigmoidFocalCrossEntropy': tfa.losses.SigmoidFocalCrossEntropy(from_logits = False,
                                                            alpha= 0.25,
                                                            gamma= 2.0,
                                                            reduction=tf.keras.losses.Reduction.AUTO,
                                                            name = 'sigmoid_focal_crossentropy'
                                                            ),
                 }
    if config['clf_loss_name'] in ['CategoricalCrossentropy','SigmoidFocalCrossEntropy']:
      self.config['flag_one_hot_encode_label']=True
      self.config['clf_metrics']=METRICS

    else: # loss_name in ['SparseCategoricalCrossentropy','SparseCategoricalFocalLoss',, ]
      self.config['flag_one_hot_encode_label']=False
      self.config['clf_metrics']=[tf.keras.metrics.SparseCategoricalAccuracy()]

    #self.config['clf_loss']=loss_dict[config['clf_loss_name']]

    self.model = create_iu_compiled_model(global_vars=config['global_vars'],
                                          model_type=config['model_type'],
                                          flag_trainable=self.config['flag_trainable'],
                                          num_bins=self.config['num_bins'],
                                          embedding_dim=self.config['embedding_dim'],
                                          flag_txt_bert=self.config['flag_txt_bert'],
                                          num_projection_layers=self.config['num_projection_layers'],
                                          dropout_rate=self.config['dropout_rate'],
                                          flag_bin_cnt=self.config['flag_bin_cnt'],

                                          layer_sizes=self.config['layer_sizes'],
                                          projection_dim=self.config['projection_dim'],
                                          l1_l2=self.config['l1_l2'],
                                          alpha=self.config['alpha'],
                                          bias_initializer=self.config['bias_initializer'],
                                          #metrics = self.config['clf_metrics'],
                                          loss_name=self.config['clf_loss_name'],
                                          optimizer=self.config['clf_optimizer'],
                                          flag_dcn_cross=self.config['flag_dcn_cross'],
                                          )


  def fit(self,
          train_df,
          valid_df,
          test_df,
          flag_balance_train_df:bool=True,
          flag_init_bias_class_weight:bool=False,
          flag_SGDRScheduler:bool=True,
          ):
    # attributes needed to fit/train the model
    epochs=self.config['epochs']
    model_path=self.config['model_path']
    model_name=self.config['model_name']
    model_type=self.config['model_type']
    train_valid_test=self.config['train_valid_test']
    batch_size=self.config['batch_size']
    max_lr=self.config['max_lr']
    min_lr=self.config['min_lr']
    monitor=self.config['monitor']
    flag_bin_cnt=self.config['flag_bin_cnt']

    # df to ds
    if flag_balance_train_df:
        train_ds=df_to_balanced_ds(df=train_df, flag_training=True, batch_size=batch_size, flag_bin_cnt=flag_bin_cnt, resize=(128, 128), flag_one_hot_encode_label=self.config['flag_one_hot_encode_label'], global_vars=self.config['global_vars'])
    else:
        train_ds=df_to_ds(df=train_df, flag_training=True, batch_size=batch_size, flag_bin_cnt=flag_bin_cnt, resize=(128, 128), flag_one_hot_encode_label=self.config['flag_one_hot_encode_label'], global_vars=self.config['global_vars'])

    if valid_df is not None:
        valid_ds=df_to_ds(df=valid_df, flag_training=False, batch_size=batch_size, flag_bin_cnt=flag_bin_cnt, resize=(128, 128), flag_one_hot_encode_label=self.config['flag_one_hot_encode_label'], global_vars=self.config['global_vars'])
    else:
        valid_ds = None
    if test_df is not None:
        test_ds=df_to_ds(df=test_df, flag_training=False, batch_size=batch_size, flag_bin_cnt=flag_bin_cnt, resize=(128, 128), flag_one_hot_encode_label=self.config['flag_one_hot_encode_label'], global_vars=self.config['global_vars'])
    else:
        test_ds = None

    # to handle imbalanced classification issue, calculate class_weight, bias_init
    class_weight, bias_init = init_imbalanced_class_weight_bias(df=train_df, label=label)
    if not flag_init_bias_class_weight:
        class_weight = {k:1 for (k,v) in class_weight.items()} # assign the same weight 1 to different classes
        bias_init = tf.constant_initializer(np.array([0 for (k,v) in class_weight.items()])) # assign 0s to bias

    print(f'===========class_weight = {class_weight}\n==========bias_init={bias_init}')



    if flag_balance_train_df: # TBD: SGDRScheduler is NOT working for balanced train_df yet
        scheduler_ls = []
    else:
      if flag_SGDRScheduler:
          scheduler_ls = []
      else:

          scheduler_ls = [SGDRScheduler(min_lr=min_lr,
                                  max_lr=max_lr,
                                  lr_decay=.9,
                                  cycle_length=3,
                                  mult_factor=1.5)]
    mode_dict={'val_prc':'max', 'val_loss':'min'}
    callbacks = [
                tf.keras.callbacks.EarlyStopping(     #EarlyStopping is used to stop at the epoch where val_accuracy does not improve significantly
                    monitor=monitor,#'val_prc', #'val_loss'
                    mode=mode_dict[monitor], #'max', #'min'
                    min_delta=1e-4,
                    patience=4,
                    verbose=1
                ),
                tf.keras.callbacks.ModelCheckpoint(
                    filepath=model_path+'weights',# replace 'weights.h5'
                    monitor=monitor, #'val_prc', #'val_loss'
                    mode=mode_dict[monitor], #'max', #'min'
                    save_best_only=True,
                    save_weights_only=True,
                    verbose=1
                    ),
                ] + scheduler_ls

    # 2 inputs

    if valid_ds is not None:
        history = self.model.fit(train_ds, #[train_ds, train_ds], #x=dict(X_train), y=y_train,
                                batch_size=batch_size,
                                validation_data= valid_ds, #[valid_ds, valid_ds],#(dict(X_test), y_test),
                                epochs=epochs,
                                callbacks=callbacks,
                                class_weight=class_weight,
                                )
    else:
        history = self.model.fit(train_ds, #[train_ds, train_ds], #x=dict(X_train), y=y_train,
                        batch_size=batch_size,
                        validation_split= 0.3, #[valid_ds, valid_ds],#(dict(X_test), y_test),
                        epochs=epochs,
                        callbacks=callbacks
                        )
    if test_ds is not None:
        test_metrics = self.model.evaluate(test_ds, verbose=False, return_dict=True)
        print(f'==========model test_metrics = {test_metrics}==========')

    """#model.load_weights('weights.h5')
    if model_name is not None:
      #tf.saved_model.save(model, model_path+model_name)
      self.model.save(model_path+model_name)
      print(f"========To load this model: model = tf.keras.models.load_model({model_path}{model_name}) =======")
    """
    #update self.model as the compiled & fitted version
    self.model = self.model
    return self.model

  def predict(self,
              test_df:pd.DataFrame,
              label:str,#=label,
              labels:list,#=[2,1,0]
              ):

      test_ds = df_to_ds(test_df, global_vars=self.config['global_vars'])

      y_prob = self.model.predict(test_ds)

      y_pred = y_prob.argmax(axis=1)

      #visualize performance
      _ = get_performance_metrics(y_true=test_df[label],
                                  y_prob=y_prob,
                                  y_pred=None,
                                  labels=labels,
                                  normalize='all',
                                  )
      return y_pred, y_prob

  def save_model(self,
                weight_filepath:str,#=model_path+'encoding_model_weight',
                config_filepath:str,#=model_path+'encoding_model_config.pkl'
                ):
    """save the model weight & config
    Args:
        iu_modeltrained
        weight_filepath:str=model_path+'encoding_model_weight',
        config_filepath:str=model_path+'encoding_model_config.pkl'
    Example:

    """

    #!mkdir -p saved_model
    # save model weights
    self.model.save_weights(filepath=weight_filepath, save_format='tf')

    #to avoid pickle error, temperatively remove iu_model.config['metrics']
    config_dic = self.config.copy()

    # the following items can NOT be pickle.dump()
    config_dic['encoding_model'], config_dic['clf_metrics'], config_dic['clf_optimizer'], config_dic['rating_metrics'], config_dic['rating_optimizer']  = None, None, None, None, None
    config_dic['interactions']=None
    config_dic['items']=None
    config_dic['users']=None
    config_dic['cached_train']=None
    config_dic['cached_valid']=None
    config_dic['cached_test']=None


    print(self.config.keys())
    # pickle model config
    import dill as pickle
    with open(config_filepath, 'wb') as handle:
        pickle.dump(config_dic, handle, protocol=pickle.HIGHEST_PROTOCOL)

    #print(f"==================self.config of flag_dcn_cross= {self.config['flag_dcn_cross']}")
    print(f"========\n To load this model: new_iu_clf = load_iu_model(\n weight_filepath=model_path+'encoding_model_weight', \n config_filepath=model_path+'encoding_model_config.pkl', \n flag_return_ItemUserClassifier_instance=True, \n metrics=METRICS,\n) \n=======")

################################




# Cell

#@title samples to embeddings
from tensorflow.keras.models import Model
def normalize_embeddings(embeddings:np.array):
    """normalize each sample (each sample's L2 norm equals 1)
    Args:
      embeddings:np.array of shape (num_sample, embedding_dim)
    Return:
      normalized embeddings
    Example:
      embs = samples_to_embeddings(sample_df=test_df.head(7), model=model, layer_name='i_emb')
      norm_embs = normalize_embeddings(embs)
    """
    from sklearn.preprocessing import normalize
    normed_embeddings = normalize(embeddings, axis=1, norm='l2') #l2 norm: sqrt sum up to 1 for each sample (each row of the array)
    #dbck
    #print(f'>>>>dbck: each sample has norm 1: {np.linalg.norm(normed_embeddings, axis=1)}<<<<<')
    return normed_embeddings

def samples_to_embeddings(sample_df:pd.DataFrame,
                          model:tf.keras.models.Model,
                          batch_size:int=64,
                          flag_bin_cnt:bool=False,
                          layer_name:str='i_emb',
                          layer_idx:int=None,
                          global_vars:tuple=None,
                          ):
    """get the samples' embeddings produced by a selected model layer
    Args:
      sample_df:pd.DataFrame,
      model:tf.keras.models.Model, a item-user 2 towers model
      batch_size:int=64,
      flag_bin_cnt:bool=False,
      layer_name:str='u_emb' (prefered) chosen from options ['u_emb', 'i_emb', 'iu_emb']
      layer_idx:int=None, (optional)
    Returns:
      samples_embs:np.array of shape (num_sample, embedding_dim)
    Examples:
      embs = samples_to_embeddings(sample_df=test_df.head(7), model=model, layer_name='u_emb')

    """
    XX = model.input
    if layer_idx is None:
      YY = model.get_layer(layer_name).output
    else:
      YY = model.layers[layer_idx].output #embedding layer
    #print(f'>>>>>>>>>Doublecheck that you are extracting output from this layer {YY}<<<<<<<<')
    new_model = tf.keras.models.Model(XX, YY)

    sample_ds = df_to_ds(df=sample_df, flag_training=False, batch_size=64, flag_bin_cnt=flag_bin_cnt, resize=(128, 128), global_vars=global_vars)
    samples_embs = new_model.predict(sample_ds)
    norm_samples_embs = normalize_embeddings(samples_embs)
    return norm_samples_embs



# Cell
#@title find similar x, where x is either item or user
# find top_n users in candidate_df that are most similar to query user
def find_similar_x(x:str, #='user', other options ['user', 'item', 'item-user']
                   query_df:pd.DataFrame, #=test_df, or ff_test_df
                   candidate_df:pd.DataFrame, #=train_df,
                   top_n:int,
                   model, #iu_clf.model
                   global_vars:tuple,
                   ):
    """ 'find top 5 people most similar to you' (here x represents user)
        'find top 5 meetings most similar to this 1:1 Alex Bob meeting'(here x represents meeting)
        'find top 5 (meeting:attendee) most sililar to this (meeting:attendee) combination (here x represents (meeting,user) pair)
    Args:
      x:str='user', choose from ['user', 'item', 'item-user']
      query_df:pd.DataFrame=test_df[user_id=query_user_id][0:1],
      candidate_df:pd.DataFrame=test_df,
      top_n:int=5,
      model=iu_clf.model)
    Return:
      similar_x_ls: a list of similar x(user|item|item_user), each represent a similar sample w.r.t. x and corresponses to each row in query_df
    Examples:
      #randomly choose a query sample in test_df
      import random
      i = random.randint(0, test_df.shape[0])
      #choose *1* query sample
      query_df=test_df.iloc[i:i+1,:]

      # to find similar users to the query user
      x= ['item', 'user', 'item_user'][1]
      x2x_cols = {'user':u_cols, 'item':i_cols, 'item_user':i_cols + u_cols}
      x2layer_name = {'user':'u_emb', 'item':'i_emb', 'item_user':'iu_emb'}
      x2x_id = {'user':[user_id], 'item':[item_id], 'item_user':[item_id, user_id]}

      top_u=3
      sim_u_ls = find_similar_x(x=x,
                              query_df=query_df,
                              candidate_df=train_df,
                              top_n=top_u,
                              model=iu_clf.model)
      for i in range(query_df.shape[0]):
        print(f"--------query {x}------")
        print(query_df.iloc[i:(i+1),:])
        print(f"========similar {x}s' ids========")
        print(set(sim_u_ls[i][x2x_id[x][0]]))
        print(f"========similar {x}s' features========")
        print(sim_u_ls[i][['similarity']+x_cols])
        # x is changed to 'item'
      x = 'item'

      #top_n could be changed to another number
      top_i=5

      sim_i_ls = find_similar_x(x=x,
                              query_df=query_df,
                              candidate_df=candidate_df,
                              top_n=top_i,
                              model=iu_clf.model)
      print(f"--------query {x}------")
      print(query_df.iloc[i:(i+1),:])
      print(f"========similar {x}s' id========")
      print(set(sim_i_ls[0][x2x_id[x][0]]))
      print(f"========similar {x}s' sample========")
      print(sim_i_ls[0][['similarity']+x_cols])
    """
    # unpack global vars
    (label, user_id, item_id, x_cols, u_cols, i_cols, cnt_cols, u_cnt_cols, i_cnt_cols,cnt_cols_to_bin, u_cnt_cols_to_bin, i_cnt_cols_to_bin,bin_cnt_cols, u_bin_cnt_cols, i_bin_cnt_cols, cat_cols, u_cat_cols, i_cat_cols, txt_cols, u_txt_cols, i_txt_cols, img_cols, u_img_cols, i_img_cols, dt_cols, u_dt_cols, i_dt_cols, label_str2num,label_num2str, num_classes, label_keys) = global_vars

    x2layer_name = {'user':'u_emb', 'item':'i_emb', 'item_user':'iu_emb'}
    x2x_id = {'user':[user_id], 'item':[item_id], 'item_user':[item_id, user_id]}

    layer_name = x2layer_name[x]
    query_emb = samples_to_embeddings(sample_df=query_df,
                                      model=model,
                                      layer_name=layer_name,
                                      global_vars=global_vars,
                                      )
    candidate_embs = samples_to_embeddings(sample_df=candidate_df,
                                           model=model,
                                           layer_name=layer_name,
                                           global_vars=global_vars,
                                           )


    # find the top_k candidate samples most similary to query_sample
    from sklearn.metrics.pairwise import cosine_similarity
    cosine_sims = cosine_similarity(X=query_emb, Y=candidate_embs)

    #result_df_ls = []
    #x_ids_ls = []
    similar_x_ls = []
    for idx in range(cosine_sims.shape[0]): #for each query_sample, comput its result_df
        candidate_df['similarity'] = list(cosine_sims[idx])
        #the resulted df need to drop duplicated rows by columns=x2x_id[x]
        result_df = candidate_df.sort_values(by='similarity', ascending=False).drop_duplicates(subset=x2x_id[x], keep='last').head(top_n)

        # normalized cosine similarity to be within [-1,1] for keras method only/no such need for sklearn method
        #result_df['similarity'] *= 1/result_df['similarity'].iloc[0]
        x_ids = list(result_df[x2x_id[x][0]])

        # candidates with x_ids
        # select rows from a df based on columns values
        # candidate_df.loc[candidate_df[x2x_id[x]].isin(x_ids)]

        # find the similar rows with column x2x_id[x] (e.g. user_id) isin certain values
        similar_x = candidate_df.loc[candidate_df[x2x_id[x][0]].isin(x_ids)].sort_values(by='similarity', ascending=False)

        #x_ids_ls.append(x_ids)
        similar_x_ls.append(similar_x)
        #result_df_ls.append(result_df)

    return similar_x_ls
def get_cosine_similarity_model(model:tf.keras.models.Model,
                                candidates_count:int,
                                layer_name:str='i_emb',
                                ):
    """ implement cosine_sim(query_sample, candidate_samples) using tf dense layer
    Args:
      model: the trained item-user 2tower classification model
      candidates_count:int the number of candidate samples
    Return:
      cosine_similarity_model:tf.keras.models.Model
    """
    #embedding model is the 'i_emb' layer in the two_tower model
    embedding_model = model.get_layer(layer_name)

    # Flatten feature vector, here x is the vectors of candidate samples
    flat_dim_size = np.prod(embedding_model.output_shape[1:])
    x = tf.keras.layers.Reshape(target_shape=(flat_dim_size,),
                name='features_flat')(embedding_model.output)

    # cosine_similarity = dot product between query sample(query_vector) and candidate samples(candidate_vectors)
    # it is implemented as a single dense layer with precomputed weights (not shown here, but will be provided latter), linear activator, 0 bias.
    x = Dense(units=candidates_count,
              activation='linear',
              name='knn_dense',
              use_bias=False)(x)

    cosine_similarity_model = tf.keras.models.Model(inputs=[model.input], outputs=x)
    return cosine_similarity_model

def keras_knn_recommendations(query_df:pd.DataFrame,
                              candidate_df:pd.DataFrame,
                              top_k:int,#=5,
                              model:tf.keras.models.Model,
                              label_str2num:dict,#=label_str2num,
                              layer_name:str,#='i_emb',
                              global_vars:tuple,#=global_vars,
                              ):
    """For each query sample in query_df, get the top_k most similar samples in candidate_df w.r.t. 'item'|'user'|'item-user'
    Args:
      query_df:pd.DataFrame, with shape (num_querries, num_features)
      candidate_df:pd.DataFrame, with shape (num_candidates, num_features)
      top_k:int=5,
      model:tf.keras.models.Model=model, a trained item-user-2towers model
      label_str2num:dict e.g. label_str2num = {'accepted': 2, 'declined': 0, 'tentative': 1, 'needsAction': 1}
      layer_name:str='i_emb', choose from ['i_emb', 'u_emb', 'iu_emb']
      global_vars:tuple=None, #global_vars,
    Returns:
      prob_ls: a list of prob where prob is the probabilities for each response (represented by a string) e.g. 'accepted', 'declined', 'needAction'
      prob_numkey_ls: a list of prob_numkey where prob_numkey is the probabilities for each response (represented by a number) e.g. 2 ('accepted'), 1('needAction' or 'tentative'), 0('declined')
      result_df_ls: a list of result_df where result_df:pd.DataFrame top_k samples from candidate_df most similar to a particular sample in query_df
    """

    # calculate query embedding vector
    query_emb = samples_to_embeddings(sample_df=query_df,
                                      model=model,
                                      layer_name=layer_name,
                                      global_vars=global_vars,
                                      )
    # calculate candidates' embedding vectors
    candidate_embs = samples_to_embeddings(sample_df=candidate_df,
                                           model=model,
                                           layer_name=layer_name,
                                           global_vars=global_vars,
                                           )

    # to compute cosine similary bw query_sample and candiates_samples, build a knn_model with weights being candidate_embs.T
    knn_model = get_cosine_similarity_model(model=model, candidates_count=candidate_embs.shape[0])
    tmp_weights = knn_model.get_weights()
    #print(tmp_weights[-1].shape) # the last element of tmp_weight list is the weight = candidate_embs.T, with shape (embedding_dim, candidats_count)

    #to provide dense layer's weights
    tmp_weights[-1] = candidate_embs.T
    knn_model.set_weights(tmp_weights)
    #knn_model.summary()

    #now compute cosine_similarity by dot product bw query_emb and candidate_embs
    cosine_sims = knn_model.predict(df_to_ds(query_df, global_vars=global_vars))
    #print(cosine_sims) # it's not totally within [-1,1] because the query_df is not normalized

    """# version0: require query_df has 1 sample, e.g. shape (1,num_features)

    candidate_df['similarity'] = list(cosine_sims[0])
    result_df = candidate_df.sort_values(by='similarity', ascending=False).head(top_k+1) #the top one is the query_df, the rest are from candiate_df

    # normalized cosine similarity to be within [-1,1]
    result_df['similarity'] *= 1/result_df['similarity'].iloc[0]

    #calculate probabilities of different responses
    prob = dict(result_df[label].value_counts()/result_df.shape[0])

    #construct a mapping that maps label from num back to str
    label_num2str = {v:k for (k,v) in label_str2num.items()}

    # easy to understand prediction
    ez_prob = {label_num2str[k]:v for (k,v) in prob.items()}"""

    # init placeholders
    result_df_ls = [] # to host candidate samples most similar to *each* query sample
    prob_ls = [] #to host ez to understand probability prediction by knn, using str key
    prob_numkey_ls = [] #to host ez to understand probability prediction by knn, using num key

    #construct a mapping that maps label from num back to str; ATTN: b/c str2num is NOT 1:1, this reverse mapping lost info
    label_num2str = {v:k for (k,v) in label_str2num.items()}

    for idx in range(cosine_sims.shape[0]): #for each query_sample, comput its result_df, ez_prob
        candidate_df['similarity'] = list(cosine_sims[idx])
        result_df = candidate_df.sort_values(by='similarity', ascending=False).head(top_k+1) #the top one is the query_df, the rest are from candiate_df

        # normalized cosine similarity to be within [-1,1]
        result_df['similarity'] *= 1/result_df['similarity'].iloc[0]

        """#calculate probabilities of different responses
        prob = dict(result_df[label].value_counts()/result_df.shape[0])"""

        ### calculate probability used for majority vote
        #calculate probabilities of different responses
        prob_majority_vote = dict(result_df[label].value_counts()/result_df.shape[0])

        ### Now calculate prob used for weighted majority vote
        dic = {key: result_df.groupby(label)['similarity'].agg(list)[key] for key in set(result_df[label])}
        ll = [v for v in dic.values()] # a list of lists
        ls = [x for l in ll for x in l] # flatten a list of lists
        s = sum(ls) # similarity sum

        # weighted majority vote: each candidate's vote is weighted by its similarity to query sample
        prob_weighted_vote = {k:sum(v)/s for (k,v) in dic.items()}

        # easy to understand prediction
        prob = {label_num2str[k]:v for (k,v) in prob_weighted_vote.items()}

        # extend prob dict to include all the keys 'accepted', 'declined', 'needAction'
        for key in set(label_num2str.values()) - set(prob.keys()):
            prob[key]=0
        # using label_str2num, to map each key in prob fr str to num:
        prob_numkey={label_str2num[k]:v for (k,v) in prob.items()}

        # save the result for each query sample:
        result_df_ls.append(result_df)
        prob_ls.append(prob)
        prob_numkey_ls.append(prob_numkey)
    return prob_ls,prob_numkey_ls, result_df_ls

# Cell
#@title knn recommendation
def sklearn_knn_recommendations(query_df:pd.DataFrame,
                                candidate_df:pd.DataFrame,
                                top_k:int,
                                model:tf.keras.models.Model,
                                label_str2num:dict,
                                layer_name:str,#='i_emb',
                                global_vars:tuple,#=None,
                                ):
    """get the top_k samples in candidate_df that are most similar to query_df
    Args:
      query_df:pd.DataFrame,
      candidate_df:pd.DataFrame,
      top_k:int=5,
      model:tf.keras.models.Model=model, a trained item-user-2towers model
      label_str2num:dict e.g. label_str2num = {'accepted': 2, 'declined': 0, 'tentative': 1, 'needsAction': 1}
      layer_name='i_emb'
    Returns:
      ez_prob:dict e.g. {'accepted': 0.8, 'needsAction': 0.2}
      result_df:pd.DataFrame top_k samples from candidate_df most similar to query_df
    """
    # calculate query embedding vector
    query_emb = samples_to_embeddings(sample_df=query_df,
                                      model=model,
                                      layer_name=layer_name,
                                      global_vars=global_vars,
                                      )
    # calculate candidates' embedding vectors
    candidate_embs = samples_to_embeddings(sample_df=candidate_df,
                                           model=model,
                                           layer_name=layer_name,
                                           global_vars=global_vars,
                                           )

    # find the top_k candidate samples most similary to query_sample
    from sklearn.metrics.pairwise import cosine_similarity
    cosine_sims = cosine_similarity(X=query_emb, Y=candidate_embs)
    """# version0: require query_df has shape (1,num_features)
    candidate_df['similarity'] = cosine_sims

    #sort all the rows in candidate_df by descending similarity scores
    result_df = candidate_df.sort_values(by='similarity', ascending=False).head(top_k+1) #the top one is the query_df, the rest are from candiate_df

    #calculate probabilities of different responses
    prob = dict(result_df[label].value_counts()/result_df.shape[0])

    #construct a mapping that maps label from num back to str
    label_num2str = {v:k for (k,v) in label_str2num.items()}

    # easy to understand prediction
    ez_prob = {label_num2str[k]:v for (k,v) in prob.items()}"""
    # init placeholders
    result_df_ls = [] # to host candidate samples most similar to *each* query sample
    prob_ls = [] #to host ez to understand probability prediction by knn, using str key
    prob_numkey_ls = [] #to host ez to understand probability prediction by knn, using num key

    #construct a mapping that maps label from num back to str; ATTN: b/c str2num is NOT 1:1, this reverse mapping lost info
    label_num2str = {v:k for (k,v) in label_str2num.items()}

    for idx in range(cosine_sims.shape[0]): #for each query_sample, comput its result_df, ez_prob
        candidate_df['similarity'] = list(cosine_sims[idx])

        #sort all the rows in candidate_df by descending similarity scores
        result_df = candidate_df.sort_values(by='similarity', ascending=False).head(top_k+1) #the top one is the query_df, the rest are from candiate_df

        #calculate probabilities of different responses
        #prob = dict(result_df[label].value_counts()/result_df.shape[0])

        ### calculate probability used for majority vote
        #calculate probabilities of different responses
        prob_majority_vote = dict(result_df[label].value_counts()/result_df.shape[0])

        ### Now calculate prob used for weighted majority vote
        dic = {key: result_df.groupby(label)['similarity'].agg(list)[key] for key in set(result_df[label])}
        ll = [v for v in dic.values()] # a list of lists
        ls = [x for l in ll for x in l] # flatten a list of lists
        s = sum(ls) # similarity sum

        # weighted majority vote: each candidate's vote is weighted by its similarity to query sample
        prob_weighted_vote = {k:sum(v)/s for (k,v) in dic.items()}

        # easy to understand prediction
        prob = {label_num2str[k]:v for (k,v) in prob_weighted_vote.items()}

        # extend prob dict to include all the keys 'accepted', 'declined', 'needAction'
        for key in set(label_num2str.values()) - set(prob.keys()):
            prob[key]=0
        # using label_str2num, to map each key in prob fr str to num:
        prob_numkey={label_str2num[k]:v for (k,v) in prob.items()}

        # save the result for each query sample:
        result_df_ls.append(result_df)
        prob_ls.append(prob)
        prob_numkey_ls.append(prob_numkey)
    return prob_ls,prob_numkey_ls, result_df_ls



# Cell

#@title knn_recommendation_1step
def knn_recommendations_1step(query_df:pd.DataFrame,
                              candidate_df:pd.DataFrame,
                              top_k:int,#=5,
                              model:tf.keras.models.Model,
                              label_str2num:dict,#=label_str2num,
                              cos_sim_method:str='sklearn',
                              query_emb=None,
                              candidate_embs=None,
                              layer_name:str='i_emb',
                              flag_return_df:bool=False,
                              global_vars:tuple=None,#global_vars,
                              ):
    """For each query sample in query_df, get the top_k samples in candidate_df that are most similar according to item features (layer_name='i_emb')
    Args:
      query_df:pd.DataFrame, with shape (num_querries, num_features)
      candidate_df:pd.DataFrame, with shape (num_candidates, num_features)
      top_k:int=5,
      model:tf.keras.models.Model=model, a trained item-user-2towers model
      label_str2num:dict e.g. label_str2num = {'accepted': 2, 'declined': 0, 'tentative': 1, 'needsAction': 1}
      cos_sim_method:str='sklearn', options are ['sklearn', 'keras']
      query_emb=None,
      candidate_embs=None,
      layer_name='i_emb',
      flag_return_df:bool=False
    Returns:
      if flag_return_df==False
          prob_ls:list of prob
          prob_numkey_ls:list of prob_numkey
          result_df_ls:list of result_df
      else:
    Examples:
        # choose a trained model
        model = iu_clf.model

        #choose a user that have upcoming meetings (with counts listed fr high to low)
        user_id_ls = list(test_df[user_id].value_counts().index)
        query_user_id = user_id_ls[1] # choose 0,1,2,3,...
        cos_sim_method=['sklearn','keras'][1]
        # query sample is **an** upcoming meeting to which user ('joe@gene.com') is invited
        query_df = test_df[test_df[user_id]==query_user_id].head(1)
        pd.set_option('display.max_rows', None)
        pd.set_option('display.max_columns', None)
        pd.set_option('display.width', 1000)
        pd.set_option('display.colheader_justify', 'center')
        pd.set_option('display.precision', 3)
        from IPython.display import display
        print(f'======query sample======')
        display(query_df[i_cols])
        # candidate samples are chosen to be the same user's past meeting
        candidate_df = train_df[train_df[user_id] ==query_user_id]

        if query_df.shape[0]>0 and candidate_df.shape[0]>0:
            prob_ls1, prob_ls2, result_df_ls = knn_recommendations_1step(query_df,
                                                                        candidate_df,
                                                                        top_k=7,
                                                                        model=model,
                                                                        label_str2num=label_str2num,
                                                                        cos_sim_method=cos_sim_method,
                                                                        layer_name='i_emb',
                                                                        )
            print(f'======The users past similar items w.r.t. i_cols to query sample======')
            print(f'--------(1): The {cos_sim_method} knn recommendation for {query_user_id} is {prob_ls1}=======')

            print(f'--------(2): The {cos_sim_method} knn recommendation for {query_user_id} is {prob_ls2}=======')


            for x in result_df_ls:
                pd.set_option('display.max_rows', None)
                pd.set_option('display.max_columns', None)
                pd.set_option('display.width', 1000)
                pd.set_option('display.colheader_justify', 'center')
                pd.set_option('display.precision', 3)
                from IPython.display import display
                display(x[['similarity']+i_cols]) #display(x([['similarity','google_event_id', 'attendee_email', 'start_datetime', 'title', 'description', 'response_status']])

        else:
            print(f'--------No recommendation! Because there is {query_df.shape[0]} query and  {candidate_df.shape[0]} candidate(s). Either is empty.-------')

    """
    #unpack global_vars
    (label, user_id, item_id, x_cols, u_cols, i_cols, cnt_cols, u_cnt_cols, i_cnt_cols,cnt_cols_to_bin, u_cnt_cols_to_bin, i_cnt_cols_to_bin,bin_cnt_cols, u_bin_cnt_cols, i_bin_cnt_cols, cat_cols, u_cat_cols, i_cat_cols, txt_cols, u_txt_cols, i_txt_cols, img_cols, u_img_cols, i_img_cols, dt_cols, u_dt_cols, i_dt_cols, label_str2num,label_num2str, num_classes, label_keys) = global_vars


    ###??? To be tested TDB
    if query_emb is None:
        # calculate query item embedding vector
        query_emb = samples_to_embeddings(sample_df=query_df,
                                          model=model,
                                          layer_name=layer_name,
                                          global_vars=global_vars,
                                          )
    if candidate_embs is None:
        # calculate candidate items' embedding vectors
        candidate_embs = samples_to_embeddings(sample_df=candidate_df,
                                               model=model,
                                               layer_name=layer_name,
                                               global_vars=global_vars,
                                               )

    # there are 2 methods to calculate cosine_sims scores: 'keras' or 'sklearn'
    if cos_sim_method == 'keras':
        # to compute cosine similary bw query_sample and candiates_samples, build a knn_model with weights being candidate_embs.T
        knn_model = get_cosine_similarity_model(model=model, candidates_count=candidate_embs.shape[0])
        tmp_weights = knn_model.get_weights()
        #print(tmp_weights[-1].shape) # the last element of tmp_weight list is the weight = candidate_embs.T, with shape (embedding_dim, candidats_count)

        #to provide dense layer's weights
        tmp_weights[-1] = candidate_embs.T
        knn_model.set_weights(tmp_weights)
        #knn_model.summary()

        #now compute cosine_similarity by dot product bw query_emb and candidate_embs
        cosine_sims = knn_model.predict(df_to_ds(query_df, global_vars=global_vars))
    else:
        # find the top_k candidate samples most similary to query_sample
        from sklearn.metrics.pairwise import cosine_similarity
        cosine_sims = cosine_similarity(X=query_emb, Y=candidate_embs)

    #print(cosine_sims) # it's not totally within [-1,1] because the query_df is not normalized

    # init placeholders
    result_df_ls = [] # to host candidate samples most similar to *each* query sample
    prob_ls = [] #to host ez to understand probability prediction by knn, using str key
    prob_numkey_ls = [] #to host ez to understand probability prediction by knn, using num key

    #construct a mapping that maps label from num back to str; ATTN: b/c str2num is NOT 1:1, this reverse mapping lost info
    label_num2str = {v:k for (k,v) in label_str2num.items()}

    for idx in range(cosine_sims.shape[0]): #for each query_sample, comput its result_df, ez_prob
        candidate_df['similarity'] = list(cosine_sims[idx])
        result_df = candidate_df.sort_values(by='similarity', ascending=False).head(top_k)

        # normalized cosine similarity to be within [-1,1]
        result_df['similarity'] *= 1/result_df['similarity'].iloc[0]

        #calculate probabilities of different responses
        #prob = dict(result_df[label].value_counts()/result_df.shape[0]) #in use keras as cos_sim_method, need to normalized prob by the norm of query_df

        ### calculate probability used for majority vote
        #calculate probabilities of different responses
        prob_majority_vote = dict(result_df[label].value_counts()/result_df.shape[0])

        ### Now calculate prob used for weighted majority vote
        dic = {key: result_df.groupby(label)['similarity'].agg(list)[key] for key in set(result_df[label])}
        ll = [v for v in dic.values()] # a list of lists
        ls = [x for l in ll for x in l] # flatten a list of lists
        s = sum(ls) # similarity sum

        # weighted majority vote: each candidate's vote is weighted by its similarity to query sample
        prob_weighted_vote = {k:sum(v)/s for (k,v) in dic.items()}

        # easy to understand prediction
        prob = {label_num2str[k]:v for (k,v) in prob_weighted_vote.items()}

        # extend prob dict to include all the keys 'accepted', 'declined', 'needAction'
        for key in set(label_num2str.values()) - set(prob.keys()):
            prob[key]=0
        # using label_str2num, to map each key in prob fr str to num:
        prob_numkey={label_str2num[k]:v for (k,v) in prob.items()}

        # save the result for each query sample:
        result_df_ls.append(result_df)
        prob_ls.append(prob)
        prob_numkey_ls.append(prob_numkey)
    #NOTE: return a list of prob dictionaries instead of a df; it will be easier for downstream process to combine list instead dfs.
    if flag_return_df:
        prob = pd.DataFrame(prob_ls)
        prob['prediction']=prob.idxmax(axis=1)
        prob_numkey = pd.DataFrame(prob_numkey_ls)
        prob_numkey['prediction']=prob_numkey.idxmax(axis=1)
        return prob, prob_numkey, result_df_ls
    else:
        return prob_ls, prob_numkey_ls, result_df_ls


# Cell

#@title knn_1step_prediction
############################## new knn_1step algo #########################################
# if a query user has enough past events in train_df, then use candidate_df = train_df[train_df[user_id] ==query_user_id] & layer_name='i_emb',
# if a query user has not enough past events in train_df, then use candidate_df = train_df & layer_name='iu_emb',
# NOTE: this code is integrated into class Multimodal_Knn_Classification_1step()
############################## new knn_1step algo #########################################


def knn_1step_prediction(test_df:pd.DataFrame,
                         model:tf.keras.models.Model=None, #iu_clf.model,
                         min_candidates_count:int=5,
                         global_vars:tuple=None):
    """make classification predictions on test_df using knn_1step algo
    Args:
      test_df:pd.DataFrame,
      model=iu_clf.model,
      min_candidates_count:int=5
    Returns:
      prob_df:pd.DataFrame the probability predictions of test_df
    """
    #unpack global_vars
    (label, user_id, item_id, x_cols, u_cols, i_cols, cnt_cols, u_cnt_cols, i_cnt_cols,cnt_cols_to_bin, u_cnt_cols_to_bin, i_cnt_cols_to_bin,bin_cnt_cols, u_bin_cnt_cols, i_bin_cnt_cols, cat_cols, u_cat_cols, i_cat_cols, txt_cols, u_txt_cols, i_txt_cols, img_cols, u_img_cols, i_img_cols, dt_cols, u_dt_cols, i_dt_cols, label_str2num,label_num2str, num_classes, label_keys) = global_vars

    prob_ls = []
    for i, query_user_id in enumerate(test_df[user_id]): # for each query user

        # a sample which query user is invited
        query_df = test_df.iloc[i:(i+1),:]

        # candidate samples are chosen to be the same user's past meeting
        candidate_df = train_df[train_df[user_id] ==query_user_id]

        if query_df.shape[0]>0 and candidate_df.shape[0]>=min_candidates_count: # in case there are enough past meetings for the query user
          layer_name='i_emb'
        else: # in case there are NOT enough past meetings for the query user
          candidate_df=train_df # update candidate_df as train_df
          layer_name='iu_emb'

        prob_ls1, prob_ls2, result_df_ls = knn_recommendations_1step(query_df=query_df,
                                                                    candidate_df=candidate_df, #<<<<<<<<<<<< 1
                                                                    top_k=5,
                                                                    model=model,
                                                                    label_str2num=label_str2num,
                                                                    cos_sim_method='sklearn',
                                                                    layer_name=layer_name, #<<<<<<<<<<<<< 2
                                                                    global_vars=global_vars,
                                                                    )
        prob_ls.append(prob_ls2[0])
    prob_df = pd.DataFrame(prob_ls)
    return prob_df




# Cell
#@title Multimodal_Knn_Classification_1step
"""default_config_knn_1step = {
            'global_vars':None,#global_vars,
            'train_df': None,
            'y_train': None,
            'X_train': None,
             #'cnt_cols': None,#cnt_cols,
             #'cat_cols': None,#cat_cols,
             #'txt_cols': None,#txt_cols,
             #'img_cols': None,#img_cols,
             #'bin_cnt_cols': None,#bin_cnt_cols,
             #'label': None,#label,
            'sort_split_by_col': 'start_datetime',
            'epochs': 100,
            'model_path': '/content/drive/My Drive/fast_tfrs/model/',
            'data_path': '/content/drive/My Drive/fast_tfrs/data/',
            'model_name': 'knn_clf_1step',
            'flag_trainable': False,
            'flag_bin_cnt': False,
            'num_bins': 5,
            'embedding_dim': 16,
            'model_type': 'vsn',
            'train_valid_test': [0.7,0.15,0.15],
            'batch_size': 64,
            'max_lr': 0.001,
            'min_lr': 0.001/(2.6)**4,
            'layer_sizes': [128,64,32],
            'projection_dim': 32,
            'l1_l2': 1,
            'alpha': 1e-4,
            'dropout_rate': 0.2,
            'candidate_embs_dic': None,
            'query_embs_dic': None,
            'min_candidates_count': 5,
            'encoding_model': None,
            'num_classes':None,#num_classes,
            'num_projection_layers':3,
            'flag_txt_bert':False ,
            'test_df':None, #used as query_df
            'candidate_embs_dic':None,
            'query_embs_dic':None,
            'top_u':3,
            'top_k':5,
            'cos_sim_method':'sklearn',
            'label_str2num': None,#label_str2num,
            ########################## results fr .fit(), .predict()
            'encoding_model':None,
            'prob':None,
            'prob_numkey':None,
            'result_df_ls':None,
            }"""


class Multimodal_Knn_Classification_1step():
  """construct a knn classifier
  - using the method/idea of few-shot learning
  - using deeplearning embeddings to represent item|user|item_user
  - TBD: being able to learn the importance for each input features/columns
  - TBD: being able to extend to multi-tasks: classification, retrieval, ranking etc.
  Examples:
    knn_clf_1step = Multimodal_Knn_Classification_1step(config= default_knn_clf_1step_config)

    knn_clf_1step.fit(train_df=train_df,
                #X_train=train_df[cnt_cols+cat_cols+txt_cols+img_cols],
                #y_train=train_df[label],
                )

    prob, prob_numkey, sim_df_ls = knn_clf_1step.predict(test_df=test_df)


    #make predictions on test_df using Non-user specific candidates (non-personalized weighted majority voting)
    _prob, _prob_numkey, _sim_df_ls =knn_clf_1step._predict(test_df=test_df)

    # save the knn_clf using built-in method of the class
    knn_clf_1step.save_model(weight_filepath=model_path+'encoding_model_weight',
                    config_filepath=model_path+'encoding_model_config.pkl',
                    knn_config_filepath=model_path+'knn_config.pkl')

  """

  def __init__(self, config=default_config):
      """
      Args:
        encoding_model:tf.keras.models.Model=None,
          - a item-user 2tower classifier trained by train_df, validated by a holdout subset of train_df;
          - In case not provided (None), train encoding_model from stratch using .fit() method
        NOTE:the rest of these attributs (train_df, y_train,...,min_candidates_count) are set to None and put here for the sake of saving & reloading knn_clf

      """
      self.config=config

  def fit(self,
          X_train:pd.DataFrame=None, #train_df[cnt_cols+cat_cols+txt_cols+img_cols],
          y_train:pd.Series=None, #train_df[label],
          train_df:pd.DataFrame=None, #train_df
          ):
      """train a encoding_model, which is a item-user 2towers classifier
      Args:
          NOTE: You can use either train_df (prefered) or X_train, y_train (optional); when both are provided, train_df will be used while (y_train, X_train) will be ignored.
          X_train:pd.DataFrame=None, #train_df[cnt_cols+cat_cols+txt_cols+img_cols],
          y_train:pd.Series=None, #train_df[label],
          train_df:pd.DataFrame=None, #train_df
      """
      # in case df is not provided by user, define df by [y,X]
      # unpack global vars
      (label, user_id, item_id, x_cols, u_cols, i_cols, cnt_cols, u_cnt_cols, i_cnt_cols,cnt_cols_to_bin, u_cnt_cols_to_bin, i_cnt_cols_to_bin,bin_cnt_cols, u_bin_cnt_cols, i_bin_cnt_cols, cat_cols, u_cat_cols, i_cat_cols, txt_cols, u_txt_cols, i_txt_cols, img_cols, u_img_cols, i_img_cols, dt_cols, u_dt_cols, i_dt_cols, label_str2num,label_num2str, num_classes, label_keys) = self.config['global_vars']

      if train_df is None:
         train_df=pd.concat([y_train,X_train], axis=1)
         train_df.rename(columns={0:label}, inplace=True) #make sure label name is correct

      encoding_model = train_valid_eval_iu_classifier(df=train_df,
                                            cnt_cols=cnt_cols,
                                            cat_cols=cat_cols,
                                            txt_cols=txt_cols,
                                            img_cols=img_cols,
                                            bin_cnt_cols=bin_cnt_cols,
                                            label=label,
                                            sort_split_by_col=self.config['sort_split_by_col'], #'start_datetime', #
                                            epochs=self.config['epochs'],
                                            model_path=self.config['model_path'],
                                            model_name=self.config['model_name'], #'iu_2towers_clf',
                                            flag_trainable=self.config['flag_trainable'],
                                            num_bins=self.config['num_bins'],
                                            embedding_dim=self.config['embedding_dim'],
                                            flag_txt_bert=self.config['flag_txt_bert'],
                                            num_projection_layers=self.config['num_projection_layers'],
                                            dropout_rate=self.config['dropout_rate'],
                                            flag_bin_cnt=self.config['flag_bin_cnt'],
                                            model_type=self.config['model_type'],
                                            train_valid_test=self.config['train_valid_test'],
                                            batch_size=self.config['batch_size'],
                                            max_lr=self.config['max_lr'],
                                            min_lr=self.config['min_lr'],
                                            layer_sizes=self.config['layer_sizes'],
                                            projection_dim=self.config['projection_dim'],
                                            l1_l2=self.config['l1_l2'],
                                            alpha=self.config['alpha'],
                                            patience=self.config['patience'],
                                            global_vars=self.config['global_vars'],
                                            loss_name=self.config['clf_loss_name'],
                                            optimizer=self.config['clf_optimizer'],
                                            flag_init_bias_class_weight=self.config['flag_init_bias_class_weight'],
                                            flag_balance_train_df=self.config['flag_balance_train_df'],
                                            flag_SGDRScheduler=self.config['flag_SGDRScheduler'],
                                            monitor=self.config['monitor'],
                                            )
      # saving config to encoding_model will cause RecursionError: maximum recursion depth exceeded in comparison
      # To re-load encoding model together with its hyperparameters
      # encoding_model.config = self.config

      #save the resulted encoding_model to the class object
      self.config['encoding_model'] = encoding_model

      # save inputs to the class object
      self.config['train_df']=train_df
      self.config['y_train'] = y_train
      self.config['X_train'] = X_train

  def predict(self,
              test_df:pd.DataFrame,
              #model=iu_clf.model,
              #min_candidates_count:int=5
              ):
    """make classification predictions on test_df using knn_1step algo
    Args:
      test_df:pd.DataFrame,
      model=iu_clf.model,
      min_candidates_count:int=5
    Returns:
      prob_df:pd.DataFrame the probability predictions of test_df
    Origin: def knn_1step_prediction()
    """
    # unpack global vars
    (label, user_id, item_id, x_cols, u_cols, i_cols, cnt_cols, u_cnt_cols, i_cnt_cols,cnt_cols_to_bin, u_cnt_cols_to_bin, i_cnt_cols_to_bin,bin_cnt_cols, u_bin_cnt_cols, i_bin_cnt_cols, cat_cols, u_cat_cols, i_cat_cols, txt_cols, u_txt_cols, i_txt_cols, img_cols, u_img_cols, i_img_cols, dt_cols, u_dt_cols, i_dt_cols, label_str2num,label_num2str, num_classes, label_keys) = self.config['global_vars']


    prob_ls = []
    result_df_ls=[]
    for i, query_user_id in enumerate(test_df[user_id]): # for each query user

        # a sample which query user is invited
        query_df = test_df.iloc[i:(i+1),:]

        # candidate samples are chosen to be the same user's past meeting
        candidate_df = train_df[train_df[user_id] ==query_user_id]

        if query_df.shape[0]>0 and candidate_df.shape[0]>=self.config['min_candidates_count']: # in case there are enough past meetings for the query user
          layer_name='i_emb'
        else: # in case there are NOT enough past meetings for the query user
          candidate_df=train_df # update candidate_df as train_df
          layer_name='iu_emb'

        prob_ls1, prob_ls2, result_df_s = knn_recommendations_1step(query_df=query_df,
                                                                    candidate_df=candidate_df, #<<<<<<<<<<<< 1
                                                                    top_k=self.config['top_k'],
                                                                    model=self.config['encoding_model'],
                                                                    label_str2num=self.config['label_str2num'],
                                                                    cos_sim_method=self.config['cos_sim_method'],
                                                                    layer_name=layer_name, #<<<<<<<<<<<<< 2
                                                                    global_vars=self.config['global_vars'],
                                                                    )

        prob_ls.append(prob_ls2[0])
        result_df_ls.append(result_df_s[0])
    prob_df = pd.DataFrame(prob_ls)
    #prob_df = prob_df[list(label_num2str.keys())] # reorder columns
    prob_df['prediction'] = prob_df.idxmax(axis=1) # the prediction for each sample is row-wise idxmax

    return prob_df, result_df_ls

  # this OLD implementation is in-efficient: majority of candidate_df is user specific, miority of candidate_df is everybody in train_df
  def _predict(self,
              test_df:pd.DataFrame, #used as query_df
              ):
      """For each query sample in test_df, get the top_k similar samples the same user attended and predict response by majority voting
      Args:
        test_df:pd.DataFrame, with shape (num_querries, num_features); used as query samples
        candidate_embs_dic:dict=None (optional), is built when each user specific candidate_df runs through self.encoding_model to output candidate_embs.
        query_embs_dic:dict=None (optional),
        min_candidates_count:int=5,
        top_u:int=5, the count of similar users to the query user
      NOTE: in case not provided by user, candidate_embs_dic & query_embs_dic will be created from stratch

      Returns:
        prob:pd.DataFrame with string type column names: label_num2str.values()
        prob_numkey:pd.DataFrame with integer type column names: label_num2str.keys()
        sim_df_ls:list of pd.DataFrame, each df keeps the top_k samples from candidate_df most similar to each row in test_df (query sample df)
      """


      # in case not provided by user, create candidate_embs_dic & query_embs_dic from stratch
      if self.config['candidate_embs_dic'] is None or self.config['query_embs_dic'] is None:
          candidate_embs_dic, query_embs_dic = create_candidates_query_embs_dic(model=self.config['encoding_model'],
                                                                                train_df=self.config['train_df'],
                                                                                test_df=test_df,
                                                                                min_candidates_count=self.config['min_candidates_count'],
                                                                                top_u=self.config['top_u'],
                                                                                data_path=self.config['data_path'],
                                                                                )
          self.config['candidate_embs_dic'] = candidate_embs_dic
          self.config['query_embs_dic'] = query_embs_dic

      #save inputs & important intermediate results (which is time consuming to produce)                                                                          )
      self.config['test_df'] = test_df



      #init containers
      prob_ls = []
      prob_numkey_ls = []
      result_df_ls = []

      # for each user in test_df[user_id],compute knn_recommendations
      user_id_ls = list(test_df[user_id])
      for idx, user in enumerate(user_id_ls): #for each user, construct his personalized candidate_df & query_df
          # user-specific candidate_df: the past meeting where user_id (e.g. wuj160@gene.com) was invited
          candidate_df = train_df[train_df[user_id] ==user]
          """ To speed up computation, the following block of code is moved to: def create_candidates_query_embs_dic(...) which returns candidate_embs_dic for each query user & query_embs_dic for each query item
          ###############
          ### edge case: in case query user is not in train_df OR query user does not have enough samples in train_df, then use similar users' samples (i.e. other people's meeting) as candidate_df
          ###############
          if candidate_df.shape[0]<=min_candidates_count:
              print(f'#{idx} query, ========BEFORE: candidate_df.shape[0]={candidate_df.shape[0]} <= {min_candidates_count}: need to reconstruct candidate_df by similar users samples======')

              sim_u_ls = find_similar_x(x=x, #'user'
                              query_df=test_df.iloc[idx:(idx+1),:],
                              candidate_df=train_df,
                              top_n=top_u,
                              model=self.config['encoding_model'])

              candidate_df = sim_u_ls[0]
              print(f'#{idx} query, ========AFTER: candidate_df.shape[0]={candidate_df.shape[0]} >= {min_candidates_count}: well DONE!======')
              """
          # query_df contains info for (query_user, query_item), it represents 1 upcoming meeting where 1 user is invited
          query_df = test_df.iloc[idx:(idx+1),:]

          # user-specific candidates' embedding for candidate_df: the past meeting where user_id (e.g. wuj160@gene.com) was invited
          (candidate_df, candidate_embs) = candidate_embs_dic[user] # candidate_df is updated!!!
          #print(f'+++++++++user={user} candidate_embs = {candidate_embs}')

          # item-specific query's embedding for query_df: the upcoming meeting where user_id is invited
          item = test_df.iloc[idx:(idx+1),:][item_id].values[0]
          (query_df, query_emb) = query_embs_dic[item] # query_df is updated!!!
          #print(f'+++++++++item={item} query_emb = {query_emb}')

          prob_s, prob_numkey_s, result_df_s = knn_recommendations_1step(query_df=query_df,
                                                                          candidate_df=candidate_df,
                                                                          top_k=self.config['top_k'],
                                                                          model=self.config['encoding_model'],
                                                                          label_str2num=self.config['label_str2num'],
                                                                          cos_sim_method=self.config['cos_sim_method'],
                                                                          query_emb=query_emb,
                                                                          candidate_embs=candidate_embs,
                                                                          flag_return_df=False,
                                                                          )
          prob_ls.append(prob_s[0])
          prob_numkey_ls.append(prob_numkey_s[0])
          result_df_ls.append(result_df_s[0])

      label_num2str = {v:k for (k,v) in label_str2num.items()}
      prob = pd.DataFrame(prob_ls)
      prob = prob[list(label_num2str.values())] # reorder columns
      prob['prediction'] = prob.idxmax(axis=1)

      prob_numkey = pd.DataFrame(prob_numkey_ls)
      prob_numkey = prob_numkey[list(label_num2str.keys())] # reorder columns
      prob_numkey['prediction'] = prob_numkey.idxmax(axis=1)

      #save the time consuming results to config
      self.config['prob']=prob
      self.config['prob_numkey']=prob_numkey
      self.config['result_df_ls']=result_df_ls

      return prob, prob_numkey, result_df_ls

  # candidate_df is non-user specific (non-personalized), but from everybody in train_df
  def __predict(self,
              test_df:pd.DataFrame, #query_df
              ):
      """For each query sample in test_df (query_df), get the top_k samples in train_df (candidate_df) that are most similar
      Args:
        test_df:pd.DataFrame, with shape (num_querries, num_features)
      Returns:
        _prob, _prob_numkey : pd.DataFrame, with columns=list(label_num2str.values())
        _sim_df_ls:pd.DataFrame top_k samples from candidate_df (train_df) most similar to query_df (test_df)
      """
      # define variables originally used in function knn_recommendations()
      #ATTN: self.train_df[0] is a pd.DataFrame while self.train_df is a tuple
      train_df = self.config['train_df']

      _prob_ls, _prob_numkey_ls, _result_df_ls = knn_recommendations_1step(query_df=test_df, #:pd.DataFrame,
                                                                        candidate_df=train_df, #:pd.DataFrame,
                                                                        top_k=self.config['top_k'], #:int=5,
                                                                        model=self.config['encoding_model'], #:tf.keras.models.Model=model, get it through self.fit()
                                                                        label_str2num=self.config['label_str2num'], #:dict=label_str2num,
                                                                        cos_sim_method=self.config['cos_sim_method'], #:str='sklearn'
                                                                        flag_return_df=False,
                                                                        )

      label_num2str = {v:k for (k,v) in label_str2num.items()}
      _prob = pd.DataFrame(_prob_ls, columns=list(label_num2str.values()))
      _prob_numkey = pd.DataFrame(_prob_numkey_ls, columns=list(label_num2str.keys()))


      #save the time consuming results to config
      self.config['_prob']=_prob
      self.config['_prob_numkey']=_prob_numkey
      self.config['_result_df_ls']=_result_df_ls

      return  _prob, _prob_numkey, _result_df_ls

    # To easily update compuation latter as needed, implement get_config() and from_config()
    # ref: https://www.tensorflow.org/guide/keras/save_and_serialize
  def get_config(self):
    return self.config

  '''@classmethod
    def from_config(cls, config:dict):
        """config:dict"""
        return clf(**config)
  '''
  def save_model(self,
                weight_filepath:str='knn_clf_1step_encoding_model_weight',
                config_filepath:str='knn_clf_1step_encoding_model_config.pkl',
                #knn_config_filepath:str='knn_clf_1step_config.pkl'
                ):
    """save the model weight & config
    Args:
        model (Multimodal_Knn_Classification): knn_clf after knn_clf.fit() and knn_clf.predict()
    Example:
        model,_,_ = get_best_model()
        save_model(model, weight_filepath='saved_model/best_model_weight', config_filepath=model_path+'knn_config.pkl')
    """
    #STEP1: save encoding_model, which is stored at self.config['encoding_model']
    save_iu_model(iu_model=self.config['encoding_model'],
                  weight_filepath=weight_filepath,
                  config_filepath=config_filepath)

    #STEP2: save config dictionary with config['encoding_model'] set to None

    #save inputs
    self.config['weight_filepath']=weight_filepath
    self.config['config_filepath']=config_filepath
    #self.config['knn_config_filepath']=knn_config_filepath


    # empty some items before you can save the config
    config_dic = self.config.copy() # to keep self.config unchanged, make a copy of it to manipulate with

    #NOTE: the encoding model has been saved at STEP1

    # the following items can NOT be pickle.dump()
    config_dic['encoding_model'], config_dic['clf_metrics'], config_dic['clf_optimizer'], config_dic['rating_metrics'], config_dic['rating_optimizer']  = None, None, None, None, None
    config_dic['interactions']=None
    config_dic['items']=None
    config_dic['users']=None
    config_dic['cached_train']=None
    config_dic['cached_valid']=None
    config_dic['cached_test']=None

    # pickle config with one of its element 'encoding_model' left out.
    import dill as pickle
    with open(config_filepath, 'wb') as handle:
        pickle.dump(config_dic, handle, protocol=pickle.HIGHEST_PROTOCOL)

    print(f"\n=====\n now we already save both config dict(with 'encoding_model' set to None): {config_filepath} \n=====\n save the weight encoding model @ {weight_filepath} and config @ {config_filepath}=====")



#model_path = '/content/drive/My Drive/fast_tfrs/model/'


def load_knn_model(weight_filepath:str,#=model_path+'encoding_model_weight',
                   config_filepath:str,#=model_path+'encoding_model_config.pkl',
                   #knn_config_filepath:str,#=model_path+'knn_config.pkl',
                   flag_knn_clf_1step=True):
    """load knn model, either knn_clf_1step or knn_clf_2steps """
    #########################################################################
    #load encoding_model (a tf model) using tf functional API
    #########################################################################

    #replace: encoding_model = tf.keras.models.load_model(model_path+'encoding_model')
    new_encoding_model = load_iu_model(weight_filepath=weight_filepath,
                                       config_filepath=config_filepath)
    #load the rest of config elements
    import dill as pickle
    with open(config_filepath, 'rb') as handle:
        new_knn_config = pickle.load(handle)

    #restore config['encoding_model']
    new_knn_config['encoding_model'] = new_encoding_model

    #########################################################################
    # use 'loss_name' to restore 'loss', 'metrics', 'optimizer' by loss_dict
    #########################################################################

    loss_name = new_knn_config['clf_loss_name']
    # define loss_dict which maps loss_name to real loss function
    loss_dict = {'SparseCategoricalFocalLoss':SparseCategoricalFocalLoss(gamma=2),
                 'SparseCategoricalCrossentropy':tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                 'CategoricalCrossentropy': tf.keras.losses.CategoricalCrossentropy(from_logits=False),
                 'SigmoidFocalCrossEntropy': tfa.losses.SigmoidFocalCrossEntropy(from_logits = False,
                                                            alpha= 0.25,
                                                            gamma= 2.0,
                                                            reduction=tf.keras.losses.Reduction.AUTO,
                                                            name = 'sigmoid_focal_crossentropy'
                                                            ),
                 }
    if loss_name in ['CategoricalCrossentropy','SigmoidFocalCrossEntropy']:
      flag_one_hot_encode_label=True
      metrics=METRICS
    else: # loss_name in ['SparseCategoricalCrossentropy','SparseCategoricalFocalLoss',, ]
      flag_one_hot_encode_label=False
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]


    new_knn_config['clf_metrics']=metrics
    new_knn_config['clf_loss']=loss_dict[loss_name]
    new_knn_config['clf_optimizer'] = tf.keras.optimizers.Adam(learning_rate=0.1)
    #re-create knn_clf by config
    if flag_knn_clf_1step:
        new_knn_clf = Multimodal_Knn_Classification_1step(config=new_knn_config)
    else:
        new_knn_clf = Multimodal_Knn_Classification_2steps(config=new_knn_config)
    return new_knn_clf



# Cell
#@title create_candidates_query_embs_dic
# to speed up the computation, compute the time consuming steps ahead separately before running
def create_candidates_query_embs_dic(model:tf.keras.models.Model,
                                     train_df:pd.DataFrame,
                                     test_df:pd.DataFrame,
                                     min_candidates_count:int, #=5,
                                     data_path:str, #=data_path,
                                     top_u:int,#=5
                                     ):
    """create item-embedding for each past item which the query user who participated in the past (train_df): candidate_i_embs_dic;
       create item-embedding for each unique item which the query user will participate in upcoming meeting (test_df): query_i_embs_dic;
    Args:
      model:tf.keras.models.Model a item-user 2towers classifier, model = train_valid_eval_iu_classifier(...) or after knn_clf.fit(...), get model = knn_clf.encoding_model
      train_df:pd.DataFrame, which stores the past meetings
      test_df:pd.DataFrame, which stores the upcoming meetings
      min_candidates_count:int=5, in case that a user's past meetings (e.g. user-specific candidates) count
                           is smaller than this number, use other similar user's meetings as this user's candidates.
      data_path:str=data_path folder to save candidate_embs_dic and query_embs_dic
      top_u:int=5, the number of others users similar to the query user
    Returns:
      candidate_i_embs_dic, the dict (key:value) pairs follows this format (user:candidate_embs)
      query_i_embs_dic, the dict (key:value) pairs follow this format (item:query_emb)
    Examples:
      candidate_i_embs_dic, query_i_embs_dic = create_candidates_query_embs_dic(model=model, train_df=train_df, test_df=test_df, min_candidates_count=5)
      #dbck: how many unique user-specific candidate embs, how many item-specific query emb
      print(len(candidate_i_embs_dic.keys()), len(query_i_embs_dic.keys()))
      print(len(candidate_i_embs_dic.keys()) == test_df[user_id].unique().shape[0], len(query_i_embs_dic.keys()) == test_df[item_id].unique().shape[0])
    """
    ##############################start:calculate|load 1)user-specific candidate_embs dictionary
    # if not provided, create them from stratch
    from collections import defaultdict
    #calculate the item-embs for those items in train_df
    """train_df_i_embs = samples_to_embeddings(sample_df=train_df,
                                          model=model,
                                          layer_name='i_emb',
                                          global_vars=global_vars,
                                          )"""

    #if candidate_i_embs_dic is None:
    candidate_i_embs_dic = defaultdict(lambda: None)

    # for debugging purpose, keep those query user that can NOT be embedded
    buggy_query_df_ls = []

    #for each unique user in test_df, calculate user-specific candidate embedding
    for user in test_df[user_id].unique():
        if candidate_i_embs_dic[user] is None: #if candidate_i_embs_dic[user] has not been created before
            candidate_df = train_df[train_df[user_id] ==user]
            ###############
            ### edge case: in case query user is not in train_df OR query user does not have enough samples in train_df, then use similar users' samples (i.e. other people's meeting) as candidate_df
            ###############

            if candidate_df.shape[0]<=min_candidates_count:
                print(f'For query user = {user},\n ========BEFORE: candidate_df.shape[0]={candidate_df.shape[0]} <= {min_candidates_count}: need to reconstruct candidate_df attended by similar users======')
                query_df = test_df[test_df[user_id]==user].iloc[0:1,:]

                try:
                  sim_u_ls = find_similar_x(x='user',
                                  query_df=query_df, #just need 1 sample in test_df where query user is invited to attend
                                  candidate_df=train_df,
                                  top_n=top_u,
                                  model=model)

                  candidate_df = sim_u_ls[0]
                  candidate_i_embs_dic[user] = (candidate_df, samples_to_embeddings(sample_df=candidate_df,
                                                                  model=model,
                                                                  layer_name='i_emb',
                                                                  global_vars=global_vars,
                                                                  ))
                  print(f'For query user ={user},\n ========AFTER: candidate_df.shape[0]={candidate_df.shape[0]} >= {min_candidates_count}: well DONE!======\n')
                except:
                  buggy_query_df_ls.append(query_df)
                  candidate_df = train_df
                  candidate_i_embs_dic[user] = (candidate_df, samples_to_embeddings(sample_df=candidate_df,
                                                                  model=model,
                                                                  layer_name='i_emb',
                                                                  global_vars=global_vars))
                  print("=============ATTENTION============== Bug in query_emb = samples_to_embeddings(query_df, layer_name='u_emb'); use train_df as candidate ======")
                  print(f'For query user ={user},\n ========AFTER: candidate_df.shape[0]={candidate_df.shape[0]} >= {min_candidates_count}: half-well DONE!======\n')


            else: # in case candidate_i_embs_dic has been created before
                # calculate candidates' embedding vectors & save it into candidate_embs_dic
                candidate_i_embs_dic[user] = (candidate_df, samples_to_embeddings(sample_df=candidate_df,
                                                                 model=model,
                                                                 layer_name='i_emb',
                                                                 global_vars=global_vars))

    # pickle candidate_embs_dic
    import pickle

    with open(data_path+'candidate_embs_dic.pickle', 'wb') as handle:
        pickle.dump(dict(candidate_i_embs_dic), handle, protocol=pickle.HIGHEST_PROTOCOL)

    #if query_i_embs_dic is None:
    query_i_embs_dic = defaultdict(lambda: None)

    #for each unique item(meeting) in test_df, calculate item-specific query_emb
    for item in test_df[item_id].unique():
        if query_i_embs_dic[item] is None: #if it has not been created before
            query_df = test_df.loc[test_df[item_id] ==item,:].iloc[0:1,:] #select 1 sample
            print(f'item={item} specific >>>>>>> query_df={query_df}')
            # calculate query embedding vector
            query_i_embs_dic[item] = (query_df, samples_to_embeddings(sample_df=query_df,
                                                                      model=model,
                                                                      layer_name='i_emb',
                                                                      global_vars=global_vars))
    # pickle query_embs_dic
    with open(data_path+'query_i_embs_dic.pickle', 'wb') as handle:
        pickle.dump(dict(query_i_embs_dic), handle, protocol=pickle.HIGHEST_PROTOCOL)

    # pickle buggy_query_df_ls
    with open(data_path+'buggy_query_df_ls.pickle', 'wb') as handle:
        pickle.dump(buggy_query_df_ls, handle, protocol=pickle.HIGHEST_PROTOCOL)
    print(f'===== buggy_query_df_ls can be  un-pickled here: {data_path}'+'buggy_query_df_ls.pickle =====')
    print(f"import dill as pickle\nwith open('{data_path}buggy_query_df_ls.pickle', 'rb') as handle:\n    buggy_query_df_ls = pickle.load(handle)")

    return dict(candidate_i_embs_dic), dict(query_i_embs_dic)


# Cell
#@title knn_recommendations_2steps
def knn_recommendations_2steps(query_df:pd.DataFrame,#=test_df, # query_df is composed of query user & item
                                candidate_df:pd.DataFrame,#=train_df,
                                top_u:int,#=3, #find top_u users similar to query user
                                top_i:int,#=5, #find top_i items similar to query item
                                model:tf.keras.models.Model,
                                label_str2num:dict,#=label_str2num,
                                flag_print:bool=True,
                                flag_return_df:bool=True,
                                global_vars:tuple=None,
                                x_u:str='user',
                                x_i:str='item_user',
                                ):
    """recommend action for each query sample by a 2steps process:
    Given query sample = query user + query item,
    STEP1 get similar users (fr. candidate_df) to query user(and all their past items as candidate samples for STEP2)
    STEP2 get similar items (fr. STEP1 resulted candidate samples) to query item

    Args:
      query_df:pd.DataFrame=test_df, # query_df is composed of features for query user & query item
      candidate_df:pd.DataFrame=train_df, # represent all past meetings
      top_u:int=3, #find top_u users similar to query user
      top_i:int=5, #find top_i items similar to query item
      model:tf.keras.models.Model=None,# iu_clf.model OR model = train_valid_eval_iu_classifier(...)
      label_str2num:dict=label_str2num, #a dictionary (key:value) mapping label from string (key) to number (value)
      flag_print:bool=True,#to show intermediate results or not
      x_u:str='user', the x argument in find_similar_x() in step_1
      x_i:str='item_user', the x argument in find_similar_x() in step_2, options include ['item', 'item-user']
    Returns:
      prob:pd.DataFrame showing prediction for each query sample and the probability of each action (accepted/declined/needAction); the probabilities are weighted by query_candidate item similarity; label is represented by string
      prob_numkey:pd.DataFrame, similiar to prob (above), but here lable is represented by integer instead of string
      result_df_ls:list of result_df, each result_df stores the candidate samples most similar to the query sample, sorted by similarity in decending order
    Examples:
      query_df = test_df.iloc[0:10,:]
      prob, prob_numkey, result_df_ls = knn_recommendations_2steps(query_df=query_df, # query_df is composed of query user & item
                                                                candidate_df=train_df,
                                                                top_u=5, #find top_u users similar to query user
                                                                top_i=7, #find top_i items similar to query item
                                                                model=iu_clf.model,
                                                                label_str2num=label_str2num,
                                                                flag_print=False,
                                                                flag_return_df=True,
                                                                )


    """
    # unpack global vars
    (label, user_id, item_id, x_cols, u_cols, i_cols, cnt_cols, u_cnt_cols, i_cnt_cols,cnt_cols_to_bin, u_cnt_cols_to_bin, i_cnt_cols_to_bin,bin_cnt_cols, u_bin_cnt_cols, i_bin_cnt_cols, cat_cols, u_cat_cols, i_cat_cols, txt_cols, u_txt_cols, i_txt_cols, img_cols, u_img_cols, i_img_cols, dt_cols, u_dt_cols, i_dt_cols, label_str2num,label_num2str, num_classes, label_keys) = global_vars


    # STEP1: find top_u similar users to the query user according to u_cols ('u_emb') similarity
    #x = ['item', 'user', 'item_user'][1]
    x2x_cols = {'user':u_cols, 'item':i_cols, 'item_user':i_cols + u_cols}
    x2layer_name = {'user':'u_emb', 'item':'i_emb', 'item_user':'iu_emb'}
    x2x_id = {'user':[user_id], 'item':[item_id], 'item_user':[item_id, user_id]}

    sim_u_ls = find_similar_x(x=x_u, #i.e. 'user'
                            query_df=query_df,
                            candidate_df=candidate_df, #i.e. train_df
                            top_n=top_u,
                            model=model,
                            global_vars=global_vars,
                            )
    if flag_print:
      pd.set_option('display.max_rows', None)
      pd.set_option('display.max_columns', None)
      pd.set_option('display.width', 1000)
      pd.set_option('display.colheader_justify', 'center')
      pd.set_option('display.precision', 3)
      from IPython.display import display
      for i in range(query_df.shape[0]):
        print(f"--------query {x} ------")
        display(query_df.iloc[i:(i+1),:])
        print(f"========the id of similar {x}s========")
        print(set(sim_u_ls[i][x2x_id[x][0]]))
        print(f"========the samples of similar {x}s========")
        display(sim_u_ls[i][['similarity']+x_cols])

    # STEP2: find top_i similar item-user(s) to the query item accroding to i_cols ('i_emb') similarity
    # init placeholders
    result_df_ls = [] # to host candidate samples most similar to *each* query sample
    prob_ls = [] #to host ez to understand probability prediction by knn, using str key
    prob_numkey_ls = [] #to host ez to understand probability prediction by knn, using num key

    #construct a dictionary that maps label from num back to str; ATTN: b/c str2num is NOT 1:1, this reverse mapping lost info
    label_num2str = {v:k for (k,v) in label_str2num.items()}

    #x= ['item', 'user', 'item_user'][-1]
    for u in range(len(sim_u_ls)):
        sim_i_ls = find_similar_x(x=x_i, #'item_user'
                                query_df=query_df.iloc[u:(u+1),:], # query_dfthe same
                                candidate_df=sim_u_ls[u], # candiate_df is changed to sim_x_ls[0]
                                top_n=top_i,
                                model=model,
                                global_vars=global_vars)
        if flag_print:
              print(f"--------query {x}------")
              display(query_df.iloc[i:(i+1),:])
              print(f"========the id of similar {x}s========")
              print(set(sim_i_ls[0][x2x_id[x][0]]))
              print(f"========the sample of similar {x}s========")
              display(sim_i_ls[0][['similarity']+x_cols])
        result_df = sim_i_ls[0] #b/c sim_i_ls is a list of 1 df (there is only 1 df in that list)
        ### calculate probability used for majority vote
        #calculate probabilities of different responses
        prob_majority_vote = dict(result_df[label].value_counts()/result_df.shape[0])

        ### Now calculate prob used for weighted majority vote
        dic = {key: result_df.groupby(label)['similarity'].agg(list)[key] for key in set(result_df[label])}
        ll = [v for v in dic.values()] # a list of lists
        ls = [x for l in ll for x in l] # flatten a list of lists
        s = sum(ls) # similarity sum

        # weighted majority vote: each candidate's vote is weighted by its similarity to query sample
        prob_weighted_vote = {k:sum(v)/s for (k,v) in dic.items()}

        # easy to understand prediction
        prob = {label_num2str[k]:v for (k,v) in prob_weighted_vote.items()}

        # extend prob dict to include all the keys 'accepted', 'declined', 'needAction'
        for key in set(label_num2str.values()) - set(prob.keys()):
            prob[key]=0
        # using label_str2num, to map each key in prob fr str to num:
        prob_numkey={label_str2num[k]:v for (k,v) in prob.items()}

        # save the result for each query sample:
        result_df_ls.append(result_df)
        prob_ls.append(prob)
        prob_numkey_ls.append(prob_numkey)
    if flag_return_df:
        prob = pd.DataFrame(prob_ls)
        prob['prediction'] = prob.idxmax(axis=1)

        prob_numkey = pd.DataFrame(prob_numkey_ls)
        prob_numkey['prediction'] = prob_numkey.idxmax(axis=1)
        return prob, prob_numkey, result_df_ls
    else:
        return prob_ls, prob_numkey_ls, result_df_ls

# Cell
#@title Multimodal_Knn_Classification_2steps

class Multimodal_Knn_Classification_2steps():
  """construct a knn classifier
  - using the method/idea of few-shot learning
  - using deeplearning embeddings to represent item|user|item_user
  - TBD: being able to learn the importance for each input features/columns
  - TBD: being able to extend to multi-tasks: classification, retrieval, ranking etc.

  Examples:
    knn_clf_2steps = Multimodal_Knn_Classification_2steps(config=default_config)

    knn_clf_2steps.fit(train_df=train_df,
                      valid_df=valid_df,
                      test_df=test_df)

    prob, prob_num, result_df_ls = knn_clf_2steps.predict(test_df=test_df, #used as query_df
                                                          train_df=train_df, #used as candidate_df
                                                          top_u=5, #find top_u users similar to query user
                                                          top_i=7, #find top_i items similar to query item
                                                          label_str2num=label_str2num,
                                                          flag_print=False,)

    print(knn_clf_2steps.config)
    print(prob.shape, prob_numkey.shape, len(result_df_ls), test_df.shape)

    knn_clf_2steps.save_model(
                weight_filepath=model_path+'encoding_model_weight',
                config_filepath=model_path+'encoding_model_config.pkl',
                knn_config_filepath=model_path+'knn_config.pkl')


    new_knn_clf_2steps= load_2steps_knn_model(
                    weight_filepath=model_path+'encoding_model_weight',
                    config_filepath=model_path+'encoding_model_config.pkl',
                    knn_config_filepath=model_path+'knn_config.pkl')
  """

  def __init__(self, config:dict=default_config):
      """
      Args:
        encoding_model:tf.keras.models.Model=None,
          - a item-user 2tower classifier trained by train_df, validated by a holdout subset of train_df;
          - In case not provided (None), train encoding_model from stratch using .fit() method
        NOTE:the rest of these attributs (train_df, y_train,...,min_candidates_count) are set to None and put here for the sake of saving & reloading knn_clf

      """
      self.config = config
      #self.encoding_model = None # to store iu_clf.model at .fit() #now saved in self.config['encoding_model']

  def fit(self,
          X_train:pd.DataFrame=None, #train_df[cnt_cols+cat_cols+txt_cols+img_cols],
          y_train:pd.Series=None, #train_df[label],
          train_df:pd.DataFrame=None,
          valid_df:pd.DataFrame=None,
          test_df:pd.DataFrame=None,
          flag_balance_train_df:bool=True,
          flag_init_bias_class_weight:bool=True,
          flag_SGDRScheduler:bool=False,
          ):

      """train a encoding_model, which is a item-user 2towers classifier iu_clf.model
      Args:
          NOTE: You can use either train_df (prefered) or X_train, y_train (optional); when both are provided, train_df will be used while (y_train, X_train) will be ignored.
          X_train:pd.DataFrame=None, #train_df[cnt_cols+cat_cols+txt_cols+img_cols],
          y_train:pd.Series=None, #train_df[label],
          train_df:pd.DataFrame=None,
          valid_df:pd.DataFrame=None,(optional)
          test_df:pd.DataFrame=None, (optional)
      """
      ### create encoding model
      # init iu_clf, an instance of ItemUserTwoTowersClassifier()
      iu_clf = ItemUserTwoTowersClassifier(config=self.config)

      #train iu_clf
      iu_clf.fit(train_df=train_df,
                valid_df=valid_df,
                test_df=test_df,
                flag_balance_train_df=flag_balance_train_df,
                flag_init_bias_class_weight=flag_init_bias_class_weight,
                flag_SGDRScheduler=flag_SGDRScheduler,
                )

      # save iu_clf, an instance of ItemUserClassifier class
      iu_clf.save_model(
                      weight_filepath=model_path+'encoding_model_weight',
                      config_filepath=model_path+'encoding_model_config.pkl')

      #save the resulted encoding_model to the class object
      #self.encoding_model = iu_clf.model
      self.config['encoding_model'] = iu_clf.model
      self.config['X_train']=X_train #train_df[cnt_cols+cat_cols+txt_cols+img_cols],
      self.config['y_train']=y_train #train_df[label],
      self.config['train_df']=train_df
      self.config['valid_df']=valid_df
      self.config['test_df']=test_df


  # candidate_df is user specific, NOT from everybody in train_df
  def predict(self,
              test_df:pd.DataFrame, #used as query_df
              top_u:int,#=5, #find top_u users similar to query user
              top_i:int,#=7, #find top_i items similar to query item
              label_str2num:dict,#=label_str2num,
              flag_print:bool=False,
              ):
      """For each query sample in test_df, get the top_k similar samples the same user attended and predict response by majority voting
      Args:
        test_df:pd.DataFrame, with shape (num_querries, num_features); used as query samples
        candidate_embs_dic:dict=None (optional), is built when each user specific candidate_df runs through self.encoding_model to output candidate_embs.
        query_embs_dic:dict=None (optional),
        min_candidates_count:int=5,
      NOTE: in case not provided by user, candidate_embs_dic & query_embs_dic will be created from stratch

      Returns:
        prob:pd.DataFrame with string type column names: label_num2str.values()
        prob_numkey:pd.DataFrame with integer type column names: label_num2str.keys()
        sim_df_ls:list of pd.DataFrame, each df keeps the top_k samples from candidate_df most similar to each row in test_df (query sample df)
      """

      prob, prob_numkey, result_df_ls = knn_recommendations_2steps(query_df=test_df, # query_df is composed of (query user, query item)
                                                          candidate_df=self.config['train_df'],
                                                          top_u=top_u, #find top_u users similar to query user
                                                          top_i=top_i, #find top_i items similar to query item
                                                          model=self.config['encoding_model'],
                                                          label_str2num=label_str2num,
                                                          flag_print=flag_print,
                                                          flag_return_df=True,
                                                          global_vars=self.config['global_vars'],
                                                          x_u='user',
                                                          x_i='item_user',
                                                          )

      #save .predict() inputs
      self.config['top_u']=top_u
      self.config['top_i']=top_i
      self.config['label_str2num']=label_str2num
      #save the time consuming results to config
      self.config['prob']=prob
      self.config['prob_numkey']=prob_numkey
      self.config['result_df_ls']=result_df_ls

      return prob, prob_numkey, result_df_ls

    # To easily update compuation latter as needed, implement get_config() and from_config()
    # ref: https://www.tensorflow.org/guide/keras/save_and_serialize
  def get_config(self):
    return self.config

  '''@classmethod
    def from_config(cls, config:dict):
        """config:dict"""
        return clf(**config)
  '''
  def save_model(self,
                weight_filepath:str='knn_clf_2steps_encoding_model_weight',
                config_filepath:str='knn_clf_2steps_encoding_model_config.pkl',
                #knn_config_filepath:str='knn_clf_2steps_config.pkl',
                 ):
    """save the model weight & config
    Args:
        model (Multimodal_Knn_Classification): knn_clf after knn_clf.fit() and knn_clf.predict()
    Example:
        model,_,_ = get_best_model()
        save_model(model, weight_filepath='saved_model/best_model_weight', config_filepath=model_path+'knn_config.pkl')
    """
    ###################################
    #0) save self.encoding_model
    save_iu_model(iu_model=self.config['encoding_model'],
                  weight_filepath=weight_filepath,
                  config_filepath=config_filepath)

    # temperatively remove encoding_model from config by setting config['encoding_model'] = None
    config_dic = self.config.copy()

    #NOTE: the encoding model has been saved at STEP1
    # the following tf element can not be pickle.dump()
    config_dic['encoding_model'], config_dic['clf_metrics'], config_dic['clf_optimizer'], config_dic['rating_metrics'], config_dic['rating_optimizer']  = None, None, None, None, None
    config_dic['interactions']=None
    config_dic['items']=None
    config_dic['users']=None
    config_dic['cached_train']=None
    config_dic['cached_valid']=None
    config_dic['cached_test']=None

    # pickle config with one of its element 'encoding_model' left out.
    import dill as pickle
    with open(config_filepath, 'wb') as handle:
        pickle.dump(config_dic, handle, protocol=pickle.HIGHEST_PROTOCOL)

    # after saving config, restore self.config['encoding_model'] = self.encoding_model
    # self.config['encoding_model'] = self.encoding_model
    print(f"\n=====\n now we already save both config dict(without 'encoding_model'): {config_filepath} \n=====\n save knn_clf.config['encoding_model']'s weight @ {weight_filepath} and config @ {config_filepath}=====")

    #save inputs
    self.config['weight_filepath']=weight_filepath
    self.config['config_filepath']=config_filepath
    #self.config['knn_config_filepath']=knn_config_filepath

def create_knn_clf_2steps(df, config=default_config):
    """instantiate a DeepCrossMultitaskModel model and compile it
    Args:
        config (dict): the configuration dictionary to specify Multimodal_Knn_Classification(), which need to be consistent with model.get_config().
    Return:
        model (Multimodal_Knn_Classification): a compiled model
    Example:
    """
    model = Multimodal_Knn_Classification_2steps(config=config)
    #model.compile(optimizer=tf.keras.optimizers.Adagrad(config['lr']))
    return model



# Cell

#@title embedding functions for cnt, cat, txt, img features
def cat_embedding_model(df:pd.DataFrame, user_id:str, item_id:str, cat_col:str, flag_use_hashing:bool=False, interactions=None, embedding_dim:int=32):
    """To make a cat_col embedding model that can turn each value of cat_col into the corresponding embedding vectors
    Args:
        df (pd.DataFrame): e.g. df, or interactions_df
        user_id (str): e.g. user_id
        item_id (str): e.g. item_id
        cat_col (str): e.g. item_id, 'job_family'
        flag_use_hashing (bool, default False): indicates using hashing method or not
        interactions (TFDS dataset, default None): it speeds up when using prefab interactions instead of making it on spot from strach
        embedding_dim (int default 32): embedding vector dimension
    Returns:
        cat_emb_model (tf.keras.Sequential())

    Examples:
        cat_emb_model = cat_embedding_model(df, user_id=user_id, item_id=item_id, cat_col=user_id, flag_use_hashing=False)
        cat_emb_model(['kingwoot@gene.com', 'yua20@gene.com'])
    """
    if not flag_use_hashing: #method1: use data-specific-vocabularies


        # define data-specific vocabularies
        cat_col_lookup = tf.keras.layers.StringLookup()
        try:cat_col_lookup.adapt(interactions.map(lambda x: x[cat_col]))
        except:
            print(f'===========build interactions for cat_col={cat_col} fr. sctrach==========')
            interactions_df = df.groupby(list(set([user_id, item_id])|set([cat_col])))[label].sum().reset_index()
            interactions_df[cat_col] = interactions_df[cat_col].astype('str')
            interactions_dict = {name: np.array(value) for name, value in interactions_df.items()}
            interactions_tfds = tf.data.Dataset.from_tensor_slices(interactions_dict)

            interactions = interactions_tfds.map(lambda x: {
                                                        user_id : x[user_id],
                                                        item_id : x[item_id],
                                                        label : float(x[label]),
                                                        cat_col: x[cat_col]})
            cat_col_lookup.adapt(interactions.map(lambda x: x[cat_col]))

        # compute cat_col embedding layer
        cat_col_embedor = tf.keras.layers.Embedding(input_dim=cat_col_lookup.vocabulary_size(),
                                                      output_dim=embedding_dim)
        # build cat_col embedding model
        cat_emb_model = tf.keras.Sequential([cat_col_lookup,
                                            cat_col_embedor,
                                            #tf.keras.layers.Dense(embedding_dim), #newly add
                                             ])
    else: #method2: use non-data-specific vocabulary size by hashing

        # We set up a large number of bins to reduce the chance of hash collisions.
        num_hashing_bins = 200_000

        # define non-data-specific vocabularies by hashing
        hashing_lookup = tf.keras.layers.Hashing(
            num_bins=num_hashing_bins
        )
        # compute cat_col embedding
        cat_col_embedor = tf.keras.layers.Embedding(input_dim=num_hashing_bins,
                                                      output_dim=embedding_dim)

        # build cat_col embedding model
        cat_emb_model = tf.keras.Sequential([hashing_lookup,
                                            cat_col_embedor,
                                            #tf.keras.layers.Dense(embedding_dim), #newly add
                                             ])
    return cat_emb_model

def cnt_embedding_model(df:pd.DataFrame, user_id:str, item_id:str, cnt_col:str, n_bucket:int, interactions=None, embedding_dim=32):
    """To make a cnt_col embedding model that can bucketize each value of cnt_col and turn bucketized value into corresponding embedding vectors
    Args:
        df (pd.DataFrame): e.g. interactions_df
        user_id (str): e.g. user_id
        item_id (str): e.g. item_id
        cnt_col (str): e.g. 'age', 'timestamp'
        n_bucket (int): e.g. 10
        interactions (TFDS dataset, default None): it speeds up when using prefab interactions instead of making it on spot from strach
        embedding_dim (int default 32): embedding vector dimension
    Returns:
        cnt_emb_model (tf.keras.Sequential())

    Examples:
        cnt_emb_model = cnt_embedding_model(df, user_id=user_id, item_id=item_id, cnt_col='timestamp', n_bucket=1000, interactions=interactions)
        cnt_emb_model([1619186400.0, 1619189000.0])

        cnt_emb_model = cnt_embedding_model(df, user_id=user_id, item_id=item_id, cnt_col='age', n_bucket=5, interactions=interactions)
        cnt_emb_model([23.0, 24.0, 57.5, 58.3])
    """


    #step1:Bucketize
    try:cnt_col_ls = np.concatenate(list(interactions.map(lambda x: x[cnt_col]).batch(100)))
    except:
        print(f'===========build interactions for cnt_col={cnt_col} fr. sctrach==========')
        interactions_df = df.groupby(list(set([user_id, item_id])|set([cnt_col])))[label].sum().reset_index()
        interactions_df[cnt_col] = interactions_df[cnt_col].astype('float64')
        interactions_dict = {name: np.array(value) for name, value in interactions_df.items()}
        interactions_tfds = tf.data.Dataset.from_tensor_slices(interactions_dict)

        interactions = interactions_tfds.map(lambda x: {
                                                    user_id : x[user_id],
                                                    item_id : x[item_id],
                                                    label : float(x[label]),
                                                    cnt_col: x[cnt_col]})
        cnt_col_ls = np.concatenate(list(interactions.map(lambda x: x[cnt_col]).batch(100)))

    #max_cnt_col = interactions.map(lambda x: x[cnt_col]).reduce(tf.cast(0, tf.float64), tf.maximum).numpy().max()
    #min_cnt_col = interactions.map(lambda x: x[cnt_col]).reduce(np.float64(1e9), tf.minimum).numpy().min()
    max_cnt_col = cnt_col_ls.max()
    min_cnt_col = cnt_col_ls.min()
    cnt_col_buckets = np.linspace(
        min_cnt_col, max_cnt_col, num=n_bucket)

    #dbck: print(f"Buckets: {cnt_col_buckets[:5]}")

    #step2: turn bucketized number into its embedding
    cnt_emb_model = tf.keras.Sequential([
    tf.keras.layers.Discretization(cnt_col_buckets.tolist()),
    tf.keras.layers.Embedding(len(cnt_col_buckets) + 1, embedding_dim) # adding 1 to the known number of buckets to deal with outliners
  ])
    """#dbck: expect the same output b/c of all 3 inputs are bucketized into the same bin
    for cnt_val in interactions.take(3).map(lambda x: x[cnt_col]).batch(1).as_numpy_iterator():
        print(f"{cnt_col}={cnt_val} embedding: {cnt_emb_model(cnt_val)}.")
        """
    return cnt_emb_model

def cnt_normalize_model(df:pd.DataFrame, user_id:str, item_id:str, cnt_col:str, interactions=None):
    """To make a model that normalizes cnt_col in df
    Args:
        df (pd.DataFrame): e.g. interactions_df
        user_id (str): e.g. user_id
        item_id (str): e.g. item_id
        cnt_col (str): e.g. 'age', 'timestamp'
        interactions (TFDS dataset, default None): it speeds up when using prefab interactions instead of making it on spot from strach

    Returns:
        cnt_norm_model (tf.keras.layers.Normalization)

    Examples:
        cnt_norm_model = cnt_normalize_modell(df, user_id=user_id, item_id=item_id, cnt_col='timestamp')
        cnt_norm_model([1619186400.0, 1619189000.0])

        cnt_norm_model = cnt_normalize_model(df, user_id=user_id, item_id=item_id, cnt_col='age')
        cnt_norm_model([23.0, 24.0, 57.5, 58.3])
    """

    cnt_col_normalization = tf.keras.layers.Normalization(axis=None)
    try:
        cnt_col_ls = np.concatenate(list(interactions.map(lambda x: x[cnt_col]).batch(1024)))
    except:
        print(f'===========build interactions for cnt_col={cnt_col} fr. sctrach==========')
        interactions_df = df.groupby(list(set([user_id, item_id])|set([cnt_col])))[label].sum().reset_index()
        interactions_df[cnt_col] = interactions_df[cnt_col].astype('float64')
        interactions_dict = {name: np.array(value) for name, value in interactions_df.items()}
        interactions_tfds = tf.data.Dataset.from_tensor_slices(interactions_dict)

        interactions = interactions_tfds.map(lambda x: {
                                                    user_id : x[user_id],
                                                    item_id : x[item_id],
                                                    label : float(x[label]),
                                                    cnt_col: x[cnt_col]})
        cnt_col_ls = np.concatenate(list(interactions.map(lambda x: x[cnt_col]).batch(1024)))
    cnt_col_normalization.adapt(cnt_col_ls)


    return cnt_col_normalization

def txt_embedding_model(df:pd.DataFrame, user_id:str, item_id:str, txt_col:str, interactions=None, embedding_dim=32):
    """To make a txt_col embedding model that turn each value of cnt_col into its corresponding embedding vectors
    Args:
        df (pd.DataFrame): e.g. interactions_df
        user_id (str): e.g. user_id
        item_id (str): e.g. item_id
        txt_col (str): e.g. 'title', 'description'
        interactions (TFDS dataset, default None): it speeds up when using prefab interactions instead of making it on spot from strach
        embedding_dim (int default 32): embedding vector dimension
    Returns:
        txt_emb_model (tf.keras.Sequential())

    Example:
        txt_emb_model = txt_embedding_model(df, user_id=user_id, item_id=item_id, txt_col='title')
        txt_emb_model(['Alex/Paul 1:1', 'Chase/Danielle weekly meeting', 'Elva/Paul biweekly meeting'])
    """

    #step1: text tokenization (splitting into constituent words or word-pieces),
    txt_vectorizer = tf.keras.layers.TextVectorization()

    #step2: vocabulary learning
    try:txt_vectorizer.adapt(interactions.map(lambda x: x[txt_col]))
    except: #in case interactions is None and need to make it fr. stratch
        print(f'===========build interactions for txt_col={txt_col} fr. sctrach==========')
        interactions_df = df.groupby(list(set([user_id, item_id])|set([txt_col])))[label].sum().reset_index()
        interactions_df[txt_col] = interactions_df[txt_col].astype('str')
        interactions_dict = {name: np.array(value) for name, value in interactions_df.items()}
        interactions_tfds = tf.data.Dataset.from_tensor_slices(interactions_dict)

        interactions = interactions_tfds.map(lambda x: {
                                                    user_id : x[user_id],
                                                    item_id : x[item_id],
                                                    label : float(x[label]),
                                                    txt_col: x[txt_col]})
        txt_vectorizer.adapt(interactions.map(lambda x: x[txt_col]))
    #step3: embedding.
    max_tokens = 1_000_000
    txt_emb_model = tf.keras.Sequential([
          txt_vectorizer,
          tf.keras.layers.Embedding(max_tokens, embedding_dim, mask_zero=True),
          # We average the embedding of individual words to get one embedding vector per sentence/document.
          tf.keras.layers.GlobalAveragePooling1D(),
        ])
    return txt_emb_model


def img_embedding_model(num_projection_layers:int=2, projection_dim:int=16, dropout_rate:float=0.2, flag_trainable:bool=False, img_col:str='', img_module_url:str="https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4"):
    """ create a img embedding model
    Args:
      num_projection_layers:int=2,
      projection_dim:int=16,
      dropout_rate:float=0.2,
      flag_trainable:bool=False,
      img_col:str='',
      img_module_url:str="https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4"
    Returns:
      tf.keras.Model

    Code source: create_image_encoder(num_projection_layers:int=2, projection_dims:int=16, dropout_rate:float=0.2, flag_trainable:bool=False, img_cols:list=[], img_module_url:str="https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4"):
    """
    # create image inputs.
    img_inputs = tf.keras.Input(shape=IMAGE_SIZE+(3,), name=img_col)
    #image_2 = keras.Input(shape=(128, 128, 3), name="image_2")

    # Load the pre-trained ResNet50V2 model to be used as the base encoder.
    hub_layer = hub.KerasLayer(img_module_url, trainable=flag_trainable)
    #embedding_layer_ls = [hub_layer(input) for input in img_inputs]
    #embeddings = tf.keras.layers.Concatenate()(embedding_layer_ls)
    embeddings = hub_layer(img_inputs)

    # Project the embeddings produced by the model.
    outputs = project_embeddings(
        embeddings, num_projection_layers, projection_dim, dropout_rate
    )
    # Create the vision encoder model.
    return tf.keras.Model(img_inputs, outputs, name=img_col)



# Cell

#title  2-task model with L1 feature selection
class DeepCrossEmbeddingModel(tf.keras.Model):
  """compute x (user or item) embeddings."""

  def __init__(self,
               df:pd.DataFrame,
               label:str,
               user_id:str,
               item_id:str,
               x_cnt_cols:list,
               x_cat_cols:list,
               x_cnt_cols_bins:dict,
               x_txt_cols:list,
               x_img_cols:list,
               interactions,
               embedding_dim:int,
               flag_use_norm:bool,
               x_layer_sizes:list,
               projection_dim:int,
               l1_l2:int=1,
               alpha:float=1e04,
               dropout_rate:float=.5,
               num_projection_layers:int=2,
               flag_trainable:bool=False,
               img_module_url:str="https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4",
               flag_dcn_cross:bool=True, #<=======new
               ):
      """initiate attributes (cat_dic, cnt_dic, txt_dic, norm_dic) and methods (dense_layers) to be called in call()

      Args:
        df (pd.DataFrame): e.g. df, interactions_df
        user_id (str): e.g. user_id
        item_id (str): e.g. item_id
        x_cnt_cols (list of str): x's continuous column names
        x_cat_cols (list of str): x's categorical column names.
        x_cnt_cols_bins (dict): x's continuous column names and their corresponding bin size for bucketization.
        x_txt_cols (list of str): x's text column names e.g. 'title', 'description'
        x_img_cols (list of str): x's image column names
        interactions (TFDS dataset, default None): it speeds up when using prefab interactions instead of making it on spot from strach
        embedding_dim (int default 32): embedding vector dimension
        layer_sizes (list of int):A list of integers where the i-th entry represents the number of units the i-th layer contains.
        flag_use_norm (bool default False): whether you want to include the normalized continuous column in embeddings
        x_layer_sizes (list of int):A list of integers where the i-th entry represents the number of units the i-th layer contains.
        projection_dim (int defaul None): a parameter in DNC (Deep Cross Network) https://www.tensorflow.org/recommenders/api_docs/python/tfrs/layers/dcn/Cross
        l1_l2(int, default 1): 0:not using regularization,  1: use regularizer.l1, 2: use regularizer.l2
        alpha (float default 1e-4): a parameter for tensorflow.keras.regularizers.l1 or l2
        dropout_rate:float=.5, within [0,1]
        flag_dcn_cross:bool=True, whether or not use dcn_cross layer, arg for create_deep_cross_layers()
      """
      # define UserModel attributes
      super().__init__()

      # unpack x_cnt_cols_bins into x_cnt_cols and bins
      x_cnt_cols, bins =  list(x_cnt_cols_bins.keys()), list(x_cnt_cols_bins.values())

      # limit the scope of interactions to input cols
      interactions = interactions.map(lambda x: {**{
                                            user_id : x[user_id],
                                            item_id : x[item_id],
                                            label: float(x[label])},
                                            **{cnt_col: x[cnt_col] for cnt_col in x_cnt_cols},
                                            **{cat_col: x[cat_col] for cat_col in x_cat_cols},
                                            **{txt_col: x[txt_col] for txt_col in x_txt_cols},
                                            **{img_col: x[img_col] for img_col in x_img_cols},
                                            })


      ## add embedding models for cat_cols, cnt_cols, txt_cols, img_cols
      #create a dict for cat_cols with (cat_col: cat_emb_model) pairs; similarly do the same for cnt_cols, txt_cols, img_cols
      self.cat_dic = {cat_col: cat_embedding_model(df, user_id, item_id, cat_col=cat_col, flag_use_hashing=False, interactions=interactions, embedding_dim=embedding_dim) for cat_col in x_cat_cols}
      self.bin_cnt_dic = {cnt_col: cnt_embedding_model(df, user_id, item_id, cnt_col=cnt_col, n_bucket=bin, interactions=interactions, embedding_dim=embedding_dim) for cnt_col, bin in zip(x_cnt_cols, bins)}
      self.txt_dic = {txt_col: txt_embedding_model(df, user_id, item_id, txt_col=txt_col, interactions=interactions, embedding_dim=embedding_dim) for txt_col in x_txt_cols}
      self.nrm_cnt_dic= {cnt_col: cnt_normalize_model(df, user_id, item_id, cnt_col=cnt_col, interactions=interactions) for cnt_col in x_cnt_cols}
      self.img_dic = {img_col: img_embedding_model(num_projection_layers, projection_dim, dropout_rate, flag_trainable=False, img_col=img_col, img_module_url=img_module_url) for img_col in x_img_cols}

      # construct the deep cross layers.
      self.deep_cross_layers = create_deep_cross_layers(layer_sizes=x_layer_sizes,
                                                        projection_dim=projection_dim,
                                                        l1_l2=l1_l2,
                                                        alpha=alpha,
                                                        dropout_rate=dropout_rate,
                                                        name='dc_layer',
                                                        flag_dcn_cross=flag_dcn_cross)

      # choose whether or not to use normalized cnt_col in embedding
      self.flag_use_norm = flag_use_norm

  def call(self, inputs):
      """calculate concat_x_embedding and use deep_cross_layers to enforce output dimension to be embedding_dim"""
      # calculate concat_x_embedding
      if self.flag_use_norm:# use normalized cnt_col in embedding
          encoded_inputs_ls = [self.cat_dic[col](inputs[col]) for col in self.cat_dic.keys()]+[self.bin_cnt_dic[col](inputs[col]) for col in self.bin_cnt_dic.keys()]+[self.txt_dic[col](inputs[col]) for col in self.txt_dic.keys()]+[tf.reshape(self.nrm_cnt_dic[col](inputs[col]), (-1, 1)) for col in self.nrm_cnt_dic.keys()]
      else:
          encoded_inputs_ls = [self.cat_dic[col](inputs[col]) for col in self.cat_dic.keys()]+[self.bin_cnt_dic[col](inputs[col]) for col in self.bin_cnt_dic.keys()]+[self.txt_dic[col](inputs[col]) for col in self.txt_dic.keys()]
      concat_x_embedding = tf.concat(encoded_inputs_ls, axis=1)

      return self.deep_cross_layers(concat_x_embedding)


class DeepCross2taskModel(tfrs.models.Model):
    """compute the embedding of user-item-interaction and then do rating_task and/or retrievel_task"""
    def __init__(self,config:dict, df:pd.DataFrame, interactions, items):
        """Model for encoding user item interaction.

        Args: config:dict has the following keys:
            label (str): e.g. 'response_status'
            #df (pd.DataFrame): e.g. df, interactions_df
            user_id (str): e.g. user_id
            item_id (str): e.g. item_id
            u_cat_cols (list of str): user's categorical column names
            u_cnt_cols_bins (dict): user's continuous column names and their corresponding bin size for bucketization.
            u_txt_cols (str): user's text column names e.g. 'title', 'description'
            i_cat_cols (list of str): user's categorical column names
            i_cnt_cols_bins (dict): item's continuous column names and their corresponding bin size for bucketization.
            i_txt_cols (list of str): user's text column names
            #interactions (TFDS dataset, default None): it speeds up when using prefab interactions instead of making it on spot from strach
            embedding_dim (int default 32): embedding vector dimension
            layer_sizes (list of int):A list of integers where the i-th entry represents the number of units the i-th layer contains.
            flag_use_norm (bool default False): whether you want to include the normalized continuous column in embeddings
            projection_dim (int defaul None): a parameter in DNC (Deep Cross Network) https://www.tensorflow.org/recommenders/api_docs/python/tfrs/layers/dcn/Cross.  In practice, we've observed that projection_dim = d/4 consistently preserved the accuracy of a full-rank version.
            rating_weight (float default 0): the weight of rating task in multitask loss function
            retrieval_weight (float default 1): the weight of retrieval task in multitask loss function
            user_layer_sizes (list of int, default=[32]): Used in user tower; A list of integers where the i-th entry represents the number of units the i-th layer contains.
            item_layer_sizes (list of int, default=[32]): Used in item tower; A list of integers where the i-th entry represents the number of units the i-th layer contains.
            loss_name (str): tf loss function, choose from ['mse', 'pairwise', 'listwise'] corresponding to optimization methods [tf.keras.losses.MeanSquaredError(),tfr.keras.losses.PairwiseHingeLoss(), tfr.keras.losses.ListMLELoss()]
            l1_l2(int, default 0): 0:not using regularization,  1: use regularizer.l1, 2: use regularizer.l2
            alpha (float default 1e-4): a parameter for tensorflow.keras.regularizers.l1 or l2
            dropout_rate (float default .5): dropout rate in tensorflow.keras.Dropout(dropout)
            lr:float=0.1 learning rate for model compile

            ### in addition to the hyperparameters in config, we need raw input data &/ parameters, includes
            df:pd.DataFrame, e.g. df, interactions_df
            interactions:TFDS dataset=None, it speeds up when using prefab interactions instead of making it on spot from strach
            items:TFDS dataset
        """
        super().__init__()
        self.config = config

        #unpack global vars & global data
        (label, user_id, item_id, x_cols, u_cols, i_cols, cnt_cols, u_cnt_cols, i_cnt_cols,cnt_cols_to_bin, u_cnt_cols_to_bin, i_cnt_cols_to_bin,bin_cnt_cols, u_bin_cnt_cols, i_bin_cnt_cols, cat_cols, u_cat_cols, i_cat_cols, txt_cols, u_txt_cols, i_txt_cols, img_cols, u_img_cols, i_img_cols, dt_cols, u_dt_cols, i_dt_cols, label_str2num,label_num2str, num_classes, label_keys) = self.config['global_vars']
        #(df, interactions, users, items, unique_user_ids, unique_item_ids, train_df, valid_df, test_df, cached_train, cached_valid, cached_test, test_items) = global_data



        # the following info is used for model serving
        #global_vars=config['global_vars']
        label=config['label']
        #df=config['df']
        user_id=config['user_id']
        item_id=config['item_id']
        u_cnt_cols = config['u_cnt_cols']
        u_cat_cols = config['u_cat_cols']
        u_cnt_cols_bins = config['u_cnt_cols_bins']
        u_txt_cols = config['u_txt_cols']
        u_img_cols = config['u_img_cols']
        i_cnt_cols = config['i_cnt_cols']
        i_cat_cols = config['i_cat_cols']
        i_cnt_cols_bins = config['i_cnt_cols_bins']
        i_txt_cols = config['i_txt_cols']
        i_img_cols = config['i_img_cols']
        #interactions=config['interactions']
        embedding_dim = config['embedding_dim']
        layer_sizes=config['layer_sizes']
        flag_use_norm=config['flag_use_norm']
        projection_dim=config['projection_dim']
        rating_weight = config['rating_weight']
        retrieval_weight = config['retrieval_weight']
        #classification_weight = config['classification_weight']
        user_layer_sizes=config['user_layer_sizes']
        item_layer_sizes=config['item_layer_sizes']
        rating_loss_name=config['rating_loss_name']
        l1_l2=config['l1_l2']
        alpha=config['alpha']
        dropout_rate=config['dropout_rate']
        #items=config['items']
        flag_vsn_deep_cross=config['flag_vsn_deep_cross'],
        num_projection_layers=config['num_projection_layers']
        flag_trainable=config['flag_trainable']
        img_module_url=config['img_module_url']
        flag_dcn_cross=config['flag_dcn_cross']
        #last_layer_activation= config['last_layer_activation']

        self.user_embeddings = None #use as a placeholder for user embeddings
        self.item_embeddings = None #use as a placeholder for item embeddings
        self.cross_ui_embeddings = None #use as a placeholder for user-item interaction embeddings
        u_cnt_cols_bins=self.config['u_cnt_cols_bins']
        i_cnt_cols_bins=self.config['i_cnt_cols_bins']
        # the following 2 variable is used for model understanding
        self._all_features = list(set(u_cat_cols) | set(i_cat_cols) |set(u_cnt_cols_bins.keys())| set(i_cnt_cols_bins.keys()) | set(u_txt_cols) | set(i_txt_cols))
        #self._cross_layer = Model(tfrs.layers.dcn.Cross(projection_dim=projection_dim, kernel_initializer="glorot_uniform"))
        self._cross_layer = tfrs.layers.dcn.Cross(projection_dim=projection_dim, kernel_initializer="glorot_uniform")

        # define user_model & item_model
        self.user_model = DeepCrossEmbeddingModel(df=df,
                                                  label=label,
                                                  user_id=user_id,
                                                  item_id=item_id,
                                                  x_cnt_cols=u_cnt_cols,
                                                  x_cat_cols=u_cat_cols,
                                                  x_cnt_cols_bins=u_cnt_cols_bins,
                                                  x_txt_cols=u_txt_cols,
                                                  x_img_cols=u_img_cols,
                                                  interactions=interactions,
                                                  embedding_dim=embedding_dim,
                                                  flag_use_norm=flag_use_norm,
                                                  x_layer_sizes=user_layer_sizes,
                                                  projection_dim=projection_dim,
                                                  l1_l2=l1_l2,
                                                  alpha=alpha,
                                                  dropout_rate=dropout_rate,
                                                  num_projection_layers=num_projection_layers,
                                                  flag_trainable=flag_trainable,
                                                  img_module_url=img_module_url, #"https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4",
                                                  flag_dcn_cross=flag_dcn_cross,
                                                  )
        self.item_model = DeepCrossEmbeddingModel(df=df,
                                                  label=label,
                                                  user_id=user_id,
                                                  item_id=item_id,
                                                  x_cnt_cols=i_cnt_cols,
                                                  x_cat_cols=i_cat_cols,
                                                  x_cnt_cols_bins=i_cnt_cols_bins,
                                                  x_txt_cols=i_txt_cols,
                                                  x_img_cols=i_img_cols,
                                                  interactions=interactions,
                                                  embedding_dim=embedding_dim,
                                                  flag_use_norm=flag_use_norm,
                                                  x_layer_sizes=item_layer_sizes,
                                                  projection_dim=projection_dim,
                                                  l1_l2=l1_l2,
                                                  alpha=alpha,
                                                  dropout_rate=dropout_rate,
                                                  num_projection_layers=num_projection_layers,
                                                  flag_trainable=flag_trainable,
                                                  img_module_url=img_module_url, #"https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4",
                                                  flag_dcn_cross=flag_dcn_cross,
                                                  )
        ## rating model (deep network is implemented here at rating/ranking model)
        # enforce the last layer size to be 1, b/c it refers to the predicted rating, a number
        if layer_sizes[-1]!=1:
            layer_sizes=layer_sizes+[1]
        self.rating_model = create_deep_cross_layers(layer_sizes, projection_dim, l1_l2, alpha, dropout_rate, name='rating_model', flag_dcn_cross=flag_dcn_cross)
        """self.rating_model = tf.keras.Sequential([
                                                tf.keras.layers.Dense(256, activation="relu"),
                                                tf.keras.layers.Dense(128, activation="relu"),
                                                tf.keras.layers.Dense(1),
                                                ])"""



        ## rating task

        # define the dictionary for loss function
        rating_loss_dict = {'mse':tf.keras.losses.MeanSquaredError(),
                     'pairwise':tfr.keras.losses.PairwiseHingeLoss(),
                     'listwise':tfr.keras.losses.ListMLELoss()}

        self.rating_task: tf.keras.layers.Layer = tfrs.tasks.Ranking(
            loss=rating_loss_dict[rating_loss_name], #tf.keras.losses.MeanSquaredError(),
            metrics=[
                tfr.keras.metrics.NDCGMetric(name="ndcg_metric"),
                tf.keras.metrics.RootMeanSquaredError("RMSE"),
                tfr.keras.metrics.PrecisionMetric('precision'),
                tfr.keras.metrics.RecallMetric('recall'),
                ]
        )

        # retrieval task.
        self.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(
            metrics=tfrs.metrics.FactorizedTopK(
                candidates=items.batch(128).map(self.item_model)
            )
        )

        ## define input and outputs for SHAP interpretation
        #self.input = df
        self.outputs = None # will be updated as the output of call()


    def call(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:
        user_dic = {self.config['user_id']: features[self.config['user_id']],
                    self.config['item_id']: features[self.config['item_id']],
                    **{cat_col: features[cat_col] for cat_col in self.config['u_cat_cols']},
                    **{cnt_col: features[cnt_col] for cnt_col in self.config['u_cnt_cols_bins'].keys()},
                    **{txt_col: features[txt_col] for txt_col in self.config['u_txt_cols']},
                    **{img_col: features[img_col] for img_col in self.config['u_img_cols']},
                    }

        item_dic = {self.config['user_id']: features[self.config['user_id']],
                    self.config['item_id']: features[self.config['item_id']],
                    **{cat_col: features[cat_col] for cat_col in self.config['i_cat_cols']},
                    **{cnt_col: features[cnt_col] for cnt_col in self.config['i_cnt_cols_bins'].keys()},
                    **{txt_col: features[txt_col] for txt_col in self.config['i_txt_cols']},
                    **{img_col: features[img_col] for img_col in self.config['i_img_cols']},
                    }
        user_embeddings = self.user_model(user_dic)
        item_embeddings = self.item_model(item_dic)

        concat_ui_emb = tf.concat([user_embeddings, item_embeddings], axis=1)

        # Build Cross Network
        cross_ui_emb = self._cross_layer(concat_ui_emb)
        rating_predictions = self.rating_model(cross_ui_emb)
        self.outputs = rating_predictions #will be used for SHAP interpretation

        # update various embeddings
        self.user_embeddings = user_embeddings
        self.item_embeddings = item_embeddings
        self.cross_ui_embeddings = cross_ui_emb

        return (user_embeddings, item_embeddings, rating_predictions)

    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:

        ## ratings go here as a method to compute loss
        ratings = features.pop(self.config['label'])

        user_embeddings, item_embeddings, rating_predictions = self(features)

        # compute the loss for each task.
        rating_loss = self.rating_task(
            labels=tf.reshape(ratings,[-1,1]), #ratings.shape == (None,), but expect rank 2 ==> Solu: reshape it!!
            predictions=tf.reshape(rating_predictions,[-1,1]) # tf.squeeze(rating_predictions, axis=-1).shape == (None,), but expect rank 2 ==> solu: reshape it
        )
        retrieval_loss = self.retrieval_task(user_embeddings, item_embeddings)

        # create weighted_loss by combining them using the loss weights.
        weighted_loss= (self.config['rating_weight'] * rating_loss
                + self.config['retrieval_weight'] * retrieval_loss)
        return weighted_loss
    # To easily update compuation latter as needed, implement get_config() and from_config()
    # ref: https://www.tensorflow.org/guide/keras/save_and_serialize
    def get_config(self):
            return self.config
    @classmethod
    def from_config(cls, config):
        return cls(**config)


# Cell

#title 2-task model w. L1/VSN features selection

class GatedLinearUnit(tf.keras.layers.Layer):
    def __init__(self, units):
        super(GatedLinearUnit, self).__init__()
        self.linear = tf.keras.layers.Dense(units)
        self.softmax = tf.keras.layers.Dense(units, activation="softmax")
        self.units = units

    def call(self, inputs):
        return self.linear(inputs) * self.softmax(inputs)
    def get_config(self):
        config = super().get_config()
        config.update({
            "units": self.units,
        })
        return config

class GatedResidualNetwork(tf.keras.layers.Layer):
    def __init__(self, units, dropout_rate):
        super(GatedResidualNetwork, self).__init__()
        self.units = units
        self.dropout_rate = dropout_rate
        self.elu_dense = tf.keras.layers.Dense(units, activation="elu")
        self.linear_dense = tf.keras.layers.Dense(units)
        self.dropout = tf.keras.layers.Dropout(dropout_rate)
        self.gated_linear_unit = GatedLinearUnit(units)
        self.layer_norm = tf.keras.layers.LayerNormalization()
        self.project = tf.keras.layers.Dense(units)

    def call(self, inputs):
        x = self.elu_dense(inputs)
        x = self.linear_dense(x)
        x = self.dropout(x)
        if inputs.shape[-1] != self.units:
            inputs = self.project(inputs)
        x = inputs + self.gated_linear_unit(x)
        x = self.layer_norm(x)
        return x
    def get_config(self):
        config = super().get_config()
        config.update({
            "units": self.units,
            "dropout_rate": self.dropout_rate,
        })
        return config

class VariableSelection(tf.keras.layers.Layer):
    def __init__(self, num_features, units, dropout_rate):
        super(VariableSelection, self).__init__()
        self.grns = list()
        # Create a GRN for each feature independently
        for idx in range(num_features):
            grn = GatedResidualNetwork(units, dropout_rate)
            self.grns.append(grn)
        # Create a GRN for the concatenation of all the features
        self.grn_concat = GatedResidualNetwork(units, dropout_rate)
        self.softmax = tf.keras.layers.Dense(units=num_features, activation="softmax")
        self.units = units
        self.dropout_rate = dropout_rate
        self.num_features = num_features

    def call(self, inputs):
        v = tf.keras.layers.concatenate(inputs)
        v = self.grn_concat(v)
        v = tf.expand_dims(self.softmax(v), axis=-1)

        x = []
        for idx, input in enumerate(inputs):
            x.append(self.grns[idx](input))
        x = tf.stack(x, axis=1)

        outputs = tf.squeeze(tf.matmul(v, x, transpose_a=True), axis=1)
        return outputs

    def get_config(self):
        config = super().get_config()
        config.update({
            "units": self.units,
            "dropout_rate": self.dropout_rate,
            "num_features": self.num_features,
        })
        return config
def create_vsn_feature_selection_layers(num_features:int, num_classes:int, embedding_dim:int, dropout_rate:float)->tf.keras.Model:
    """create feature selection layers by vsn
    input for vsn_layers: encoded_inputs
    output of vsn_layers: weighted_encoded_inputs
    what is inside? vsn learn weight for each input feature
    Args:
      num_features:int, num_classes:int, embedding_dim:int, dropout_rate:float
    Returns:
      vsn_layers:tf.keras.Sequential
    Example:
      num_features =
      num_classes =
      embedding_dim = 32
      dropout_rate = 0.5
      vsn_layer = create_feature_selection_vsn_layers(num_features, num_classes, embedding_dim, dropout_rate)

      if model_type == 'vsn':
        num_features = len(encoded_inputs)
        # select encoded inputs by learning weight for each feature
        weighted_features = VariableSelection(num_features, embedding_dim, dropout_rate)(
            encoded_inputs # encoded_inputs is a list, not dict
        )

        # map weighted_features to outputs by 1 dense layer
        outputs = tf.keras.layers.Dense(units=num_classes, activation="softmax")(weighted_features)
        model = tf.keras.Model(inputs=inputs, outputs=outputs)
    """
    # init the vsn layers.
    vsn_layers = tf.keras.Sequential()

    # select encoded inputs by learning weight for each feature
    vsn_layers.add(VariableSelection(num_features, embedding_dim, dropout_rate))

    # map weighted_features to outputs by 1 dense layer
    #vsn_layers.add(tf.keras.layers.Dense(units=num_classes, activation="softmax"))
    return vsn_layers

class VSN_DeepCrossEmbeddingModel(tf.keras.Model):
  """compute x (user or item) embeddings.
  Example:
    #features:Dict[Text, tf.Tensor]
    user_dic = {user_id: features[user_id],
          item_id: features[item_id],
          **{cat_col: features[cat_col] for cat_col in u_cat_cols},
          **{cnt_col: features[cnt_col] for cnt_col in u_cnt_cols_bins.keys()},
          **{txt_col: features[txt_col] for txt_col in u_txt_cols},
          }
    user_model =  VSN_DeepCrossEmbeddingModel(df=df,
                                              label=label,
                                              user_id=user_id,
                                              item_id=item_id,
                                              x_cnt_cols=x_cnt_cols,
                                              x_cat_cols=x_cat_cols,
                                              x_cnt_cols_bins=x_cnt_cols_bins,
                                              x_txt_cols=x_txt_cols,
                                              x_img_cols=x_img_cols,
                                              interactions=interactions,
                                              embedding_dim=embedding_dim,
                                              flag_use_norm=flag_use_norm,
                                              x_layer_sizes=x_layer_sizes,
                                              projection_dim=projection_dim,
                                              l1_l2=l1_l2,
                                              alpha=alpha,
                                              dropout_rate=dropout_rate,
                                              flag_vsn_deep_cross=flag_vsn_deep_cross,
                                              num_projection_layers=num_projection_layers,
                                              flag_trainable=flag_trainable,
                                              img_module_url=img_module_url,
                                              )
    user_embeddings = user_model(user_dic)

  """

  def __init__(self,
               df:pd.DataFrame,
               label:str,
               user_id:str,
               item_id:str,
               x_cnt_cols:list,
               x_cat_cols:list,
               x_cnt_cols_bins:dict,
               x_txt_cols:list,
               x_img_cols:list,
               interactions,
               embedding_dim:int,
               flag_use_norm:bool,
               x_layer_sizes:list,
               projection_dim:int,
               l1_l2:int=0,
               alpha:float=1e04,
               dropout_rate:float=.5,
               flag_vsn_deep_cross:bool=True,
               num_projection_layers=2,
               flag_trainable:bool=False,
               img_module_url:str="https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4",
               flag_dcn_cross=True,
               ):
      """initiate attributes (cat_dic, cnt_dic, txt_dic, norm_dic) and methods (dense_layers) to be called in call()

      Args:
        df (pd.DataFrame): e.g. df, interactions_df
        user_id (str): e.g. user_id
        item_id (str): e.g. item_id
        x_cnt_cols (list of str): x's continuous column names
        x_cat_cols (list of str): x's categorical column names.
        x_cnt_cols_bins (dict): x's continuous column names and their corresponding bin size for bucketization.
        x_txt_cols (str): x's text column names e.g. 'title', 'description'
        interactions (TFDS dataset, default None): it speeds up when using prefab interactions instead of making it on spot from strach
        embedding_dim (int default 32): embedding vector dimension
        x_layer_sizes (list of int):A list of integers where the i-th entry represents the number of units the i-th layer contains.
        flag_use_norm (bool default False): whether you want to include the normalized continuous column in embeddings
        x_layer_sizes (list of int):A list of integers where the i-th entry represents the number of units the i-th layer contains.
        projection_dim (int defaul None): a parameter in DNC (Deep Cross Network) https://www.tensorflow.org/recommenders/api_docs/python/tfrs/layers/dcn/Cross
        l1_l2(int, default 0): 0:not using regularization,  1: use regularizer.l1, 2: use regularizer.l2
        alpha (float default 1e-4): a parameter for tensorflow.keras.regularizers.l1 or l2
        dropout_rate:float=.5,
        flag_vsn_deep_cross:bool=True whether or not use vsn features selection before deep_cross processing
      """
      # define UserModel attributes
      super().__init__()

      # unpack x_cnt_cols_bins into x_cnt_cols and bins
      x_cnt_cols, bins =  list(x_cnt_cols_bins.keys()), list(x_cnt_cols_bins.values())

      # limit the scope of interactions to input cols
      interactions = interactions.map(lambda x: {**{
                                            user_id : x[user_id],
                                            item_id : x[item_id],
                                            label: float(x[label])},
                                            **{cnt_col: x[cnt_col] for cnt_col in x_cnt_cols},
                                            **{cat_col: x[cat_col] for cat_col in x_cat_cols},
                                            **{txt_col: x[txt_col] for txt_col in x_txt_cols},
                                            **{img_col: x[img_col] for img_col in x_img_cols},
                                            })


      ## add embedding models for cat_cols, cnt_cols, txt_cols
      #create a dict for cat_cols with (cat_col: cat_emb_model) pairs; similarly do the same for cnt_cols, txt_cols
      self.cat_dic = {cat_col: cat_embedding_model(df, user_id, item_id, cat_col=cat_col, flag_use_hashing=False, interactions=interactions, embedding_dim=embedding_dim) for cat_col in x_cat_cols}
      self.nrm_cnt_dic= {cnt_col: cnt_normalize_model(df, user_id, item_id, cnt_col=cnt_col, interactions=interactions) for cnt_col in x_cnt_cols}
      self.bin_cnt_dic = {cnt_col: cnt_embedding_model(df, user_id, item_id, cnt_col=cnt_col, n_bucket=bin, interactions=interactions, embedding_dim=embedding_dim) for cnt_col, bin in zip(x_cnt_cols, bins)}
      self.txt_dic = {txt_col: txt_embedding_model(df, user_id, item_id, txt_col=txt_col, interactions=interactions, embedding_dim=embedding_dim) for txt_col in x_txt_cols}
      self.img_dic = {img_col: img_embedding_model(num_projection_layers, projection_dim, dropout_rate, flag_trainable=False, img_col=img_col, img_module_url=img_module_url) for img_col in x_img_cols}

      # choose whether or not to use normalized cnt_cols (stored in nrm_cnt_dic) in embedding
      self.flag_use_norm = flag_use_norm
      if self.flag_use_norm:
        num_features = len(self.cat_dic) + len(self.nrm_cnt_dic) + len(self.bin_cnt_dic) + len(self.txt_dic)
      else:
        num_features = len(self.cat_dic) + len(self.bin_cnt_dic) + len(self.txt_dic)

      # construct the vsn layers for features selection
      self.vsn = VariableSelection(num_features, embedding_dim, dropout_rate=dropout_rate)

      # construct the deep cross layers for features interaction
      self.deep_cross_layers = create_deep_cross_layers(x_layer_sizes, projection_dim, l1_l2, alpha, dropout_rate, name='deep_cross_layers', flag_dcn_cross=flag_dcn_cross)
      self.flag_vsn_deep_cross = flag_vsn_deep_cross

  def call(self, inputs:Dict[Text, tf.Tensor]) -> tf.Tensor:
      """calculate concat_x_embedding and use deep_cross_layers to enforce output dimension to be embedding_dim
      Args:
        inputs: a dict of TEXT:tf.Tensor pairs
      Returns:
        embeddings
      """
      # calculate concat_x_embedding
      if self.flag_use_norm:# use normalized cnt_col in embedding
          encoded_inputs_ls = [self.cat_dic[col](inputs[col]) for col in self.cat_dic.keys()]+[self.bin_cnt_dic[col](inputs[col]) for col in self.bin_cnt_dic.keys()]+[self.txt_dic[col](inputs[col]) for col in self.txt_dic.keys()]+[tf.reshape(self.nrm_cnt_dic[col](inputs[col]), (-1, 1)) for col in self.nrm_cnt_dic.keys()]
      else:
          encoded_inputs_ls = [self.cat_dic[col](inputs[col]) for col in self.cat_dic.keys()]+[self.bin_cnt_dic[col](inputs[col]) for col in self.bin_cnt_dic.keys()]+[self.txt_dic[col](inputs[col]) for col in self.txt_dic.keys()]


      # choose 1 of the 2 ways for feature selection: vsn or L1/L2
      if self.flag_vsn_deep_cross:
        selected_inputs = self.vsn(encoded_inputs_ls) #replace: self.vsn(concat_x_embedding) because self.vsn() takes in encoded_inputs
        embeddings = self.deep_cross_layers(selected_inputs) #
      else:
        concat_encoded_inputs = tf.concat(encoded_inputs_ls, axis=1)
        embeddings = self.deep_cross_layers(concat_encoded_inputs)
      return embeddings


class VSN_DeepCross2taskModel(tfrs.models.Model):
    """compute the embedding of user-item-interaction and then rate the interaction"""
    def __init__(self,
                 config:dict,
                 df:pd.DataFrame,
                 interactions,
                 items,
                 ):
        """Model for encoding user item interaction.

        Args: config:dict contains hyperparameters only, no raw input data (e.g. df, interaction, items); config has the following keys:
            label (str): e.g. 'response_status'
            #df (pd.DataFrame): e.g. df, interactions_df
            user_id (str): e.g. user_id
            item_id (str): e.g. item_id
            u_cat_cols (list of str): user's categorical column names
            u_cnt_cols_bins (dict): user's continuous column names and their corresponding bin size for bucketization.
            u_txt_cols (str): user's text column names e.g. 'title', 'description'
            i_cat_cols (list of str): user's categorical column names
            i_cnt_cols_bins (dict): item's continuous column names and their corresponding bin size for bucketization.
            i_txt_cols (list of str): user's text column names
            #interactions (TFDS dataset, default None): it speeds up when using prefab interactions instead of making it on spot from strach
            embedding_dim (int default 32): embedding vector dimension
            layer_sizes (list of int):A list of integers where the i-th entry represents the number of units the i-th layer contains.
            flag_use_norm (bool default False): whether you want to include the normalized continuous column in embeddings
            projection_dim (int defaul None): a parameter in DNC (Deep Cross Network) https://www.tensorflow.org/recommenders/api_docs/python/tfrs/layers/dcn/Cross.  In practice, we've observed that projection_dim = d/4 consistently preserved the accuracy of a full-rank version.
            rating_weight (float default 0): the weight of rating task in multitask loss function
            retrieval_weight (float default 1): the weight of retrieval task in multitask loss function
            user_layer_sizes (list of int, default=[32]): Used in user tower; A list of integers where the i-th entry represents the number of units the i-th layer contains.
            item_layer_sizes (list of int, default=[32]): Used in item tower; A list of integers where the i-th entry represents the number of units the i-th layer contains.
            loss_name (str): tf loss function, choose from ['mse', 'pairwise', 'listwise'] corresponding to optimization methods [tf.keras.losses.MeanSquaredError(),tfr.keras.losses.PairwiseHingeLoss(), tfr.keras.losses.ListMLELoss()]
            l1_l2(int, default 0): 0:not using regularization,  1: use regularizer.l1, 2: use regularizer.l2
            alpha (float default 1e-4): a parameter for tensorflow.keras.regularizers.l1 or l2
            dropout (float default .5): dropout rate in tensorflow.keras.Dropout(dropout)
            lr:float=0.1 learning rate for model compile
            flag_vsn_deep_cross=True whether or not use vsn features selection before deep_cross processing
        """
        super().__init__()
        self.config = config

        #unpack global vars & global data
        (label, user_id, item_id, x_cols, u_cols, i_cols, cnt_cols, u_cnt_cols, i_cnt_cols,cnt_cols_to_bin, u_cnt_cols_to_bin, i_cnt_cols_to_bin,bin_cnt_cols, u_bin_cnt_cols, i_bin_cnt_cols, cat_cols, u_cat_cols, i_cat_cols, txt_cols, u_txt_cols, i_txt_cols, img_cols, u_img_cols, i_img_cols, dt_cols, u_dt_cols, i_dt_cols, label_str2num,label_num2str, num_classes, label_keys) = self.config['global_vars']


        # the following info is used for model serving
        label=config['label']
        #df=config['df']
        user_id=config['user_id']
        item_id=config['item_id']
        u_cnt_cols = config['u_cnt_cols']
        u_cat_cols = config['u_cat_cols']
        u_cnt_cols_bins = config['u_cnt_cols_bins']
        u_txt_cols = config['u_txt_cols']
        u_img_cols = config['u_img_cols']
        i_cnt_cols = config['i_cnt_cols']
        i_cat_cols = config['i_cat_cols']
        i_cnt_cols_bins = config['i_cnt_cols_bins']
        i_txt_cols = config['i_txt_cols']
        i_img_cols = config['i_img_cols']
        #interactions=config['interactions']
        embedding_dim = config['embedding_dim']
        layer_sizes=config['layer_sizes']
        flag_use_norm=config['flag_use_norm']
        projection_dim=config['projection_dim']
        rating_weight = config['rating_weight']
        retrieval_weight = config['retrieval_weight']
        user_layer_sizes=config['user_layer_sizes']
        item_layer_sizes=config['item_layer_sizes']
        rating_loss_name=config['rating_loss_name']
        l1_l2=config['l1_l2']
        alpha=config['alpha']
        dropout_rate=config['dropout_rate']
        flag_vsn_deep_cross=config['flag_vsn_deep_cross']
        num_projection_layers=config['num_projection_layers']
        flag_trainable=config['flag_trainable']
        img_module_url=config['img_module_url']
        flag_dcn_cross=config['flag_dcn_cross']

        #items=config['items']
        self.user_embeddings = None #use as a placeholder for user embeddings
        self.item_embeddings = None #use as a placeholder for item embeddings
        self.cross_ui_embeddings = None #use as a placeholder for user-item interaction embeddings

        # the following 2 variable is used for model understanding
        self._all_features = list(set(u_cat_cols) | set(i_cat_cols) |set(u_cnt_cols_bins.keys())| set(i_cnt_cols_bins.keys()) | set(u_txt_cols) | set(i_txt_cols)| set(u_img_cols) | set(i_img_cols))
        #self._cross_layer = Model(tfrs.layers.dcn.Cross(projection_dim=projection_dim, kernel_initializer="glorot_uniform"))
        self._cross_layer = tfrs.layers.dcn.Cross(projection_dim=projection_dim, kernel_initializer="glorot_uniform")

        # define user_model & item_model
        self.user_model = VSN_DeepCrossEmbeddingModel(df=df,
                                                      label=label,
                                                      user_id=user_id,
                                                      item_id=item_id,
                                                      x_cnt_cols=u_cnt_cols,
                                                      x_cat_cols=u_cat_cols,
                                                      x_cnt_cols_bins=u_cnt_cols_bins,
                                                      x_txt_cols=u_txt_cols,
                                                      x_img_cols=u_img_cols,
                                                      interactions=interactions,
                                                      embedding_dim=embedding_dim,
                                                      flag_use_norm=flag_use_norm,
                                                      x_layer_sizes=user_layer_sizes,
                                                      projection_dim=projection_dim,
                                                      l1_l2=l1_l2,
                                                      alpha=alpha,
                                                      dropout_rate=dropout_rate,
                                                      flag_vsn_deep_cross=flag_vsn_deep_cross,
                                                      num_projection_layers=num_projection_layers,
                                                      flag_trainable=flag_trainable,
                                                      img_module_url=img_module_url,
                                                      flag_dcn_cross=flag_dcn_cross,
                                                      )
        self.item_model = VSN_DeepCrossEmbeddingModel(df=df,
                                                      label=label,
                                                      user_id=user_id,
                                                      item_id=item_id,
                                                      x_cnt_cols=i_cnt_cols,
                                                      x_cat_cols=i_cat_cols,
                                                      x_cnt_cols_bins=i_cnt_cols_bins,
                                                      x_txt_cols=i_txt_cols,
                                                      x_img_cols=i_img_cols,
                                                      interactions=interactions,
                                                      embedding_dim=embedding_dim,
                                                      flag_use_norm=flag_use_norm,
                                                      x_layer_sizes=item_layer_sizes,
                                                      projection_dim=projection_dim,
                                                      l1_l2=l1_l2,
                                                      alpha=alpha,
                                                      dropout_rate=dropout_rate,
                                                      flag_vsn_deep_cross=flag_vsn_deep_cross,
                                                      num_projection_layers=num_projection_layers,
                                                      flag_trainable=flag_trainable,
                                                      img_module_url=img_module_url,
                                                      flag_dcn_cross=flag_dcn_cross,
                                                      )
        ## rating model (deep network is implemented here at rating/ranking model)
        # enforce the last layer size to be 1, b/c it refers to the predicted rating, a number
        if layer_sizes[-1]!=1:
            layer_sizes=layer_sizes+[1]
        self.rating_model = create_deep_cross_layers(layer_sizes, projection_dim, l1_l2, alpha, dropout_rate, name='rating_model', flag_dcn_cross=flag_dcn_cross)
        ## rating task

        # define the dictionary for loss function
        rating_loss_dict = {'mse':tf.keras.losses.MeanSquaredError(),
                     'pairwise':tfr.keras.losses.PairwiseHingeLoss(),
                     'listwise':tfr.keras.losses.ListMLELoss()}

        self.rating_task: tf.keras.layers.Layer = tfrs.tasks.Ranking(
            loss=rating_loss_dict[rating_loss_name], #tf.keras.losses.MeanSquaredError(),
            metrics=[
                tfr.keras.metrics.NDCGMetric(name="ndcg_metric"),
                tf.keras.metrics.RootMeanSquaredError("RMSE"),
                tfr.keras.metrics.PrecisionMetric('precision'),
                tfr.keras.metrics.RecallMetric('recall'),
                ],
        )

        # retrieval task.
        self.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(
            metrics=tfrs.metrics.FactorizedTopK(
                candidates=items.batch(128).map(self.item_model)
            )
        )

        ## define input and outputs for SHAP interpretation
        #self.input = df
        self.outputs = None # will be updated as the output of call()


    def call(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:
        user_dic = {self.config['user_id']: features[self.config['user_id']],
                    self.config['item_id']: features[self.config['item_id']],
                    **{cat_col: features[cat_col] for cat_col in self.config['u_cat_cols']},
                    **{cnt_col: features[cnt_col] for cnt_col in self.config['u_cnt_cols_bins'].keys()},
                    **{txt_col: features[txt_col] for txt_col in self.config['u_txt_cols']},
                    **{img_col: features[img_col] for img_col in self.config['u_img_cols']},

                    }

        item_dic = {self.config['user_id']: features[self.config['user_id']],
                    self.config['item_id']: features[self.config['item_id']],
                    **{cat_col: features[cat_col] for cat_col in self.config['i_cat_cols']},
                    **{cnt_col: features[cnt_col] for cnt_col in self.config['i_cnt_cols_bins'].keys()},
                    **{txt_col: features[txt_col] for txt_col in self.config['i_txt_cols']},
                    **{img_col: features[img_col] for img_col in self.config['i_img_cols']},

                    }
        user_embeddings = self.user_model(user_dic)
        item_embeddings = self.item_model(item_dic)

        concat_ui_emb = tf.concat([user_embeddings, item_embeddings], axis=1)

        # Build Cross Network
        cross_ui_emb = self._cross_layer(concat_ui_emb)
        rating_predictions = self.rating_model(cross_ui_emb)
        self.outputs = rating_predictions #will be used for SHAP interpretation

        # update various embeddings
        self.user_embeddings = user_embeddings
        self.item_embeddings = item_embeddings
        self.cross_ui_embeddings = cross_ui_emb

        return (user_embeddings, item_embeddings, rating_predictions)

    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:

        ## ratings go here as a method to compute loss
        ratings = features.pop(self.config['label'])

        user_embeddings, item_embeddings, rating_predictions = self(features)

        # compute the loss for each task.
        rating_loss = self.rating_task(
            labels=tf.reshape(ratings,[-1,1]), #ratings.shape == (None,), but expect rank 2 ==> Solu: reshape it!!
            predictions=tf.reshape(rating_predictions,[-1,1]) # tf.squeeze(rating_predictions, axis=-1).shape == (None,), but expect rank 2 ==> solu: reshape it
        )
        retrieval_loss = self.retrieval_task(user_embeddings, item_embeddings)

        # create weighted_loss by combining them using the loss weights.
        weighted_loss= (self.config['rating_weight'] * rating_loss
                + self.config['retrieval_weight'] * retrieval_loss)
        return weighted_loss
    # To easily update compuation latter as needed, implement get_config() and from_config()
    # ref: https://www.tensorflow.org/guide/keras/save_and_serialize
    def get_config(self):
            return self.config
    @classmethod
    def from_config(cls, config):
        return cls(**config)



# Cell

#@title 3-task w. L1/VSN features selection

"""A classifciation task."""

from typing import List, Optional, Text

import tensorflow as tf


from tensorflow_recommenders.tasks import base


class Classification(tf.keras.layers.Layer):
  """A classification task.
  Recommender systems are often composed of two components retrieval & ranker; but here I add a classifier
  - a retrieval model, retrieving O(10^3) candidates from a corpus of
    O(10^6) candidates.
  - a ranker model, scoring the candidates retrieved by the retrieval model to
    return a ranked shortlist of a few dozen O(10^1) candidates.
  - a classification model, predicting the probability item-user interaction outcome. e.g. in skippable meeting recommedation, {'accepted':0.35, 'need_action':0.1, 'declined':0.55}

  This task helps with building classification models. Usually, these will involve
  predicting the probability of different signals such as actions (accept, decline, ignore), ratings (1-5 points).

  Origin: class Ranking() https://github.com/tensorflow/recommenders/blob/v0.6.0/tensorflow_recommenders/tasks/ranking.py
  """

  def __init__(
      self,
      loss: Optional[tf.keras.losses.Loss] = None,
      metrics: Optional[List[tf.keras.metrics.Metric]] = None,
      prediction_metrics: Optional[List[tf.keras.metrics.Metric]] = None,
      label_metrics: Optional[List[tf.keras.metrics.Metric]] = None,
      loss_metrics: Optional[List[tf.keras.metrics.Metric]] = None,
      name: Optional[Text] = None) -> None:
    """Initializes the task.
    Args:
      loss: Loss function. Defaults to BinaryCrossentropy.
      metrics: List of Keras metrics to be evaluated.
      prediction_metrics: List of Keras metrics used to summarize the
        predictions.
      label_metrics: List of Keras metrics used to summarize the labels.
      loss_metrics: List of Keras metrics used to summarize the loss.
      name: Optional task name.
    """

    super(Classification, self).__init__(name=name)

    self._loss = (
        loss if loss is not None else tf.keras.losses.BinaryCrossentropy())
    self._classification_metrics = metrics or []
    self._prediction_metrics = prediction_metrics or []
    self._label_metrics = label_metrics or []
    self._loss_metrics = loss_metrics or []

  def call(self,
           labels: tf.Tensor,
           predictions: tf.Tensor,
           sample_weight: Optional[tf.Tensor] = None,
           training: bool = False,
           compute_metrics: bool = True) -> tf.Tensor:
    """Computes the task loss and metrics.
    Args:
      labels: Tensor of labels.
      predictions: Tensor of predictions.
      sample_weight: Tensor of sample weights.
      training: Indicator whether training or test loss is being computed.
      compute_metrics: Whether to compute metrics. Set this to False
        during training for faster training.
    Returns:
      loss: Tensor of loss values.
    """

    loss = self._loss(
        y_true=labels, y_pred=predictions, sample_weight=sample_weight)

    if not compute_metrics:
      return loss

    update_ops = []

    for metric in self._classification_metrics:
      update_ops.append(metric.update_state(
          y_true=labels, y_pred=predictions, sample_weight=sample_weight))

    for metric in self._prediction_metrics:
      update_ops.append(
          metric.update_state(predictions, sample_weight=sample_weight))

    for metric in self._label_metrics:
      update_ops.append(
          metric.update_state(labels, sample_weight=sample_weight))

    for metric in self._loss_metrics:
      update_ops.append(
          metric.update_state(loss, sample_weight=sample_weight))

    # Custom metrics may not return update ops, unlike built-in
    # Keras metrics.
    update_ops = [x for x in update_ops if x is not None]

    with tf.control_dependencies(update_ops):
      return tf.identity(loss)

# define the dictionary for clf loss function
clf_loss_dict = {'SparseCategoricalFocalLoss':SparseCategoricalFocalLoss(gamma=2),
              'SparseCategoricalCrossentropy':tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              'CategoricalCrossentropy': tf.keras.losses.CategoricalCrossentropy(from_logits=False),
              'SigmoidFocalCrossEntropy': tfa.losses.SigmoidFocalCrossEntropy(from_logits = False,
                                                          alpha= 0.25,
                                                          gamma= 2.0,
                                                          reduction=tf.keras.losses.Reduction.AUTO,
                                                          name = 'sigmoid_focal_crossentropy'
                                                          ),
              }

class DeepCross3taskModel(tfrs.models.Model):
    """compute the embedding of user-item-interaction and then do rating_task and/or retrievel_task"""
    def __init__(self,
                 config:dict,
                 df:pd.DataFrame,
                 interactions,
                 items,
                 ):
        """init:
        1) raw data => [encoder] => encoded data
        2) example: define clf, compile clf with (loss, optimizer, metrics)

        Args: config:dict has the following keys:
            label (str): e.g. 'response_status'
            #df (pd.DataFrame): e.g. df, interactions_df
            user_id (str): e.g. user_id
            item_id (str): e.g. item_id
            u_cat_cols (list of str): user's categorical column names
            u_cnt_cols_bins (dict): user's continuous column names and their corresponding bin size for bucketization.
            u_txt_cols (str): user's text column names e.g. 'title', 'description'
            i_cat_cols (list of str): user's categorical column names
            i_cnt_cols_bins (dict): item's continuous column names and their corresponding bin size for bucketization.
            i_txt_cols (list of str): user's text column names
            #interactions (TFDS dataset, default None): it speeds up when using prefab interactions instead of making it on spot from strach
            embedding_dim (int default 32): embedding vector dimension
            layer_sizes (list of int):A list of integers where the i-th entry represents the number of units the i-th layer contains.
            flag_use_norm (bool default False): whether you want to include the normalized continuous column in embeddings
            projection_dim (int defaul None): a parameter in DNC (Deep Cross Network) https://www.tensorflow.org/recommenders/api_docs/python/tfrs/layers/dcn/Cross.  In practice, we've observed that projection_dim = d/4 consistently preserved the accuracy of a full-rank version.
            rating_weight (float default 0): the weight of rating task in multitask loss function
            retrieval_weight (float default 1): the weight of retrieval task in multitask loss function
            user_layer_sizes (list of int, default=[32]): Used in user tower; A list of integers where the i-th entry represents the number of units the i-th layer contains.
            item_layer_sizes (list of int, default=[32]): Used in item tower; A list of integers where the i-th entry represents the number of units the i-th layer contains.
            loss_name (str): tf loss function, choose from ['mse', 'pairwise', 'listwise'] corresponding to optimization methods [tf.keras.losses.MeanSquaredError(),tfr.keras.losses.PairwiseHingeLoss(), tfr.keras.losses.ListMLELoss()]
            l1_l2(int, default 0): 0:not using regularization,  1: use regularizer.l1, 2: use regularizer.l2
            alpha (float default 1e-4): a parameter for tensorflow.keras.regularizers.l1 or l2
            dropout_rate (float default .5): dropout rate in tensorflow.keras.Dropout(dropout)
            lr:float=0.1 learning rate for model compile

            ### in addition to the hyperparameters in config, we need raw input data &/ parameters, includes
            df:pd.DataFrame, e.g. df, interactions_df
            interactions:TFDS dataset=None, it speeds up when using prefab interactions instead of making it on spot from strach
            items:TFDS dataset
        """
        super().__init__()
        self.config = config

        # the following info is used for model serving
        #global_vars=config['global_vars']
        label=config['label']
        #df=config['df']
        user_id=config['user_id']
        item_id=config['item_id']
        u_cnt_cols = config['u_cnt_cols']
        u_cat_cols = config['u_cat_cols']
        u_cnt_cols_bins = config['u_cnt_cols_bins']
        u_txt_cols = config['u_txt_cols']
        u_img_cols = config['u_img_cols']
        i_cnt_cols = config['i_cnt_cols']
        i_cat_cols = config['i_cat_cols']
        i_cnt_cols_bins = config['i_cnt_cols_bins']
        i_txt_cols = config['i_txt_cols']
        i_img_cols = config['i_img_cols']
        #interactions=config['interactions']
        embedding_dim = config['embedding_dim']
        layer_sizes=config['layer_sizes']
        flag_use_norm=config['flag_use_norm']
        projection_dim=config['projection_dim']
        rating_weight = config['rating_weight']
        retrieval_weight = config['retrieval_weight']
        user_layer_sizes=config['user_layer_sizes']
        item_layer_sizes=config['item_layer_sizes']
        clf_loss_name=config['clf_loss_name']
        rating_loss_name=config['rating_loss_name']
        clf_metrics=config['clf_metrics']
        rating_metrics=config['rating_metrics']
        l1_l2=config['l1_l2']
        alpha=config['alpha']
        dropout_rate=config['dropout_rate']
        #items=config['items']
        flag_vsn_deep_cross=config['flag_vsn_deep_cross'],
        num_projection_layers=config['num_projection_layers']
        flag_trainable=config['flag_trainable']
        num_classes= len(config['label_num2str'].values()) #config['num_classes']
        flag_dcn_cross= config['flag_dcn_cross']

        self.user_embeddings = None #use as a placeholder for user embeddings
        self.item_embeddings = None #use as a placeholder for item embeddings
        self.cross_ui_embeddings = None #use as a placeholder for user-item interaction embeddings

        # the following 2 variable is used for model understanding
        self._all_features = list(set(u_cat_cols) | set(i_cat_cols) |set(u_cnt_cols_bins.keys())| set(i_cnt_cols_bins.keys()) | set(u_txt_cols) | set(i_txt_cols)| set(u_img_cols) | set(i_img_cols))

        #self._cross_layer = Model(tfrs.layers.dcn.Cross(projection_dim=projection_dim, kernel_initializer="glorot_uniform"))
        self._cross_layer = tfrs.layers.dcn.Cross(projection_dim=projection_dim, kernel_initializer="glorot_uniform")

        # define user_model & item_model
        self.user_model = DeepCrossEmbeddingModel(df, label, user_id, item_id, u_cnt_cols, u_cat_cols, u_cnt_cols_bins, u_txt_cols, u_img_cols, interactions, embedding_dim, flag_use_norm, user_layer_sizes, projection_dim, l1_l2, alpha, dropout_rate)
        self.item_model = DeepCrossEmbeddingModel(df, label, user_id, item_id, i_cnt_cols, i_cat_cols, i_cnt_cols_bins, i_txt_cols, i_img_cols, interactions, embedding_dim, flag_use_norm, item_layer_sizes, projection_dim, l1_l2, alpha, dropout_rate)

        ###########################################################################
        ### rating model (deep network is implemented here at rating/ranking model)
        ###########################################################################
        # define rating model

        #NOTE: enforce the last layer size to be 1, b/c it refers to the predicted rating, a number
        if layer_sizes[-1]!=1:
            layer_sizes=layer_sizes+[1]
        self.rating_model = create_deep_cross_layers(layer_sizes=layer_sizes, projection_dim=projection_dim, l1_l2=l1_l2, alpha=alpha, dropout_rate=dropout_rate, name='rating_model', flag_dcn_cross=flag_dcn_cross)




        ## define rating task to compute rating loss

        # define the dictionary for loss function
        rating_loss_dict = {'mse':tf.keras.losses.MeanSquaredError(),
                     'pairwise':tfr.keras.losses.PairwiseHingeLoss(),
                     'listwise':tfr.keras.losses.ListMLELoss()}

        self.rating_task: tf.keras.layers.Layer = tfrs.tasks.Ranking(
            loss=rating_loss_dict[rating_loss_name], #tf.keras.losses.MeanSquaredError(),
            metrics=[
                tfr.keras.metrics.NDCGMetric(name="ndcg_metric"),
                tf.keras.metrics.RootMeanSquaredError("RMSE"),
                tfr.keras.metrics.PrecisionMetric('precision'),
                tfr.keras.metrics.RecallMetric('recall'),
                ]
        )
        ##########################################################################
        ### classification model: define & compile
        ##########################################################################
        # define classification model
        self.clf_model = tf.keras.models.Sequential([#tf.keras.layers.Concatenate(),
                                                    tf.keras.layers.Dense(256,activation ='tanh', # 'relu',
                                                        kernel_regularizer=tf.keras.regularizers.L1(0.01)),
                                                    tf.keras.layers.Dropout(0.2),
                                                    tf.keras.layers.Dense(num_classes, activation = 'softmax'),
                                                    ], name='classification_model')
        # compile classification model with (loss, metrics, optimizer)

        # define the dictionary for clf loss function
        clf_loss_dict = {'SparseCategoricalFocalLoss':SparseCategoricalFocalLoss(gamma=2),
                        'SparseCategoricalCrossentropy':tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                        'CategoricalCrossentropy': tf.keras.losses.CategoricalCrossentropy(from_logits=False),
                        'SigmoidFocalCrossEntropy': tfa.losses.SigmoidFocalCrossEntropy(from_logits = False,
                                                                    alpha= 0.25,
                                                                    gamma= 2.0,
                                                                    reduction=tf.keras.losses.Reduction.AUTO,
                                                                    name = 'sigmoid_focal_crossentropy'
                                                                    ),
                        }
        if config['clf_loss_name'] in ['CategoricalCrossentropy','SigmoidFocalCrossEntropy']:
          self.config['flag_one_hot_encode_label']=True
          self.config['clf_metrics']=METRICS


        else: # loss_name in ['SparseCategoricalCrossentropy','SparseCategoricalFocalLoss',, ]
          self.config['flag_one_hot_encode_label']=False
          self.config['clf_metrics']=[tf.keras.metrics.SparseCategoricalAccuracy()]

        self.clf_model.compile(optimizer=self.config['clf_optimizer'],
                              loss=clf_loss_dict[self.config['clf_loss_name']],
                              metrics=self.config['clf_metrics'],
                              #run_eagerly=True,
                              )

        #----------------------------------clf_model2 START
        self.clf_model2 = create_iu_compiled_model(global_vars=self.config['global_vars'],
                                                  flag_trainable=self.config['flag_trainable'],
                                                  num_bins=self.config['num_bins'],
                                                  embedding_dim=self.config['embedding_dim'],
                                                  flag_txt_bert=self.config['flag_txt_bert'],
                                                  num_projection_layers=self.config['num_projection_layers'],
                                                  dropout_rate=self.config['dropout_rate'],
                                                  flag_bin_cnt=self.config['flag_bin_cnt'],

                                                  layer_sizes=self.config['layer_sizes'],
                                                  projection_dim=self.config['projection_dim'],
                                                  l1_l2=self.config['l1_l2'],
                                                  alpha=self.config['alpha'],
                                                  )
        #----------------------------------clf_model2 END
        self.clf_task: tf.keras.layers.Layer = Classification(loss=clf_loss_dict[clf_loss_name], #tf.keras.losses.MeanSquaredError(),
                                                              metrics=self.config['clf_metrics']
                                                              )

        ###########################################################
        ### retrieval
        ###########################################################
        self.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(
            metrics=tfrs.metrics.FactorizedTopK(
            candidates=items.batch(128).map(self.item_model)
            )
        )

        ## define input and outputs for SHAP interpretation
        #self.input = df
        self.outputs = None # will be updated as the output of call()


    def call(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:
        """call method: input embs and output predictions"""
        user_dic = {self.config['user_id']: features[self.config['user_id']],
                    self.config['item_id']: features[self.config['item_id']],
                    **{cat_col: features[cat_col] for cat_col in self.config['u_cat_cols']},
                    **{cnt_col: features[cnt_col] for cnt_col in self.config['u_cnt_cols_bins'].keys()},
                    **{txt_col: features[txt_col] for txt_col in self.config['u_txt_cols']},
                    **{img_col: features[img_col] for img_col in self.config['u_img_cols']},
                    }

        item_dic = {self.config['user_id']: features[self.config['user_id']],
                    self.config['item_id']: features[self.config['item_id']],
                    **{cat_col: features[cat_col] for cat_col in self.config['i_cat_cols']},
                    **{cnt_col: features[cnt_col] for cnt_col in self.config['i_cnt_cols_bins'].keys()},
                    **{txt_col: features[txt_col] for txt_col in self.config['i_txt_cols']},
                    **{img_col: features[img_col] for img_col in self.config['i_img_cols']},
                    }
        user_embeddings = self.user_model(user_dic)
        item_embeddings = self.item_model(item_dic)

        concat_ui_emb = tf.concat([user_embeddings, item_embeddings], axis=1)

        # Build Cross Network
        cross_ui_emb = self._cross_layer(concat_ui_emb)

        # make rating predictions
        rating_predictions = self.rating_model(cross_ui_emb)
        self.rating_predictions = rating_predictions #will be used for SHAP interpretation

        # make clf predictions
        clf_predictions = self.clf_model2(cross_ui_emb)
        self.clf_predictions = clf_predictions
        #print(f'>>>>>>>>>>>>>>>>>>>>>clf_predictions={clf_predictions}')

        # update various embeddings
        self.user_embeddings = user_embeddings
        self.item_embeddings = item_embeddings
        self.cross_ui_embeddings = cross_ui_emb

        return (user_embeddings, item_embeddings, rating_predictions, clf_predictions)

    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:
        """compute_loss: input y_pred & y_true and output loss"""
        ## ratings go here as a method to compute loss
        labels = features.pop(self.config['label'])

        user_embeddings, item_embeddings, rating_predictions, clf_predictions = self(features)

        # compute rating_loss
        rating_loss = self.rating_task(
            labels=tf.reshape(labels,[-1,1]), #ratings.shape == (None,), but expect rank 2 ==> Solu: reshape it!!
            predictions=tf.reshape(rating_predictions,[-1,1]) # tf.squeeze(rating_predictions, axis=-1).shape == (None,), but expect rank 2 ==> solu: reshape it
        )

        # compute clf_loss
        if self.config['flag_one_hot_encode_label']:
            #one hot encode labels
            num_classes = len(self.config['label_num2str'].values())
            labels = tf.one_hot(labels, num_classes)

        #print(f'000000000>>>>>>>>>>>>>>>>>>>>>labels={labels}')
        #print(f'111111111>>>>>>>>>>>>>>>>>>>>>clf_predictions={clf_predictions}')

        clf_loss = clf_loss_dict[self.config['clf_loss_name']](
                                                              y_true=labels, #ratings, #tf.reshape(ratings,[-1,1]), #ratings.shape == (None,), but expect rank 2 ==> Solu: reshape it!!
                                                              y_pred=clf_predictions, #tf.reshape(classification_predictions,[-1,1]), # tf.squeeze(rating_predictions, axis=-1).shape == (None,), but expect rank 2 ==> solu: reshape it
                                                              )
        """clf_loss = self.clf_task(
            labels=tf.reshape(labels,[-1,1]), #ratings.shape == (None,), but expect rank 2 ==> Solu: reshape it!!
            predictions=tf.reshape(rating_predictions,[-1,1]) # tf.squeeze(rating_predictions, axis=-1).shape == (None,), but expect rank 2 ==> solu: reshape it
        )"""

        # compute retrieval_loss
        retrieval_loss = self.retrieval_task(user_embeddings, item_embeddings)

        # create weighted_loss by combining them using the loss weights.
        weighted_loss= (self.config['rating_weight'] * rating_loss
                      + self.config['retrieval_weight'] * retrieval_loss
                      + self.config['classification_weight'] * clf_loss)

        return weighted_loss
    # To easily update compuation latter as needed, implement get_config() and from_config()
    # ref: https://www.tensorflow.org/guide/keras/save_and_serialize
    def get_config(self):
            return self.config
    @classmethod
    def from_config(cls, config):
        return cls(**config)

    def fit(self,
          train_df,
          valid_df,
          test_df,
          ):
        # attributes needed to fit/train the model
        epochs=self.config['epochs']
        model_path=self.config['model_path']
        model_name=self.config['model_name']
        model_type=self.config['model_type']
        train_valid_test=self.config['train_valid_test']
        batch_size=self.config['batch_size']
        max_lr=self.config['max_lr']
        min_lr=self.config['min_lr']
        flag_balance_train_df=self.config['flag_balance_train_df']
        flag_init_bias_class_weight=self.config['flag_init_bias_class_weight']

         # df to ds
        if flag_balance_train_df:
            train_ds=df_to_balanced_ds(df=train_df, flag_training=True, batch_size=batch_size, flag_bin_cnt=flag_bin_cnt, resize=(128, 128), flag_one_hot_encode_label=self.config['flag_one_hot_encode_label'], global_vars=self.config['global_vars'])
        else:
            train_ds=df_to_ds(df=train_df, flag_training=True, batch_size=batch_size, flag_bin_cnt=flag_bin_cnt, resize=(128, 128), flag_one_hot_encode_label=self.config['flag_one_hot_encode_label'], global_vars=self.config['global_vars'])

        if valid_df is not None:
            valid_ds=df_to_ds(df=valid_df, flag_training=False, batch_size=batch_size, flag_bin_cnt=flag_bin_cnt, resize=(128, 128), flag_one_hot_encode_label=self.config['flag_one_hot_encode_label'], global_vars=self.config['global_vars'])
        else:
            valid_ds = None
        if test_df is not None:
            test_ds=df_to_ds(df=test_df, flag_training=False, batch_size=batch_size, flag_bin_cnt=flag_bin_cnt, resize=(128, 128), flag_one_hot_encode_label=self.config['flag_one_hot_encode_label'], global_vars=self.config['global_vars'])
        else:
            test_ds = None

        # to handle imbalanced classification issue, calculate class_weight, bias_init
        class_weight, bias_init = init_imbalanced_class_weight_bias(df=train_df, lable=label)
        if not flag_init_bias_class_weight:
            class_weight = {k:1 for (k,v) in class_weight.items()} # assign the same weight 1 to different classes
            bias_init = tf.constant_initializer(np.array([0 for (k,v) in class_weight.items()])) # assign 0s to bias

        print(f'===========class_weight = {class_weight}\n==========bias_init={bias_init}')



        if flag_balance_train_df: # TBD: SGDRScheduler is NOT working for balanced train_df yet
            scheduler_ls = []
        else:
          if flag_SGDRScheduler:
              scheduler_ls = []
          else:

              scheduler_ls = [SGDRScheduler(min_lr=min_lr,
                                      max_lr=max_lr,
                                      lr_decay=.9,
                                      cycle_length=3,
                                      mult_factor=1.5)]
        mode_dict={'val_prc':'max', 'val_loss':'min'}
        callbacks = [
                    tf.keras.callbacks.EarlyStopping(     #EarlyStopping is used to stop at the epoch where val_accuracy does not improve significantly
                        monitor=monitor,#'val_prc', #'val_loss'
                        mode=mode_dict[monitor], #'max', #'min'
                        min_delta=1e-4,
                        patience=4,
                        verbose=1
                    ),
                    tf.keras.callbacks.ModelCheckpoint(
                        filepath=model_path+'weights',# replace 'weights.h5'
                        monitor=monitor, #'val_prc', #'val_loss'
                        mode=mode_dict[monitor], #'max', #'min'
                        save_best_only=True,
                        save_weights_only=True,
                        verbose=1
                        ),
                    ] + scheduler_ls

        # 2 inputs

        if valid_ds is not None:
            history = self.clf_model2.fit(train_ds, #[train_ds, train_ds], #x=dict(X_train), y=y_train,
                                          batch_size=batch_size,
                                          validation_data= valid_ds, #[valid_ds, valid_ds],#(dict(X_test), y_test),
                                          epochs=epochs,
                                          callbacks=callbacks,
                                          class_weight=class_weight,
                                          )
        else:
            history = self.clf_model2.fit(train_ds, #[train_ds, train_ds], #x=dict(X_train), y=y_train,
                                          batch_size=batch_size,
                                          validation_split= 0.3, #[valid_ds, valid_ds],#(dict(X_test), y_test),
                                          epochs=epochs,
                                          callbacks=callbacks
                                          )
        if test_ds is not None:
            test_metrics = self.clf_model2.evaluate(test_ds, verbose=False, return_dict=True)
            print(f'==========model test_metrics = {test_metrics}==========')

        """#model.load_weights('weights.h5')
        if model_name is not None:
          #tf.saved_model.save(model, model_path+model_name)
          self.model.save(model_path+model_name)
          print(f"========To load this model: model = tf.keras.models.load_model({model_path}{model_name}) =======")
        """
        #update self.model as the compiled & fitted version
        self.clf_model2 = self.clf_model2
        return self.clf_model2

    def predict(self,
              test_df:pd.DataFrame,
              label:str,#=label,
              labels:list,#=[2,1,0]
              ):

        test_ds = df_to_ds(test_df, global_vars=self.config['global_vars'])

        y_prob = self.clf_model2.predict(test_ds)

        y_pred = y_prob.argmax(axis=1)

        #visualize performance
        _ = get_performance_metrics(y_true=test_df[label],
                                    y_prob=y_prob,
                                    y_pred=None,
                                    labels=labels,
                                    normalize='all',
                                    )
        return y_pred, y_prob


####################################


# Cell

#@title create_model
def create_model(config:dict,#=default_config,
                 df:pd.DataFrame,
                 interactions,
                 items,
                 #model_type:str='VSN_L1_2task'
                 optimizer_name=['Adagrad', 'Adam'][1],
                 ):
    """instantiate a DeepCrossMultitaskModel model and compile it
    Args:
        config (dict): the configuration dictionary to specify DeepCrossMultitaskModel(), which need to be consistent with model.get_config().
    Return:
        model (DeepCrossMultitaskModel): a compiled model
    Example:
    """
    if config['model_type']=='VSN_L1_2task':
        model = VSN_DeepCross2taskModel(config=config,
                                        df=df,
                                        interactions=interactions,
                                        items=items,
                                        )
    elif config['model_type']=='VSN_L1_3task':
        model = VSN_DeepCross3taskModel(config=config,
                                        df=df,
                                        interactions=interactions,
                                        items=items,
                                        )
    elif config['model_type']=='L1_3task':
        model = DeepCross3taskModel(config=config,
                                  df=df,
                                  interactions=interactions,
                                  items=items,
                                  )
    else: # config['model_type']=='L1_2task':
        model = DeepCross2taskModel(config=config,
                                        df=df,
                                        interactions=interactions,
                                        items=items,
                                        )

    optimizer_dict = {'Adam': tf.keras.optimizers.Adam(learning_rate=config['lr']),
                      'Adadelta': tf.keras.optimizers.Adadelta(learning_rate=config['lr']),
                      'Adagrad': tf.keras.optimizers.Adagrad(lr=config['lr'], global_clipnorm=1.0),
                      }
    model.compile(optimizer=optimizer_dict[optimizer_name],
                  #loss=config['clf_loss_name'], disable here! b/c it's a multi-task optimization
                  )
    #model.compile(optimizer=tf.keras.optimizers.SGD(lr=config['lr'],momentum=0.9, clipnorm=1.0)) , #clipvalue=5.0
    return model


# Cell

#@title helpers to create_fit_validate_evaluate_model


def plot_history(history,
                 metrics=['factorized_top_k/top_50_categorical_accuracy',
                          'factorized_top_k/top_100_categorical_accuracy',
                          'ndcg_metric',
                          ]
                 ):

    l=len(history.history["factorized_top_k/top_100_categorical_accuracy"])
    n_epoch = [i for i in range(l)]

    # plot loss during training
    plt.title('Loss')
    plt.plot(history.history['loss'], label='train')
    plt.plot(history.history['val_loss'], label='valid')
    plt.legend()
    plt.show()

    for metric in metrics:
      plt.plot(n_epoch, history.history[metric], label=metric)
      plt.title(f"{metric} vs epoch")
      plt.xlabel("epoch")
      plt.ylabel(f"{metric}");
      plt.legend()



def create_fit_validate_evaluate_model(config=default_config):
    """create, fit, validate, evaluate model using config, also plot fitting history"""
    # initantiate & compile model using a wrapper function create_model()
    model = create_model(config=config,
                        df=config['df'],
                        interactions=config['interactions'],
                        items=config['items'],
                        #optimizer=config['optimizer'],
                        #model_type= config['model_type'],
                        )

    if config['flag_SGDRScheduler']:
        scheduler_ls = [SGDRScheduler(min_lr=config['min_lr'],
                                      max_lr=config['max_lr'],
                                      lr_decay=.9,
                                      cycle_length=3,
                                      mult_factor=1.5)]
    else:
        scheduler_ls = []


    mode_dict={'val_prc': 'max', 'loss':'min', 'val_ndcg_metric':'max','val_RMSE':'min','val_precision':'max','val_recall':'max','val_factorized_top_k/top_1_categorical_accuracy':'max','val_factorized_top_k/top_5_categorical_accuracy':'max','val_factorized_top_k/top_10_categorical_accuracy':'max','val_factorized_top_k/top_50_categorical_accuracy':'max','val_factorized_top_k/top_100_categorical_accuracy':'max','val_loss':'min','val_regularization_loss':'min','val_total_loss':'min'}

    callbacks = [
                tf.keras.callbacks.EarlyStopping(     #EarlyStopping is used to stop at the epoch where val_accuracy does not improve significantly
                    monitor=config['monitor'],
                    min_delta=1e-4,
                    patience=config['patience'],
                    mode=mode_dict[config['monitor']],
                    verbose=1,
                    restore_best_weights=True,
                ),
                tf.keras.callbacks.ModelCheckpoint(
                    filepath=config['model_path'], #'weights.h5',
                    monitor=config['monitor'],
                    mode=mode_dict[config['monitor']],
                    save_best_only=True,
                    save_weights_only=True,
                    verbose=1,
                    ),
                #sgd_scheduler, # NOT working for flag_balance_train_df=True,
                ] + scheduler_ls

    """# define early stopping to avoid overfitting
    model_monitor = EarlyStopping(monitor='loss', #replace 'total_loss'
                            min_delta=1e-3,
                            patience=5,
                            verbose=0,
                            mode='auto',
                            restore_best_weights=True)

    # set model checkpoint to record the weight during & after fitting
    model_checkpoint=tf.keras.callbacks.ModelCheckpoint(filepath=default_config['model_path']+default_config['model_type']+'_epochs_'+str(default_config['epochs']),
                                                        monitor='loss',#replace 'total_loss'
                                                        verbose=0,
                                                        save_best_only=True,
                                                        save_freq='epoch',
                                                        save_weights_only=True)
    """

    # fit model with trainning data, validate model with validation data
    history = model.fit(config['cached_train'],
                        validation_data=config['cached_valid'],
                        validation_freq=1,
                        epochs=config['epochs'],
                        callbacks=callbacks,
                        verbose=0)
    """#clf_model2
    history=model.fit(train_df,
                      valid_df,
                      test_df,
                      )"""
    #accuracy = history.history['factorized_top_k/top_100_categorical_accuracy']
    #print(f"Validation Data: Top-100 accuracy = {accuracy}.")

    # evaluate model with test data
    test_metrics = model.evaluate(config['cached_test'],
                                  return_dict=True)

    print(f"Test Data: metrics={test_metrics}")

    #We will plot performance metrics (e.g. accuracy) vs epochs:
    plot_history(history,
                 metrics=config['plot_metrics'],
                 )

    return model


# Cell

#title recommend items to user
def recommend_items_to_user(model,
                            u_id:str,
                            top_k:int=5,
                            global_vars:tuple=None,
                            global_data:tuple=None,

                            ):
    """recommend top rating items to user (with id as u_id)
    Args:
      model: a trained model
      u_id:str u_is should be a string in unique_user_ids
      top_k:int=5
    Returns:
      top_k rated item by model for user u_id
    """
    #unpack global vars & global data
    (label, user_id, item_id, x_cols, u_cols, i_cols, cnt_cols, u_cnt_cols, i_cnt_cols,cnt_cols_to_bin, u_cnt_cols_to_bin, i_cnt_cols_to_bin,bin_cnt_cols, u_bin_cnt_cols, i_bin_cnt_cols, cat_cols, u_cat_cols, i_cat_cols, txt_cols, u_txt_cols, i_txt_cols, img_cols, u_img_cols, i_img_cols, dt_cols, u_dt_cols, i_dt_cols, label_str2num,label_num2str, num_classes, label_keys) = global_vars
    (df, interactions, users, items, unique_user_ids, unique_item_ids, train_df, valid_df, test_df, cached_train, cached_valid, cached_test, test_items) = global_data

    if not u_id in unique_user_ids:
        print(f'{u_id} is NOT a valid user')
        #return None

    # the user profile of u_id
    user_df = df[df[user_id]==u_id][[user_id]+u_cols].iloc[:1,:]

    # all the available items
    try: # to include label if available
      items_df = pd.DataFrame([df[df[item_id] == i_id].iloc[0,:][[label]+i_cols] for i_id in unique_item_ids])
    except:
      items_df = pd.DataFrame([df[df[item_id] == i_id].iloc[0,:][i_cols] for i_id in unique_item_ids])

    # repeat user_df for n=items_df.shape[0] times
    df_tmp = user_df.loc[user_df.index.repeat(items_df.shape[0])]
    df_tmp.set_index(keys=items_df.index)

    # repeat the one-row user_df for multiple (items_df.shape[0]) times
    df_tmp = user_df.loc[user_df.index.repeat(items_df.shape[0])]

    # construct item-user-interaction df by horizontally merging items_df and repeated-version-user_df
    test_df1 = pd.concat([items_df, df_tmp.set_index(keys=items_df.index)], axis=1)

    # example of using trained model to make predictions
    #model = best_model
    if (model.config['model_type']=='VSN_L1_2task') or (model.config['model_type']=='L1_2task'):
      u_emb, i_emb, pred_rating = model(dict(test_df1)) # make predictions
      print(f'========= 2-task model: {model.config["model_type"]}')
    elif model.config['model_type']=='VSN_L1_3task':
      u_emb, i_emb, pred_rating, prob_clf = model(dict(test_df1)) # make predictions
      print(f'========= 3-task model: {model.config["model_type"]}')

    # the indices of decending predicted rating
    sorted_idx = (-1*pred_rating.numpy().reshape(-1,)).argsort(axis=0)

    # dbck:
    # prediced ratings ranked descending
    #print("======Double check sorted_idx: expect descending ratings======")
    #print(pred_rating.numpy().reshape(-1,)[sorted_idx])
    # the top_k recommended items
    try: # in case table is available for test_df1
      sorted_rec = test_df1.iloc[sorted_idx, :][[label]+i_cols]
    except:
      sorted_rec = test_df1.iloc[sorted_idx, :][i_cols]

    sorted_rec['predicted_rating'] = pred_rating.numpy().reshape(-1,)[sorted_idx]
    from scipy.stats.stats import pearsonr, spearmanr
    print(f"====== spearman_cor(rating, predicted_rating)={spearmanr(sorted_rec[label],sorted_rec['predicted_rating'])}")
    top_rec = sorted_rec.iloc[:top_k,:]
    return top_rec[i_cols+[label, 'predicted_rating']]




# Cell

#@title helpers to save and reload model

def convert_model_config_to_py(model_config):
    """clean up the DeepCrossMultitask model config which contains
    'tensorflow.python.training.tracking.data_structures._DictWrapper'=> dict
    'tensorflow.python.training.tracking.data_structures.ListWrapper' => list
    'tensorflow.python.data.ops.dataset_ops.MapDataset' => pd.DataFrame
    Args:
        model_confit (dict): e.g. model_config = model.get_config()
    Returns:
        d (dict): a normal python dictionary that can be pickled
        tfds2df_ls (list): a list of keys that was in tfds format but converted into df, those items in tfds2df_ls will be converted fr. df back to tfds
    Example:
        ready_model_config, tfds2df_ls = convert_model_config_to_py(model_config=model.get_config())

    """
    d = model_config.copy()
    tfds2df_ls = []
    #other_dtype_ls = []
    for key in d.keys():
        if isinstance(d[key],pd.DataFrame):
            print(f'key={key} is converted fr.{type(d[key])} into pd.DataFrame')
            d[key]=pd.DataFrame(d[key])
        elif isinstance(d[key],list): # dealing ListWrapper
            print(f'key={key} is converted fr.{type(d[key])} into list')
            d[key]=list(d[key])
        elif isinstance(d[key],dict): # dealing _DictWrapper
            print(f'key={key} is fr.{type(d[key])} converted into dict')
            d[key]=dict(d[key])
        elif not isinstance(d[key],(str, float, int, bool)):
            try: #dealing MapDataset
                print(f'------key={key} is converted fr.{type(d[key])} into pd.DataFrame------')
                d[key]=pd.DataFrame(list(d[key].as_numpy_iterator()))
                tfds2df_ls.append(key)
            except: pass

    return d, tfds2df_ls

def prep_df_tfds(df, cols):
    """turn df into tf.data.Dataset format, which is ready to digested by """
    try:df = df[cols].drop_duplicates()
    except:
      df_tmp=pd.DataFrame()
      for col in cols:
        df_tmp[col] = df[cols[0]].tolist()[0]
      df = df_tmp.copy()
      df = df[cols].drop_duplicates()
    df_dict = {name: np.array(value) for name, value in df.items()}
    df_tfds = tf.data.Dataset.from_tensor_slices(df_dict)

    return df_tfds.map(lambda x: {**{col: x[col] for col in cols}})

def convert_model_config_to_tf(model_config, tfds2df_ls):
    """convert model_config from a normal python dictionary to the tf format which is digestable by create_model()
    Args:
        model_config (dict): a normal python dictionary e.g. model_config, tfds2df_ls = convert_model_config_to_df(model.get_config())
        tfds2df_ls (list of str)
    Return:
        model_config (dict): a dict in tfds format which is digestable by create_model(), e.g. my_model = create_model(model_config)
    """
    #convert df back to tfds
    for key in tfds2df_ls:
        model_config[key]=prep_df_tfds(df=model_config[key], cols=model_config[key].columns)
        print(f'===== key={key} is converted from df back to tfds=====')
    return model_config

def save_model(model, weight_filepath:str='saved_model/model_weight', config_filepath:str='saved_model/model_config.pkl'):
    """save the model weight & config
    Args:
        model (DeepCrossMultitaskModel): after trained
    Example:
        model,_,_ = get_best_model()
        save_model(model, weight_filepath='saved_model/best_model_weight', config_filepath='saved_model/best_model_config.pkl')
    """

    #!mkdir -p saved_model
    # save model weights
    model.save_weights(filepath=weight_filepath, save_format='tf')

    #temperatively disable optimizer
    model_config = model.get_config()
    ready_model_config, tfds2df_ls = convert_model_config_to_py(model_config)
    ready_model_config['clf_optimizer'] = None
    ready_model_config['clf_metrics'] = None
    ready_model_config['rating_optimizer'] = None
    ready_model_config['rating_metrics'] = None

    # pickle model config
    import pickle
    with open(config_filepath, 'wb') as handle:
        pickle.dump((ready_model_config, tfds2df_ls), handle, protocol=pickle.HIGHEST_PROTOCOL)

def reload_model(df,#=df,
                 interactions, #=interactions,
                 items, #=items,
                 #model_type, #='vsn_3',
                 weight_filepath:str='saved_model/model_weight',
                 config_filepath:str='saved_model/model_config.pkl',
                 ):
    """reload the model weight & config
    Example:
        new_model = reload_model(weight_filepath='saved_model/best_model_weight', config_filepath='saved_model/best_model_config.pkl')
    """
    import pickle
    with open(config_filepath, 'rb') as handle:
        (model_config, tfds2df_ls) = pickle.load(handle)
        model_config = convert_model_config_to_tf(model_config, tfds2df_ls)


    # ini & compile a new model using the reloaded model config
    new_model = create_model(config=model_config,
                             df=df,
                             interactions=interactions,
                             items=items,
                             #optimizer=optimizer,
                             #model_type=model_type,
                             )

    # reload the weights back to the new model
    new_model.load_weights(weight_filepath)
    return new_model



# Cell

#@title Bayesian Hyperparameters Tuning

from bayes_opt import BayesianOptimization


def get_best_model(metric='factorized_top_k/top_100_categorical_accuracy',
                   model_config=default_config,
                   df:pd.DataFrame=None,#df,
                   interactions=None,#interactions,
                   items=None,#items,
                   cached_train=None,#cached_train,
                   cached_valid=None,#cached_valid
                   cached_test=None, #cached_test,
                   ):
    """get the best model according to metric
    Args:
        metric (str, default 'ndcg_metric'): metric to maximize, choose from ['ndcg_metric','factorized_top_k/top_100_categorical_accuracy', 'factorized_top_k/top_50_categorical_accuracy',...,'RMSE', 'total_loss'] # we need to minimize 'RMSE', 'total_loss' , but maximize others
        model_config=None,#default_config_vsn
        df=None,#df
        interactions=None, #interactions,
        items=None, #items,
        model_type='vsn_3', #['vsn', 'vsn_3', 'dcm'],
        cached_train=None, #cached_train,
        cached_test=None, #cached_test,
    Returns:
      best_model: e.g. a trained DeepCross3MultitaskModel|VSN_DeepCrossMultitaskModel|VSN_DeepCross3MultitaskModel
      best_metrics:dict
      best_para:dict

    """
    # declare parameters bounds (hyperparameter search space)
    pbounds = {'flag_use_norm':(-.49,1.49),
        'rating_weight': (0,1),
        'retrieval_weight':(0,0.3),
        #'classification_weight':(0,1),
        'learning_rate': (1e-03, 1e-01),
        'n_layers': (0.51,3.49),
        'layer_1': (20, 160),
        'layer_2': (50, 80),
        'layer_3': (20, 40),
        'loss_name':(-1.49,1.49),
        'l1_l2':(-0.49,2.49),
        'alpha':(0,1e-1),
        'dropout_rate':(0,0.5),
        #'flag_vsn_deep_cross':(-.49,1.49),
        'model_type': (-0.49,1.49),
        }

     #unpack global vars & global data
    (label, user_id, item_id, x_cols, u_cols, i_cols, cnt_cols, u_cnt_cols, i_cnt_cols,cnt_cols_to_bin, u_cnt_cols_to_bin, i_cnt_cols_to_bin,bin_cnt_cols, u_bin_cnt_cols, i_bin_cnt_cols, cat_cols, u_cat_cols, i_cat_cols, txt_cols, u_txt_cols, i_txt_cols, img_cols, u_img_cols, i_img_cols, dt_cols, u_dt_cols, i_dt_cols, label_str2num,label_num2str, num_classes, label_keys) = model_config['global_vars']
    #(df, interactions, users, items, unique_user_ids, unique_item_ids, train_df, valid_df, test_df, cached_train, cached_valid, cached_test, test_items) = model_config['global_data']

    ####################Start def fit_model()
    def fit_model(flag_use_norm:float,
              rating_weight:float,
              retrieval_weight:float,
              #classification_weight:float,
              learning_rate:float,
              n_layers:float,
              layer_1:float,
              layer_2:float,
              layer_3:float,
              loss_name:float,
              l1_l2:float,
              alpha:float,
              dropout_rate:float,
              #flag_vsn_deep_cross:bool,
              model_type:float,
              return_best_model=False,
              metric:str='RMSE',
              epochs:int=50,
              model_config:dict=model_config,
              df:pd.DataFrame=df, #df
              interactions=interactions, # interactions
              items=items, # items
              cached_train=cached_train,
              cached_valid=cached_valid,
              cached_test=cached_test,
              ):
        """fit a DeepCrossMultitaskModel with parameters
        Args:
            #embedding_dim:int,
            flag_use_norm:float, will be mapped to {0,1}, indicate whether use cnt_normalization info
            rating_weight:float, within [0,1]
            retrieval_weight:float, within [0,1]
            #classification_weight:float, within [0,1]
            learning_rate:float learning rate
            n_layers:float,
            layer_1:float layer 1 size
            layer_2:float layer 2 size
            layer_3:float layer 3 size
            loss_name:float, will be mapped to one of {'listwise', 'mse', 'pairwise'}
            return_best_model:bool=False: set it True only AFTER the best model has been found and you want to save it for future use
            metric:str, choose from ['ndcg_metric','factorized_top_k/top_100_categorical_accuracy', 'factorized_top_k/top_50_categorical_accuracy',...,'RMSE', 'total_loss'] # we need to minimize 'RMSE', 'total_loss' , but maximize others
            epochs (int, default 50): the number of epochs to train the neural network model
            dropout_rate:float
            model_type:float, will be mapped to one of {'L1_2task', 'VSN_L1_2task'} by dict {0:'L1_2task', 1:'VSN_L1_2task'}
            model_config:dict=default_config_vsn
            df:pd.DataFrame=None, #df
            interactions=None, # interactions
            items=None, # items
        """

        if round(n_layers) == 2:
            layer_sizes = [round(layer_1), round(layer_2)]
        elif int(n_layers) == 3:
            layer_sizes = [round(layer_1), round(layer_2), round(layer_3)]
        else:
            layer_sizes = [round(layer_1)]

        loss_dict = {-1:'pairwise', 0:'listwise', 1:'mse'}
        norm_dict = {0:False, 1:True}
        model_type_dict = {0:'L1_2task', 1:'VSN_L1_2task'}

        # update model config (e.g. default_config_vsn) with user input data
        model_config['flag_use_norm']=norm_dict[round(flag_use_norm)] # flag_use_norm in {True, False}
        model_config['rating_weight']=rating_weight
        #model_config['classification_weight']=classification_weight
        model_config['retrieval_weight']=retrieval_weight
        model_config['layer_sizes']=layer_sizes
        model_config['user_layer_sizes']=layer_sizes
        model_config['item_layer_sizes']=layer_sizes
        model_config['loss_name']=loss_dict[round(loss_name)] #loss_name in {'pairwise', 'listwise', 'mse'}
        model_config['learning_rate']=learning_rate
        model_config['l1_l2']=round(l1_l2) #l1_l2 in {0, 1, 2}
        model_config['alpha']=alpha
        model_config['dropout_rate']=dropout_rate
        #model_config['flag_vsn_deep_cross']=norm_dict[round(flag_vsn_deep_cross)]
        model_config['model_type']=model_type_dict[round(model_type)]
        #model_config['df']=df
        #model_config['interactions']=interactions
        #model_config['items']=items

        #print(f'>>>>>>>>>>>>>model_config.keys()={model_config.keys()},\n <<<<<<<<<<<u_cnt_cols_bins={ model_config["u_cnt_cols_bins"]}')
        # ini & compile model
        model = create_model(config=model_config,
                            df=df,
                            interactions=interactions,
                            items=items,
                            )

        ## fit the model
        # early stopping to avoid overfitting
        monitor = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=1e-3,
                                patience=100, verbose=0, mode='auto', restore_best_weights=True)

        # set model checkpoint to record the weight during & after fitting
        model_checkpoint=tf.keras.callbacks.ModelCheckpoint(filepath=model_config['model_type']+'_best_model',
                                                            monitor='loss',
                                                            verbose=0,
                                                            save_best_only=True,
                                                            save_freq='epoch',
                                                            save_weights_only=True)

        # to deal with imbalance classification, calculate class_weight
        d = dict(df[label].value_counts())
        m = np.mean(list(d.values()))
        class_weight = {k:m/v for (k,v) in d.items()}

        if return_best_model: # use this option only **after** best model has been found

            history = model.fit(
                cached_train,
                validation_data=cached_valid,
                validation_freq=1,
                epochs=epochs,
                callbacks=[model_checkpoint, monitor],
                verbose=0,
                #class_weight=class_weight,
                )
        else:
            history = model.fit(
                                cached_train,
                                validation_data=cached_valid,
                                validation_freq=1,
                                epochs=epochs,
                                callbacks=[monitor],
                                verbose=0,
                                #class_weight=class_weight,
                                )

        accuracy = history.history['factorized_top_k/top_100_categorical_accuracy']
        print(f"------Train data: Top-100 accuracy = {accuracy}.")

        # evaluate the model on test data
        try:
          metrics = model.evaluate(cached_test, return_dict=True)
          print(f"======Test data: Retrieval top-100 accuracy = {metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}.======")
          print(f"======Test data: Ranking RMSE = {metrics['RMSE']:.3f}.======")
          print(f"======Test data: Ranking NDCG = {metrics['ndcg_metric']:.3f}.======")
        except Exception:
          pass

        if return_best_model:
            return model, metrics
        else:
            if metric not in ['RMSE', 'total_loss', 'loss', 'regularization_loss']:
                return metrics[metric] # if measuring performance such as 'ndcg_metric','factorized_top_k/top_100_categorical_accuracy', need to maximize it.
            else:
                return -1*metrics[metric] # if measuring loss, need to minimize it, therefore flip its sign

    ####################End def fit_model()

    # build optimizer
    optimizer = BayesianOptimization(
        f = fit_model, # function to be optimized (max)
        pbounds = pbounds, #  hyper parameters to tune
        verbose = 2, # 0: silent, 1:prints out when a maximum is observed, 2: prints out everything
        random_state=123
    )

    # search hyperparameter w. best performance
    # to deal with Error"Queue is empty while using Bayesian optimization", change init_points=10 to 50 Ref: https://stackoverflow.com/questions/60045053/queue-is-empty-while-using-bayesian-optimization
    optimizer.maximize(init_points=50, n_iter=100)

    # best hyperparameters
    best_para = optimizer.max

    print(f'best_para={best_para}')

    """#update model_config with user inputed data
    model_config['df'] = df
    model_config['interactions'] = interactions
    model_config['items'] = items"""

    best_model, best_metrics = fit_model(
                                        flag_use_norm=best_para['params']['flag_use_norm'],
                                        rating_weight=best_para['params']['rating_weight'],
                                        retrieval_weight=best_para['params']['retrieval_weight'],
                                        #classification_weight=best_para['params']['classification_weight'],
                                        learning_rate=best_para['params']['learning_rate'],
                                        n_layers=best_para['params']['n_layers'],
                                        layer_1=best_para['params']['layer_1'],
                                        layer_2=best_para['params']['layer_2'],
                                        layer_3=best_para['params']['layer_3'],
                                        loss_name=best_para['params']['loss_name'],
                                        l1_l2=best_para['params']['l1_l2'],
                                        alpha=best_para['params']['alpha'],
                                        dropout_rate=best_para['params']['dropout_rate'],
                                        model_type=best_para['params']['model_type'],
                                        return_best_model=True,
                                        metric=metric,
                                        #flag_vsn_deep_cross=best_para['params']['flag_vsn_deep_cross'],
                                        model_config=model_config,
                                        df=df,
                                        interactions=interactions,
                                        items=items,
                                        )

    # update best_model_config with best learning rate
    best_model.learning_rate = best_para['params']['learning_rate']

    return best_model, best_metrics, best_para




# Cell
#@title helpers for model understanding
def run_models(model, epochs=10, n_runs=5, learning_rate=0.1):
    """runs a model multiple times and returns the model's RMSE mean and standard deviation out of multiple runs
    Args:
        model: e.g. model = DeepCrossMultitaskModel(...) before you compile & fit it
        epochs (int): number of epochs to train the model, e.g. 100
        n_runs (int): number of time to run the model, e.g. 5
        learning_rate (float, default 0.1)
    Returns:

    Reference:

    """

    models = []
    metrics_ls = []
    rmses = []
    top100_cat_accus = []
    top50_cat_accus = []
    top10_cat_accus = []
    top5_cat_accus = []
    top1_cat_accus = []

    for i in range(n_runs):
        print(f'==============={i}_runs===============\n')
        model.compile(optimizer=[tf.keras.optimizers.Adam(learning_rate),
                                 tf.keras.optimizers.Adagrad(learning_rate=learning_rate, global_clipnorm=1.0),
                                 ][1])
        model.fit(cached_train, epochs=epochs, verbose=False)
        metrics = model.evaluate(cached_test, return_dict=True)

        models.append(model)
        top100_cat_accus.append(metrics['factorized_top_k/top_100_categorical_accuracy'])
        top50_cat_accus.append(metrics['factorized_top_k/top_50_categorical_accuracy'])
        top10_cat_accus.append(metrics['factorized_top_k/top_10_categorical_accuracy'])
        top5_cat_accus.append(metrics['factorized_top_k/top_5_categorical_accuracy'])
        top1_cat_accus.append(metrics['factorized_top_k/top_1_categorical_accuracy'])
        try:rmses.append(metrics["RMSE"])
        except:rmses.append(np.nan)
        metrics_ls.append(metrics)
        print(f"top_100_categorical_accuracy={metrics['factorized_top_k/top_100_categorical_accuracy']}\n")
        #print(f'RMSE={metrics["RMSE"]}\n')

    res= {"model": models,
          "metrics":metrics_ls,
          "top100": top100_cat_accus,
          "top50" : top50_cat_accus,
          "top10": top10_cat_accus,
          "top5" : top5_cat_accus,
          "top1": top1_cat_accus,
          "rmse": rmses,
          }

    return res




def plot_cross_feature_importances(model):
    """visualize cross feature importances
    Args:
        model: trained  # e.g. model provided by run_modles()['model'][0]
    References:
        https://www.tensorflow.org/recommenders/examples/dcn#model_understanding_2
    """
    import matplotlib.pyplot as plt
    from mpl_toolkits.axes_grid1 import make_axes_locatable
    #mat = model._cross_layer._dense.kernel
    #mat = tf.linalg.matmul(model._cross_layer._dense_u.kernel, model._cross_layer._dense_v.kernel)
    mat = tf.linalg.matmul(model._cross_layer.get_weights()[0], model._cross_layer.get_weights()[1])
    features = model._all_features

    block_norm = np.ones([len(features), len(features)])

    dim = model.config['embedding_dim']

    # Compute the norms of the blocks.
    for i in range(len(features)):
      for j in range(len(features)):
        block = mat[i * dim:(i + 1) * dim,
                    j * dim:(j + 1) * dim]
        block_norm[i,j] = np.linalg.norm(block, ord="fro")

    plt.figure(figsize=(9,9))
    im = plt.matshow(block_norm, cmap=plt.cm.Blues)
    ax = plt.gca()
    divider = make_axes_locatable(plt.gca())
    cax = divider.append_axes("right", size="5%", pad=0.05)
    plt.colorbar(im, cax=cax)
    cax.tick_params(labelsize=10)
    _ = ax.set_xticklabels([""] + features, rotation=45, ha="left", fontsize=10)
    _ = ax.set_yticklabels([""] + features, fontsize=10)

